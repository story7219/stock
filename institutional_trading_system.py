"""
ê¸°ê´€íˆ¬ììê¸‰ íŠ¸ë ˆì´ë”© ì‹œìŠ¤í…œ
- ìµœì‹  í€€íŠ¸ ê¸°ë²• ì ìš©
- ë¦¬ìŠ¤í¬ ê´€ë¦¬ ê³ ë„í™”
- í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”
- ì‹¤ì‹œê°„ ì•ŒíŒŒ ë°œêµ´
"""

import numpy as np
import pandas as pd
import asyncio
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Tuple
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor
import warnings
warnings.filterwarnings('ignore')

# ìµœì‹  ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤
import yfinance as yf
import ta
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import TimeSeriesSplit
import xgboost as xgb
import lightgbm as lgb
from scipy import optimize
from scipy.stats import norm, jarque_bera
import cvxpy as cp
from arch import arch_model
import statsmodels.api as sm
from statsmodels.tsa.vector_ar.var_model import VAR

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

@dataclass
class InstitutionalConfig:
    """ê¸°ê´€íˆ¬ììê¸‰ ì„¤ì •"""
    # í¬íŠ¸í´ë¦¬ì˜¤ ì„¤ì •
    max_position_size: float = 0.05  # ê°œë³„ ì¢…ëª© ìµœëŒ€ 5%
    max_sector_exposure: float = 0.20  # ì„¹í„°ë³„ ìµœëŒ€ 20%
    target_volatility: float = 0.15  # ëª©í‘œ ë³€ë™ì„± 15%
    
    # ë¦¬ìŠ¤í¬ ê´€ë¦¬
    var_confidence: float = 0.05  # VaR 95% ì‹ ë¢°êµ¬ê°„
    max_drawdown: float = 0.10  # ìµœëŒ€ ë‚™í­ 10%
    correlation_threshold: float = 0.7  # ìƒê´€ê´€ê³„ ì„ê³„ê°’
    
    # ì•ŒíŒŒ ë°œêµ´
    min_alpha_threshold: float = 0.02  # ìµœì†Œ ì•ŒíŒŒ 2%
    lookback_period: int = 252  # 1ë…„ ë£©ë°±
    rebalancing_frequency: str = "weekly"  # ì£¼ê°„ ë¦¬ë°¸ëŸ°ì‹±
    
    # ì‹¤í–‰ ì„¤ì •
    slippage_bps: float = 5.0  # ìŠ¬ë¦¬í”¼ì§€ 5bp
    commission_bps: float = 3.0  # ìˆ˜ìˆ˜ë£Œ 3bp
    market_impact_factor: float = 0.1  # ë§ˆì¼“ ì„íŒ©íŠ¸

class AdvancedFactorModel:
    """ê³ ê¸‰ íŒ©í„° ëª¨ë¸ (Fama-French 5íŒ©í„° + ëª¨ë©˜í…€ + í’ˆì§ˆ)"""
    
    def __init__(self):
        self.factors = [
            'market_beta',      # ì‹œì¥ ë² íƒ€
            'size_factor',      # ê·œëª¨ íŒ©í„° (SMB)
            'value_factor',     # ê°€ì¹˜ íŒ©í„° (HML)
            'profitability',    # ìˆ˜ìµì„± íŒ©í„° (RMW)
            'investment',       # íˆ¬ì íŒ©í„° (CMA)
            'momentum',         # ëª¨ë©˜í…€ íŒ©í„°
            'quality',          # í’ˆì§ˆ íŒ©í„°
            'low_volatility',   # ì €ë³€ë™ì„± íŒ©í„°
            'dividend_yield'    # ë°°ë‹¹ìˆ˜ìµë¥  íŒ©í„°
        ]
        
    def calculate_factors(self, price_data: pd.DataFrame, 
                         fundamental_data: pd.DataFrame) -> pd.DataFrame:
        """íŒ©í„° ê³„ì‚°"""
        factors_df = pd.DataFrame(index=price_data.index)
        
        # 1. ì‹œì¥ ë² íƒ€ (60ì¼ ë¡¤ë§)
        market_returns = price_data.pct_change().mean(axis=1)
        for stock in price_data.columns:
            stock_returns = price_data[stock].pct_change()
            rolling_beta = stock_returns.rolling(60).cov(market_returns) / market_returns.rolling(60).var()
            factors_df[f'{stock}_market_beta'] = rolling_beta
        
        # 2. ê·œëª¨ íŒ©í„° (ì‹œê°€ì´ì•¡ ê¸°ë°˜)
        market_cap = fundamental_data['market_cap']
        size_factor = np.log(market_cap) / np.log(market_cap.median())
        factors_df = factors_df.join(size_factor.rename('size_factor'))
        
        # 3. ê°€ì¹˜ íŒ©í„° (PBR, PER ê¸°ë°˜)
        pbr = fundamental_data['pbr']
        per = fundamental_data['per']
        value_factor = (1/pbr + 1/per) / 2
        factors_df = factors_df.join(value_factor.rename('value_factor'))
        
        # 4. ëª¨ë©˜í…€ íŒ©í„° (12-1ê°œì›”)
        momentum_12m = price_data.pct_change(252).shift(21)  # 12ê°œì›” ì „ ëŒ€ë¹„, 1ê°œì›” ì§€ì—°
        factors_df = factors_df.join(momentum_12m.add_suffix('_momentum'))
        
        # 5. í’ˆì§ˆ íŒ©í„° (ROE, ë¶€ì±„ë¹„ìœ¨ ê¸°ë°˜)
        roe = fundamental_data['roe']
        debt_ratio = fundamental_data['debt_ratio']
        quality_factor = roe / (1 + debt_ratio)
        factors_df = factors_df.join(quality_factor.rename('quality_factor'))
        
        # 6. ì €ë³€ë™ì„± íŒ©í„°
        volatility_60d = price_data.pct_change().rolling(60).std()
        low_vol_factor = 1 / volatility_60d
        factors_df = factors_df.join(low_vol_factor.add_suffix('_low_vol'))
        
        return factors_df

class RiskModel:
    """ê³ ê¸‰ ë¦¬ìŠ¤í¬ ëª¨ë¸"""
    
    def __init__(self, config: InstitutionalConfig):
        self.config = config
        
    def calculate_var(self, returns: pd.Series, confidence: float = 0.05) -> float:
        """Value at Risk ê³„ì‚° (Historical Simulation + Monte Carlo)"""
        # Historical VaR
        hist_var = np.percentile(returns.dropna(), confidence * 100)
        
        # Parametric VaR (ì •ê·œë¶„í¬ ê°€ì •)
        mean_return = returns.mean()
        std_return = returns.std()
        param_var = norm.ppf(confidence, mean_return, std_return)
        
        # Monte Carlo VaR
        np.random.seed(42)
        simulated_returns = np.random.normal(mean_return, std_return, 10000)
        mc_var = np.percentile(simulated_returns, confidence * 100)
        
        # ê°€ì¤‘í‰ê·  (Historical 50%, Parametric 30%, MC 20%)
        combined_var = 0.5 * hist_var + 0.3 * param_var + 0.2 * mc_var
        
        return combined_var
    
    def calculate_expected_shortfall(self, returns: pd.Series, confidence: float = 0.05) -> float:
        """Expected Shortfall (CVaR) ê³„ì‚°"""
        var = self.calculate_var(returns, confidence)
        es = returns[returns <= var].mean()
        return es
    
    def calculate_maximum_drawdown(self, prices: pd.Series) -> float:
        """ìµœëŒ€ ë‚™í­ ê³„ì‚°"""
        cumulative = (1 + prices.pct_change()).cumprod()
        running_max = cumulative.expanding().max()
        drawdown = (cumulative - running_max) / running_max
        return drawdown.min()
    
    def estimate_garch_volatility(self, returns: pd.Series) -> pd.Series:
        """GARCH ëª¨ë¸ì„ ì´ìš©í•œ ë³€ë™ì„± ì˜ˆì¸¡"""
        try:
            # GARCH(1,1) ëª¨ë¸
            model = arch_model(returns.dropna() * 100, vol='Garch', p=1, q=1)
            fitted_model = model.fit(disp='off')
            
            # ì¡°ê±´ë¶€ ë³€ë™ì„± ì˜ˆì¸¡
            forecast = fitted_model.forecast(horizon=1)
            conditional_vol = np.sqrt(forecast.variance.iloc[-1, 0]) / 100
            
            return conditional_vol
            
        except Exception as e:
            logger.warning(f"GARCH ëª¨ë¸ ì‹¤íŒ¨, ë‹¨ìˆœ ë³€ë™ì„± ì‚¬ìš©: {e}")
            return returns.rolling(30).std().iloc[-1]

class PortfolioOptimizer:
    """í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™” (Black-Litterman + Risk Parity)"""
    
    def __init__(self, config: InstitutionalConfig):
        self.config = config
        
    def black_litterman_optimization(self, 
                                   expected_returns: pd.Series,
                                   cov_matrix: pd.DataFrame,
                                   market_caps: pd.Series,
                                   views: Dict[str, float] = None,
                                   view_confidence: float = 0.5) -> pd.Series:
        """Black-Litterman ëª¨ë¸ ìµœì í™”"""
        
        # 1. ì‹œì¥ ê· í˜• ìˆ˜ìµë¥  ê³„ì‚°
        risk_aversion = 3.0  # ìœ„í—˜íšŒí”¼ê³„ìˆ˜
        market_weights = market_caps / market_caps.sum()
        equilibrium_returns = risk_aversion * cov_matrix.dot(market_weights)
        
        # 2. íˆ¬ìì ê²¬í•´ ë°˜ì˜
        if views:
            # ê²¬í•´ í–‰ë ¬ Pì™€ ê²¬í•´ ë²¡í„° Q êµ¬ì„±
            P = np.zeros((len(views), len(expected_returns)))
            Q = np.zeros(len(views))
            
            for i, (asset, view_return) in enumerate(views.items()):
                if asset in expected_returns.index:
                    asset_idx = expected_returns.index.get_loc(asset)
                    P[i, asset_idx] = 1
                    Q[i] = view_return
            
            # ê²¬í•´ì˜ ë¶ˆí™•ì‹¤ì„± í–‰ë ¬ Omega
            omega = np.diag(np.diag(P @ cov_matrix @ P.T)) / view_confidence
            
            # Black-Litterman ê³µì‹
            tau = 0.05  # ë¶ˆí™•ì‹¤ì„± ë§¤ê°œë³€ìˆ˜
            M1 = np.linalg.inv(tau * cov_matrix)
            M2 = P.T @ np.linalg.inv(omega) @ P
            M3 = np.linalg.inv(tau * cov_matrix) @ equilibrium_returns
            M4 = P.T @ np.linalg.inv(omega) @ Q
            
            bl_returns = np.linalg.inv(M1 + M2) @ (M3 + M4)
            bl_returns = pd.Series(bl_returns, index=expected_returns.index)
        else:
            bl_returns = equilibrium_returns
        
        # 3. ìµœì  í¬íŠ¸í´ë¦¬ì˜¤ ê³„ì‚°
        weights = self.mean_variance_optimization(bl_returns, cov_matrix)
        
        return weights
    
    def risk_parity_optimization(self, cov_matrix: pd.DataFrame) -> pd.Series:
        """ë¦¬ìŠ¤í¬ íŒ¨ë¦¬í‹° ìµœì í™”"""
        n_assets = len(cov_matrix)
        
        def risk_budget_objective(weights, cov_matrix):
            """ë¦¬ìŠ¤í¬ ê¸°ì—¬ë„ ê· ë“±í™” ëª©ì í•¨ìˆ˜"""
            portfolio_vol = np.sqrt(weights.T @ cov_matrix @ weights)
            marginal_contrib = cov_matrix @ weights / portfolio_vol
            contrib = weights * marginal_contrib
            return np.sum((contrib - contrib.mean()) ** 2)
        
        # ì œì•½ì¡°ê±´: ê°€ì¤‘ì¹˜ í•© = 1, ëª¨ë“  ê°€ì¤‘ì¹˜ >= 0
        constraints = [
            {'type': 'eq', 'fun': lambda x: np.sum(x) - 1},
        ]
        bounds = [(0.01, self.config.max_position_size) for _ in range(n_assets)]
        
        # ì´ˆê¸°ê°’: ë™ì¼ê°€ì¤‘
        x0 = np.array([1/n_assets] * n_assets)
        
        # ìµœì í™” ì‹¤í–‰
        result = optimize.minimize(
            risk_budget_objective,
            x0,
            args=(cov_matrix.values,),
            method='SLSQP',
            bounds=bounds,
            constraints=constraints
        )
        
        if result.success:
            weights = pd.Series(result.x, index=cov_matrix.index)
            return weights / weights.sum()  # ì •ê·œí™”
        else:
            logger.warning("ë¦¬ìŠ¤í¬ íŒ¨ë¦¬í‹° ìµœì í™” ì‹¤íŒ¨, ë™ì¼ê°€ì¤‘ ì‚¬ìš©")
            return pd.Series([1/n_assets] * n_assets, index=cov_matrix.index)
    
    def mean_variance_optimization(self, 
                                 expected_returns: pd.Series,
                                 cov_matrix: pd.DataFrame,
                                 target_return: float = None) -> pd.Series:
        """í‰ê· -ë¶„ì‚° ìµœì í™” (CVXPY ì‚¬ìš©)"""
        
        n_assets = len(expected_returns)
        weights = cp.Variable(n_assets)
        
        # ëª©ì í•¨ìˆ˜: í¬íŠ¸í´ë¦¬ì˜¤ ë¶„ì‚° ìµœì†Œí™”
        portfolio_variance = cp.quad_form(weights, cov_matrix.values)
        objective = cp.Minimize(portfolio_variance)
        
        # ì œì•½ì¡°ê±´
        constraints = [
            cp.sum(weights) == 1,  # ê°€ì¤‘ì¹˜ í•© = 1
            weights >= 0.01,       # ìµœì†Œ ê°€ì¤‘ì¹˜
            weights <= self.config.max_position_size  # ìµœëŒ€ ê°€ì¤‘ì¹˜
        ]
        
        # ëª©í‘œ ìˆ˜ìµë¥  ì œì•½ (ì„ íƒì )
        if target_return:
            portfolio_return = weights.T @ expected_returns.values
            constraints.append(portfolio_return >= target_return)
        
        # ìµœì í™” ë¬¸ì œ ì •ì˜ ë° í•´ê²°
        problem = cp.Problem(objective, constraints)
        problem.solve()
        
        if problem.status == cp.OPTIMAL:
            optimal_weights = pd.Series(weights.value, index=expected_returns.index)
            return optimal_weights / optimal_weights.sum()
        else:
            logger.warning("í‰ê· -ë¶„ì‚° ìµœì í™” ì‹¤íŒ¨, ë™ì¼ê°€ì¤‘ ì‚¬ìš©")
            return pd.Series([1/n_assets] * n_assets, index=expected_returns.index)

class AlphaModel:
    """ì•ŒíŒŒ ë°œêµ´ ëª¨ë¸ (ML + ì „í†µì  ê¸°ë²• ê²°í•©)"""
    
    def __init__(self):
        self.models = {
            'xgboost': xgb.XGBRegressor(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                random_state=42
            ),
            'lightgbm': lgb.LGBMRegressor(
                n_estimators=100,
                max_depth=6,
                learning_rate=0.1,
                random_state=42
            ),
            'random_forest': RandomForestRegressor(
                n_estimators=100,
                max_depth=10,
                random_state=42
            )
        }
        self.scaler = RobustScaler()
        
    def create_features(self, price_data: pd.DataFrame, 
                       volume_data: pd.DataFrame = None,
                       fundamental_data: pd.DataFrame = None) -> pd.DataFrame:
        """í”¼ì²˜ ìƒì„± (ê¸°ìˆ ì  + ê¸°ë³¸ì  + ëŒ€ì•ˆì  ì§€í‘œ)"""
        
        features = pd.DataFrame(index=price_data.index)
        
        for stock in price_data.columns:
            stock_price = price_data[stock]
            
            # ê¸°ìˆ ì  ì§€í‘œ
            features[f'{stock}_rsi'] = ta.momentum.RSIIndicator(stock_price).rsi()
            features[f'{stock}_macd'] = ta.trend.MACD(stock_price).macd()
            features[f'{stock}_bb_upper'] = ta.volatility.BollingerBands(stock_price).bollinger_hband()
            features[f'{stock}_bb_lower'] = ta.volatility.BollingerBands(stock_price).bollinger_lband()
            features[f'{stock}_atr'] = ta.volatility.AverageTrueRange(stock_price, stock_price, stock_price).average_true_range()
            
            # ëª¨ë©˜í…€ ì§€í‘œ
            features[f'{stock}_mom_1m'] = stock_price.pct_change(21)
            features[f'{stock}_mom_3m'] = stock_price.pct_change(63)
            features[f'{stock}_mom_6m'] = stock_price.pct_change(126)
            features[f'{stock}_mom_12m'] = stock_price.pct_change(252)
            
            # ë³€ë™ì„± ì§€í‘œ
            features[f'{stock}_vol_5d'] = stock_price.pct_change().rolling(5).std()
            features[f'{stock}_vol_20d'] = stock_price.pct_change().rolling(20).std()
            features[f'{stock}_vol_60d'] = stock_price.pct_change().rolling(60).std()
            
            # ê°€ê²© íŒ¨í„´
            features[f'{stock}_price_position'] = (stock_price - stock_price.rolling(252).min()) / (stock_price.rolling(252).max() - stock_price.rolling(252).min())
            features[f'{stock}_ma_ratio'] = stock_price / stock_price.rolling(20).mean()
            
            # ë³¼ë¥¨ ì§€í‘œ (ìˆëŠ” ê²½ìš°)
            if volume_data is not None and stock in volume_data.columns:
                volume = volume_data[stock]
                features[f'{stock}_volume_ratio'] = volume / volume.rolling(20).mean()
                features[f'{stock}_price_volume'] = stock_price.pct_change() * volume
        
        # ì‹œì¥ ì „ì²´ ì§€í‘œ
        market_return = price_data.pct_change().mean(axis=1)
        features['market_momentum'] = market_return.rolling(20).mean()
        features['market_volatility'] = market_return.rolling(20).std()
        features['market_trend'] = (price_data.mean(axis=1) > price_data.mean(axis=1).rolling(50).mean()).astype(int)
        
        return features.fillna(method='ffill').fillna(0)
    
    def train_ensemble_model(self, features: pd.DataFrame, 
                           returns: pd.DataFrame,
                           lookback_days: int = 252) -> Dict:
        """ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨"""
        
        trained_models = {}
        
        for stock in returns.columns:
            logger.info(f"ğŸ“ˆ {stock} ì•ŒíŒŒ ëª¨ë¸ í›ˆë ¨ ì¤‘...")
            
            # íƒ€ê²Ÿ ë³€ìˆ˜: ë¯¸ë˜ ìˆ˜ìµë¥  (5ì¼ í›„)
            target = returns[stock].shift(-5)
            
            # í”¼ì²˜ ì„ íƒ (í•´ë‹¹ ì¢…ëª© + ì‹œì¥ ì§€í‘œ)
            stock_features = features.filter(regex=f'{stock}_|market_')
            
            # ë°ì´í„° ì •ë¦¬
            valid_idx = ~(stock_features.isna().any(axis=1) | target.isna())
            X = stock_features[valid_idx]
            y = target[valid_idx]
            
            if len(X) < lookback_days:
                logger.warning(f"{stock}: ë°ì´í„° ë¶€ì¡±ìœ¼ë¡œ ìŠ¤í‚µ")
                continue
            
            # ì‹œê³„ì—´ ë¶„í• 
            tscv = TimeSeriesSplit(n_splits=5)
            
            stock_models = {}
            for model_name, model in self.models.items():
                try:
                    # í”¼ì²˜ ìŠ¤ì¼€ì¼ë§
                    X_scaled = self.scaler.fit_transform(X)
                    
                    # ëª¨ë¸ í›ˆë ¨
                    model.fit(X_scaled, y)
                    
                    # êµì°¨ê²€ì¦ ì ìˆ˜
                    cv_scores = []
                    for train_idx, val_idx in tscv.split(X_scaled):
                        X_train, X_val = X_scaled[train_idx], X_scaled[val_idx]
                        y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]
                        
                        model.fit(X_train, y_train)
                        score = model.score(X_val, y_val)
                        cv_scores.append(score)
                    
                    avg_score = np.mean(cv_scores)
                    stock_models[model_name] = {
                        'model': model,
                        'score': avg_score,
                        'scaler': self.scaler
                    }
                    
                    logger.info(f"  {model_name}: RÂ² = {avg_score:.4f}")
                    
                except Exception as e:
                    logger.error(f"  {model_name} í›ˆë ¨ ì‹¤íŒ¨: {e}")
            
            trained_models[stock] = stock_models
        
        return trained_models
    
    def predict_alpha(self, models: Dict, features: pd.DataFrame) -> pd.Series:
        """ì•ŒíŒŒ ì˜ˆì¸¡ (ì•™ìƒë¸”)"""
        
        predictions = {}
        
        for stock, stock_models in models.items():
            if not stock_models:
                continue
                
            # í•´ë‹¹ ì¢…ëª© í”¼ì²˜ ì¶”ì¶œ
            stock_features = features.filter(regex=f'{stock}_|market_').iloc[-1:].fillna(0)
            
            # ê° ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’
            model_predictions = []
            model_weights = []
            
            for model_name, model_info in stock_models.items():
                try:
                    model = model_info['model']
                    scaler = model_info['scaler']
                    score = model_info['score']
                    
                    # í”¼ì²˜ ìŠ¤ì¼€ì¼ë§ ë° ì˜ˆì¸¡
                    X_scaled = scaler.transform(stock_features)
                    pred = model.predict(X_scaled)[0]
                    
                    model_predictions.append(pred)
                    model_weights.append(max(score, 0))  # ìŒìˆ˜ ì ìˆ˜ëŠ” 0ìœ¼ë¡œ
                    
                except Exception as e:
                    logger.warning(f"{stock} {model_name} ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            
            if model_predictions:
                # ê°€ì¤‘í‰ê·  ì•™ìƒë¸”
                weights = np.array(model_weights)
                if weights.sum() > 0:
                    weights = weights / weights.sum()
                    ensemble_pred = np.average(model_predictions, weights=weights)
                    predictions[stock] = ensemble_pred
        
        return pd.Series(predictions)

class ExecutionModel:
    """ì‹¤í–‰ ëª¨ë¸ (TWAP, VWAP, Implementation Shortfall)"""
    
    def __init__(self, config: InstitutionalConfig):
        self.config = config
        
    def calculate_optimal_execution(self, 
                                  target_positions: pd.Series,
                                  current_positions: pd.Series,
                                  market_data: pd.DataFrame,
                                  execution_horizon: int = 5) -> pd.DataFrame:
        """ìµœì  ì‹¤í–‰ ì „ëµ ê³„ì‚°"""
        
        # ê±°ë˜í•´ì•¼ í•  ìˆ˜ëŸ‰ ê³„ì‚°
        trade_sizes = target_positions - current_positions
        
        execution_schedule = pd.DataFrame(
            index=range(execution_horizon),
            columns=trade_sizes.index
        )
        
        for stock in trade_sizes.index:
            if abs(trade_sizes[stock]) < 0.001:  # ê±°ë˜ëŸ‰ì´ ë„ˆë¬´ ì‘ìœ¼ë©´ ìŠ¤í‚µ
                execution_schedule[stock] = 0
                continue
            
            # ì‹œì¥ ì„íŒ©íŠ¸ ê³ ë ¤í•œ ìµœì  ì‹¤í–‰
            total_size = abs(trade_sizes[stock])
            
            # TWAP ê¸°ë°˜ (ì‹œê°„ ê°€ì¤‘ í‰ê· ê°€ê²©)
            if total_size < 0.02:  # ì‘ì€ ê±°ë˜ëŸ‰
                # ê· ë“± ë¶„í• 
                daily_size = trade_sizes[stock] / execution_horizon
                execution_schedule[stock] = daily_size
            else:
                # ì‹œì¥ ì„íŒ©íŠ¸ ìµœì†Œí™” (ì•ìª½ì— ë” ë§ì´)
                weights = np.array([0.4, 0.25, 0.15, 0.12, 0.08])[:execution_horizon]
                weights = weights / weights.sum()
                
                for i in range(execution_horizon):
                    execution_schedule.loc[i, stock] = trade_sizes[stock] * weights[i]
        
        return execution_schedule.fillna(0)
    
    def estimate_transaction_costs(self, 
                                 execution_schedule: pd.DataFrame,
                                 market_data: pd.DataFrame) -> pd.Series:
        """ê±°ë˜ë¹„ìš© ì¶”ì •"""
        
        total_costs = {}
        
        for stock in execution_schedule.columns:
            daily_trades = execution_schedule[stock]
            
            # ê¸°ë³¸ ë¹„ìš©
            commission_cost = abs(daily_trades).sum() * self.config.commission_bps / 10000
            
            # ìŠ¬ë¦¬í”¼ì§€ ë¹„ìš©
            slippage_cost = abs(daily_trades).sum() * self.config.slippage_bps / 10000
            
            # ë§ˆì¼“ ì„íŒ©íŠ¸ (ê±°ë˜ëŸ‰ì— ë¹„ë¡€)
            avg_volume = market_data.get(f'{stock}_volume', pd.Series([1000000])).mean()
            impact_cost = (abs(daily_trades).sum() ** 1.5) * self.config.market_impact_factor / avg_volume
            
            total_costs[stock] = commission_cost + slippage_cost + impact_cost
        
        return pd.Series(total_costs)

class InstitutionalTradingSystem:
    """ê¸°ê´€íˆ¬ììê¸‰ í†µí•© íŠ¸ë ˆì´ë”© ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: InstitutionalConfig = None):
        self.config = config or InstitutionalConfig()
        
        # ì„œë¸Œì‹œìŠ¤í…œ ì´ˆê¸°í™”
        self.factor_model = AdvancedFactorModel()
        self.risk_model = RiskModel(self.config)
        self.optimizer = PortfolioOptimizer(self.config)
        self.alpha_model = AlphaModel()
        self.execution_model = ExecutionModel(self.config)
        
        # ìƒíƒœ ë³€ìˆ˜
        self.current_positions = pd.Series(dtype=float)
        self.trained_models = {}
        self.last_rebalance = None
        
        logger.info("ğŸ¦ ê¸°ê´€íˆ¬ììê¸‰ íŠ¸ë ˆì´ë”© ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def run_daily_process(self, 
                              price_data: pd.DataFrame,
                              volume_data: pd.DataFrame = None,
                              fundamental_data: pd.DataFrame = None) -> Dict:
        """ì¼ì¼ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰"""
        
        logger.info("ğŸš€ ì¼ì¼ íŠ¸ë ˆì´ë”© í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
        
        results = {
            'timestamp': datetime.now(),
            'signals': {},
            'positions': {},
            'risk_metrics': {},
            'execution_plan': {}
        }
        
        try:
            # 1. íŒ©í„° ë¶„ì„
            logger.info("ğŸ“Š íŒ©í„° ë¶„ì„ ì¤‘...")
            if fundamental_data is not None:
                factors = self.factor_model.calculate_factors(price_data, fundamental_data)
            else:
                factors = pd.DataFrame()  # ê¸°ë³¸ì  ë¶„ì„ ì—†ì´ ì§„í–‰
            
            # 2. ì•ŒíŒŒ ì‹ í˜¸ ìƒì„±
            logger.info("ğŸ” ì•ŒíŒŒ ì‹ í˜¸ ìƒì„± ì¤‘...")
            features = self.alpha_model.create_features(price_data, volume_data, fundamental_data)
            
            if not self.trained_models:
                logger.info("ğŸ¤– ML ëª¨ë¸ í›ˆë ¨ ì¤‘...")
                returns = price_data.pct_change()
                self.trained_models = self.alpha_model.train_ensemble_model(features, returns)
            
            alpha_signals = self.alpha_model.predict_alpha(self.trained_models, features)
            results['signals'] = alpha_signals.to_dict()
            
            # 3. ë¦¬ìŠ¤í¬ ë¶„ì„
            logger.info("âš ï¸ ë¦¬ìŠ¤í¬ ë¶„ì„ ì¤‘...")
            returns = price_data.pct_change().dropna()
            
            risk_metrics = {}
            for stock in returns.columns:
                stock_returns = returns[stock]
                risk_metrics[stock] = {
                    'var_95': self.risk_model.calculate_var(stock_returns, 0.05),
                    'expected_shortfall': self.risk_model.calculate_expected_shortfall(stock_returns, 0.05),
                    'max_drawdown': self.risk_model.calculate_maximum_drawdown(price_data[stock]),
                    'volatility': self.risk_model.estimate_garch_volatility(stock_returns)
                }
            
            results['risk_metrics'] = risk_metrics
            
            # 4. í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™”
            logger.info("âš–ï¸ í¬íŠ¸í´ë¦¬ì˜¤ ìµœì í™” ì¤‘...")
            
            # ì˜ˆìƒ ìˆ˜ìµë¥  (ì•ŒíŒŒ ì‹ í˜¸ + ë¦¬ìŠ¤í¬ ì¡°ì •)
            expected_returns = alpha_signals.copy()
            for stock in expected_returns.index:
                # ë¦¬ìŠ¤í¬ ì¡°ì • ìˆ˜ìµë¥ 
                vol = risk_metrics[stock]['volatility']
                expected_returns[stock] = expected_returns[stock] / (1 + vol * 2)
            
            # ê³µë¶„ì‚° í–‰ë ¬
            cov_matrix = returns.cov() * 252  # ì—°ìœ¨í™”
            
            # Black-Litterman ìµœì í™”
            if fundamental_data is not None:
                market_caps = fundamental_data.get('market_cap', pd.Series([1] * len(expected_returns), index=expected_returns.index))
            else:
                market_caps = pd.Series([1] * len(expected_returns), index=expected_returns.index)
            
            optimal_weights = self.optimizer.black_litterman_optimization(
                expected_returns, cov_matrix, market_caps
            )
            
            # ë¦¬ìŠ¤í¬ íŒ¨ë¦¬í‹°ì™€ ê²°í•© (50:50)
            risk_parity_weights = self.optimizer.risk_parity_optimization(cov_matrix)
            combined_weights = 0.5 * optimal_weights + 0.5 * risk_parity_weights
            
            results['positions'] = combined_weights.to_dict()
            
            # 5. ì‹¤í–‰ ê³„íš
            logger.info("ğŸ“‹ ì‹¤í–‰ ê³„íš ìˆ˜ë¦½ ì¤‘...")
            execution_schedule = self.execution_model.calculate_optimal_execution(
                combined_weights, self.current_positions, price_data
            )
            
            transaction_costs = self.execution_model.estimate_transaction_costs(
                execution_schedule, price_data
            )
            
            results['execution_plan'] = {
                'schedule': execution_schedule.to_dict(),
                'estimated_costs': transaction_costs.to_dict()
            }
            
            # í¬ì§€ì…˜ ì—…ë°ì´íŠ¸
            self.current_positions = combined_weights
            self.last_rebalance = datetime.now()
            
            logger.info("âœ… ì¼ì¼ í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"âŒ ì¼ì¼ í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")
            results['error'] = str(e)
        
        return results
    
    def generate_performance_report(self, 
                                  price_data: pd.DataFrame,
                                  benchmark_data: pd.DataFrame = None) -> Dict:
        """ì„±ê³¼ ë¦¬í¬íŠ¸ ìƒì„±"""
        
        if self.current_positions.empty:
            return {"error": "í¬ì§€ì…˜ ë°ì´í„° ì—†ìŒ"}
        
        # í¬íŠ¸í´ë¦¬ì˜¤ ìˆ˜ìµë¥  ê³„ì‚°
        returns = price_data.pct_change().dropna()
        portfolio_returns = (returns * self.current_positions).sum(axis=1)
        
        # ì„±ê³¼ ì§€í‘œ ê³„ì‚°
        total_return = (1 + portfolio_returns).prod() - 1
        annualized_return = (1 + total_return) ** (252 / len(portfolio_returns)) - 1
        volatility = portfolio_returns.std() * np.sqrt(252)
        sharpe_ratio = annualized_return / volatility if volatility > 0 else 0
        
        # ìµœëŒ€ ë‚™í­
        cumulative_returns = (1 + portfolio_returns).cumprod()
        max_drawdown = self.risk_model.calculate_maximum_drawdown(cumulative_returns)
        
        # ë²¤ì¹˜ë§ˆí¬ ëŒ€ë¹„ ì„±ê³¼ (ìˆëŠ” ê²½ìš°)
        alpha = 0
        beta = 1
        information_ratio = 0
        
        if benchmark_data is not None:
            benchmark_returns = benchmark_data.pct_change().dropna()
            
            # ê³µí†µ ê¸°ê°„ ì¶”ì¶œ
            common_dates = portfolio_returns.index.intersection(benchmark_returns.index)
            port_ret = portfolio_returns.loc[common_dates]
            bench_ret = benchmark_returns.loc[common_dates]
            
            if len(common_dates) > 30:
                # ë² íƒ€ ê³„ì‚°
                covariance = np.cov(port_ret, bench_ret)[0, 1]
                benchmark_variance = np.var(bench_ret)
                beta = covariance / benchmark_variance if benchmark_variance > 0 else 1
                
                # ì•ŒíŒŒ ê³„ì‚°
                alpha = annualized_return - beta * (bench_ret.mean() * 252)
                
                # ì •ë³´ë¹„ìœ¨
                active_returns = port_ret - bench_ret
                tracking_error = active_returns.std() * np.sqrt(252)
                information_ratio = (active_returns.mean() * 252) / tracking_error if tracking_error > 0 else 0
        
        report = {
            'performance_metrics': {
                'total_return': total_return,
                'annualized_return': annualized_return,
                'volatility': volatility,
                'sharpe_ratio': sharpe_ratio,
                'max_drawdown': max_drawdown,
                'alpha': alpha,
                'beta': beta,
                'information_ratio': information_ratio
            },
            'risk_metrics': {
                'var_95': self.risk_model.calculate_var(portfolio_returns, 0.05),
                'expected_shortfall': self.risk_model.calculate_expected_shortfall(portfolio_returns, 0.05),
                'current_volatility': self.risk_model.estimate_garch_volatility(portfolio_returns)
            },
            'portfolio_composition': self.current_positions.to_dict(),
            'last_rebalance': self.last_rebalance
        }
        
        return report

# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # ì„¤ì •
    config = InstitutionalConfig(
        max_position_size=0.05,
        target_volatility=0.15,
        max_drawdown=0.10
    )
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    trading_system = InstitutionalTradingSystem(config)
    
    # ìƒ˜í”Œ ë°ì´í„° (ì‹¤ì œë¡œëŠ” ë°ì´í„° í”¼ë“œì—ì„œ ê°€ì ¸ì˜´)
    dates = pd.date_range('2023-01-01', '2024-01-01', freq='D')
    stocks = ['AAPL', 'GOOGL', 'MSFT', 'TSLA', 'NVDA']
    
    # ê°€ê²© ë°ì´í„° ì‹œë®¬ë ˆì´ì…˜
    np.random.seed(42)
    price_data = pd.DataFrame(
        np.random.randn(len(dates), len(stocks)).cumsum() + 100,
        index=dates,
        columns=stocks
    )
    
    # ì¼ì¼ í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰
    results = await trading_system.run_daily_process(price_data)
    
    print("ğŸ¦ ê¸°ê´€íˆ¬ììê¸‰ íŠ¸ë ˆì´ë”© ì‹œìŠ¤í…œ ê²°ê³¼:")
    print(f"ğŸ“Š ì•ŒíŒŒ ì‹ í˜¸: {results['signals']}")
    print(f"âš–ï¸ ìµœì  í¬ì§€ì…˜: {results['positions']}")
    print(f"ğŸ“‹ ì‹¤í–‰ ê³„íš: {results['execution_plan']['estimated_costs']}")
    
    # ì„±ê³¼ ë¦¬í¬íŠ¸
    performance = trading_system.generate_performance_report(price_data)
    print(f"ğŸ“ˆ ì„±ê³¼ ì§€í‘œ: {performance['performance_metrics']}")

if __name__ == "__main__":
    asyncio.run(main()) 