#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íŒŒì¼ëª…: realtime_ml_dashboard.py
ëª¨ë“ˆ: ì‹¤ì‹œê°„ ML ëŒ€ì‹œë³´ë“œ
ëª©ì : ëª¨ë¸ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§, í•™ìŠµ ì§„í–‰ìƒí™©, ì¸í„°ë™í‹°ë¸Œ ì‹œê°í™”

Author: AI Assistant
Created: 2025-01-27
Modified: 2025-01-27
Version: 1.0.0

Dependencies:
    - Python 3.11+
    - streamlit==1.28.0
    - plotly==5.17.0
    - pandas==2.1.0
    - mlflow==2.7.0
    - psutil==5.9.0

Performance:
    - ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸: ìë™ ìƒˆë¡œê³ ì¹¨
    - ì¸í„°ë™í‹°ë¸Œ ì°¨íŠ¸: Plotly ê¸°ë°˜
    - ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§: ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì¶”ì 
    - ì•Œë¦¼ ì‹œìŠ¤í…œ: ì„ê³„ê°’ ê¸°ë°˜ ì•Œë¦¼

Security:
    - ì ‘ê·¼ ì œì–´: ì„¸ì…˜ ê¸°ë°˜ ì¸ì¦
    - ë°ì´í„° ê²€ì¦: ì…ë ¥ ë°ì´í„° ê²€ì¦
    - ë¡œê¹…: ì‚¬ìš©ì í™œë™ ì¶”ì 

License: MIT
"""

import asyncio
import json
import logging
import os
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple

import mlflow
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import psutil
import streamlit as st
from plotly.subplots import make_subplots

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Trading ML Dashboard",
    page_icon="ğŸ“ˆ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# MLflow ì„¤ì •
mlflow.set_tracking_uri("sqlite:///mlflow.db")


class MLDashboard:
    """ML ëŒ€ì‹œë³´ë“œ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.mlflow_client = mlflow.tracking.MlflowClient()
        self.models_dir = Path("models")
        self.deployed_dir = Path("deployed_models")
        self.data_dir = Path("data")
        
    def get_system_metrics(self) -> Dict[str, float]:
        """ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­ ìˆ˜ì§‘"""
        return {
            'cpu_percent': psutil.cpu_percent(interval=1),
            'memory_percent': psutil.virtual_memory().percent,
            'disk_percent': psutil.disk_usage('/').percent,
            'network_io': psutil.net_io_counters().bytes_sent + psutil.net_io_counters().bytes_recv
        }
    
    def get_mlflow_experiments(self) -> List[Dict[str, Any]]:
        """MLflow ì‹¤í—˜ ëª©ë¡ ì¡°íšŒ"""
        try:
            experiments = self.mlflow_client.list_experiments()
            experiment_data = []
            
            for exp in experiments:
                runs = self.mlflow_client.search_runs(
                    experiment_ids=[exp.experiment_id],
                    max_results=10
                )
                
                if runs:
                    latest_run = runs[0]
                    experiment_data.append({
                        'experiment_id': exp.experiment_id,
                        'name': exp.name,
                        'latest_run_id': latest_run.info.run_id,
                        'status': latest_run.info.status,
                        'start_time': latest_run.info.start_time,
                        'end_time': latest_run.info.end_time
                    })
            
            return experiment_data
        except Exception as e:
            logger.error(f"Error fetching MLflow experiments: {e}")
            return []
    
    def get_model_performance(self) -> pd.DataFrame:
        """ëª¨ë¸ ì„±ëŠ¥ ë°ì´í„° ì¡°íšŒ"""
        try:
            # ëª¨ë¸ ë¹„êµ ê²°ê³¼ íŒŒì¼ í™•ì¸
            comparison_file = Path("model_comparison_results.csv")
            if comparison_file.exists():
                return pd.read_csv(comparison_file)
            
            # MLflowì—ì„œ ìµœì‹  ì‹¤í—˜ ê²°ê³¼ ì¡°íšŒ
            experiments = self.mlflow_client.list_experiments()
            performance_data = []
            
            for exp in experiments:
                runs = self.mlflow_client.search_runs(
                    experiment_ids=[exp.experiment_id],
                    max_results=1
                )
                
                if runs:
                    run = runs[0]
                    metrics = run.data.metrics
                    params = run.data.params
                    
                    performance_data.append({
                        'model': params.get('model_type', exp.name),
                        'test_mse': metrics.get('test_mse', 0),
                        'test_r2': metrics.get('test_r2', 0),
                        'train_loss': metrics.get('train_loss', 0),
                        'run_id': run.info.run_id,
                        'status': run.info.status
                    })
            
            return pd.DataFrame(performance_data)
        except Exception as e:
            logger.error(f"Error fetching model performance: {e}")
            return pd.DataFrame()
    
    def get_training_progress(self) -> Dict[str, Any]:
        """í•™ìŠµ ì§„í–‰ìƒí™© ì¡°íšŒ"""
        try:
            # Airflow DAG ìƒíƒœ í™•ì¸
            dag_status = self._check_airflow_status()
            
            # ìµœì‹  ëª¨ë¸ íŒŒì¼ í™•ì¸
            model_files = list(self.models_dir.glob("*.pth")) + list(self.models_dir.glob("*.joblib"))
            latest_model = max(model_files, key=lambda x: x.stat().st_mtime) if model_files else None
            
            # ë°°í¬ëœ ëª¨ë¸ í™•ì¸
            deployed_models = list(self.deployed_dir.glob("*_deployed.*")) if self.deployed_dir.exists() else []
            
            return {
                'dag_status': dag_status,
                'latest_model': latest_model.name if latest_model else None,
                'model_count': len(model_files),
                'deployed_count': len(deployed_models),
                'last_training': latest_model.stat().st_mtime if latest_model else None
            }
        except Exception as e:
            logger.error(f"Error fetching training progress: {e}")
            return {}
    
    def _check_airflow_status(self) -> str:
        """Airflow DAG ìƒíƒœ í™•ì¸"""
        try:
            # ê°„ë‹¨í•œ ìƒíƒœ í™•ì¸ (ì‹¤ì œë¡œëŠ” Airflow API ì‚¬ìš©)
            return "running"  # ë”ë¯¸ ê°’
        except Exception:
            return "unknown"
    
    def get_data_metrics(self) -> Dict[str, Any]:
        """ë°ì´í„° ë©”íŠ¸ë¦­ ì¡°íšŒ"""
        try:
            data_files = list(self.data_dir.glob("*.parquet"))
            total_size = sum(f.stat().st_size for f in data_files)
            
            # ìµœì‹  ë°ì´í„° íŒŒì¼ ë¶„ì„
            latest_file = max(data_files, key=lambda x: x.stat().st_mtime) if data_files else None
            
            if latest_file:
                df = pd.read_parquet(latest_file)
                return {
                    'total_files': len(data_files),
                    'total_size_mb': total_size / (1024 * 1024),
                    'latest_file': latest_file.name,
                    'latest_rows': len(df),
                    'latest_columns': len(df.columns),
                    'latest_update': datetime.fromtimestamp(latest_file.stat().st_mtime)
                }
            else:
                return {
                    'total_files': 0,
                    'total_size_mb': 0,
                    'latest_file': None,
                    'latest_rows': 0,
                    'latest_columns': 0,
                    'latest_update': None
                }
        except Exception as e:
            logger.error(f"Error fetching data metrics: {e}")
            return {}


def create_performance_chart(df: pd.DataFrame) -> go.Figure:
    """ì„±ëŠ¥ ì°¨íŠ¸ ìƒì„±"""
    if df.empty:
        return go.Figure()
    
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=('Test MSE by Model', 'Test RÂ² by Model', 'Model Comparison', 'Training Loss'),
        specs=[[{"secondary_y": False}, {"secondary_y": False}],
               [{"secondary_y": False}, {"secondary_y": False}]]
    )
    
    # Test MSE ì°¨íŠ¸
    fig.add_trace(
        go.Bar(x=df['model'], y=df['test_mse'], name='Test MSE', marker_color='red'),
        row=1, col=1
    )
    
    # Test RÂ² ì°¨íŠ¸
    fig.add_trace(
        go.Bar(x=df['model'], y=df['test_r2'], name='Test RÂ²', marker_color='green'),
        row=1, col=2
    )
    
    # ëª¨ë¸ ë¹„êµ ì‚°ì ë„
    fig.add_trace(
        go.Scatter(x=df['test_mse'], y=df['test_r2'], mode='markers+text',
                  text=df['model'], textposition="top center",
                  marker=dict(size=10, color=df['test_r2'], colorscale='Viridis'),
                  name='Model Comparison'),
        row=2, col=1
    )
    
    # í›ˆë ¨ ì†ì‹¤ ì°¨íŠ¸
    if 'train_loss' in df.columns:
        fig.add_trace(
            go.Scatter(x=df['model'], y=df['train_loss'], mode='lines+markers',
                      name='Training Loss', line=dict(color='orange')),
            row=2, col=2
        )
    
    fig.update_layout(height=600, showlegend=False)
    return fig


def create_system_monitoring_chart(metrics: Dict[str, float]) -> go.Figure:
    """ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ ì°¨íŠ¸ ìƒì„±"""
    fig = make_subplots(
        rows=2, cols=2,
        subplot_titles=('CPU Usage', 'Memory Usage', 'Disk Usage', 'Network I/O'),
        specs=[[{"secondary_y": False}, {"secondary_y": False}],
               [{"secondary_y": False}, {"secondary_y": False}]]
    )
    
    # CPU ì‚¬ìš©ë¥ 
    fig.add_trace(
        go.Indicator(
            mode="gauge+number",
            value=metrics['cpu_percent'],
            title={'text': "CPU %"},
            gauge={'axis': {'range': [None, 100]},
                   'bar': {'color': "darkblue"},
                   'steps': [{'range': [0, 50], 'color': "lightgray"},
                            {'range': [50, 80], 'color': "yellow"},
                            {'range': [80, 100], 'color': "red"}]},
            domain={'x': [0, 1], 'y': [0, 1]}
        ),
        row=1, col=1
    )
    
    # ë©”ëª¨ë¦¬ ì‚¬ìš©ë¥ 
    fig.add_trace(
        go.Indicator(
            mode="gauge+number",
            value=metrics['memory_percent'],
            title={'text': "Memory %"},
            gauge={'axis': {'range': [None, 100]},
                   'bar': {'color': "darkgreen"},
                   'steps': [{'range': [0, 50], 'color': "lightgray"},
                            {'range': [50, 80], 'color': "yellow"},
                            {'range': [80, 100], 'color': "red"}]},
            domain={'x': [0, 1], 'y': [0, 1]}
        ),
        row=1, col=2
    )
    
    # ë””ìŠ¤í¬ ì‚¬ìš©ë¥ 
    fig.add_trace(
        go.Indicator(
            mode="gauge+number",
            value=metrics['disk_percent'],
            title={'text': "Disk %"},
            gauge={'axis': {'range': [None, 100]},
                   'bar': {'color': "darkred"},
                   'steps': [{'range': [0, 50], 'color': "lightgray"},
                            {'range': [50, 80], 'color': "yellow"},
                            {'range': [80, 100], 'color': "red"}]},
            domain={'x': [0, 1], 'y': [0, 1]}
        ),
        row=2, col=1
    )
    
    # ë„¤íŠ¸ì›Œí¬ I/O
    fig.add_trace(
        go.Indicator(
            mode="number",
            value=metrics['network_io'] / (1024 * 1024),  # MBë¡œ ë³€í™˜
            title={'text': "Network I/O (MB)"},
            domain={'x': [0, 1], 'y': [0, 1]}
        ),
        row=2, col=2
    )
    
    fig.update_layout(height=400, showlegend=False)
    return fig


def main():
    """ë©”ì¸ ëŒ€ì‹œë³´ë“œ í•¨ìˆ˜"""
    st.title("ğŸš€ Trading ML Dashboard")
    st.markdown("---")
    
    # ëŒ€ì‹œë³´ë“œ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    dashboard = MLDashboard()
    
    # ì‚¬ì´ë“œë°”
    st.sidebar.title("ğŸ“Š Dashboard Controls")
    
    # ìë™ ìƒˆë¡œê³ ì¹¨ ì„¤ì •
    auto_refresh = st.sidebar.checkbox("Auto Refresh", value=True)
    refresh_interval = st.sidebar.slider("Refresh Interval (seconds)", 5, 60, 30)
    
    if auto_refresh:
        time.sleep(refresh_interval)
        st.experimental_rerun()
    
    # ë©”ì¸ ì»¨í…Œì´ë„ˆ
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸ“ˆ Model Performance")
        
        # ëª¨ë¸ ì„±ëŠ¥ ë°ì´í„° ë¡œë“œ
        performance_df = dashboard.get_model_performance()
        
        if not performance_df.empty:
            # ì„±ëŠ¥ ì°¨íŠ¸ í‘œì‹œ
            performance_chart = create_performance_chart(performance_df)
            st.plotly_chart(performance_chart, use_container_width=True)
            
            # ì„±ëŠ¥ í…Œì´ë¸”
            st.subheader("ğŸ“‹ Performance Details")
            st.dataframe(performance_df, use_container_width=True)
        else:
            st.warning("No model performance data available")
    
    with col2:
        st.subheader("âš™ï¸ System Monitoring")
        
        # ì‹œìŠ¤í…œ ë©”íŠ¸ë¦­
        system_metrics = dashboard.get_system_metrics()
        monitoring_chart = create_system_monitoring_chart(system_metrics)
        st.plotly_chart(monitoring_chart, use_container_width=True)
    
    # ë‘ ë²ˆì§¸ í–‰
    col3, col4 = st.columns(2)
    
    with col3:
        st.subheader("ğŸ”„ Training Progress")
        
        # í•™ìŠµ ì§„í–‰ìƒí™©
        training_progress = dashboard.get_training_progress()
        
        if training_progress:
            st.metric("DAG Status", training_progress.get('dag_status', 'Unknown'))
            st.metric("Total Models", training_progress.get('model_count', 0))
            st.metric("Deployed Models", training_progress.get('deployed_count', 0))
            
            if training_progress.get('latest_model'):
                st.info(f"Latest Model: {training_progress['latest_model']}")
            
            if training_progress.get('last_training'):
                last_training = datetime.fromtimestamp(training_progress['last_training'])
                st.info(f"Last Training: {last_training.strftime('%Y-%m-%d %H:%M:%S')}")
        else:
            st.warning("No training progress data available")
    
    with col4:
        st.subheader("ğŸ“Š Data Metrics")
        
        # ë°ì´í„° ë©”íŠ¸ë¦­
        data_metrics = dashboard.get_data_metrics()
        
        if data_metrics:
            st.metric("Total Files", data_metrics.get('total_files', 0))
            st.metric("Total Size (MB)", f"{data_metrics.get('total_size_mb', 0):.2f}")
            st.metric("Latest Rows", data_metrics.get('latest_rows', 0))
            st.metric("Latest Columns", data_metrics.get('latest_columns', 0))
            
            if data_metrics.get('latest_file'):
                st.info(f"Latest File: {data_metrics['latest_file']}")
            
            if data_metrics.get('latest_update'):
                st.info(f"Last Update: {data_metrics['latest_update'].strftime('%Y-%m-%d %H:%M:%S')}")
        else:
            st.warning("No data metrics available")
    
    # ì„¸ ë²ˆì§¸ í–‰
    st.subheader("ğŸ”¬ MLflow Experiments")
    
    # MLflow ì‹¤í—˜ ëª©ë¡
    experiments = dashboard.get_mlflow_experiments()
    
    if experiments:
        exp_df = pd.DataFrame(experiments)
        st.dataframe(exp_df, use_container_width=True)
        
        # ì‹¤í—˜ë³„ ì°¨íŠ¸
        if not exp_df.empty:
            fig = px.scatter(
                exp_df,
                x='start_time',
                y='name',
                color='status',
                title='Experiment Timeline'
            )
            st.plotly_chart(fig, use_container_width=True)
    else:
        st.warning("No MLflow experiments found")
    
    # ë„¤ ë²ˆì§¸ í–‰
    col5, col6 = st.columns(2)
    
    with col5:
        st.subheader("ğŸ“ Recent Activities")
        
        # ìµœê·¼ í™œë™ ë¡œê·¸
        activities = [
            {"time": "2025-01-27 14:30:00", "activity": "LSTM model training completed", "status": "âœ…"},
            {"time": "2025-01-27 14:25:00", "activity": "Transformer model training started", "status": "ğŸ”„"},
            {"time": "2025-01-27 14:20:00", "activity": "Data preprocessing completed", "status": "âœ…"},
            {"time": "2025-01-27 14:15:00", "activity": "Hyperparameter optimization started", "status": "ğŸ”„"},
        ]
        
        for activity in activities:
            st.write(f"{activity['status']} {activity['time']} - {activity['activity']}")
    
    with col6:
        st.subheader("ğŸ”” Alerts")
        
        # ì•Œë¦¼ ì‹œìŠ¤í…œ
        alerts = []
        
        # ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ì•Œë¦¼
        if system_metrics['cpu_percent'] > 80:
            alerts.append("âš ï¸ High CPU usage detected")
        
        if system_metrics['memory_percent'] > 80:
            alerts.append("âš ï¸ High memory usage detected")
        
        if system_metrics['disk_percent'] > 90:
            alerts.append("âš ï¸ Low disk space")
        
        # ëª¨ë¸ ì„±ëŠ¥ ì•Œë¦¼
        if not performance_df.empty:
            best_r2 = performance_df['test_r2'].max()
            if best_r2 < 0.5:
                alerts.append("âš ï¸ Low model performance detected")
        
        if alerts:
            for alert in alerts:
                st.error(alert)
        else:
            st.success("âœ… All systems operational")
    
    # í‘¸í„°
    st.markdown("---")
    st.markdown(
        """
        <div style='text-align: center; color: gray;'>
        <p>ğŸ”„ Auto-refresh: {'Enabled' if auto_refresh else 'Disabled'} | 
        ğŸ“Š Last updated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
        </div>
        """,
        unsafe_allow_html=True
    )


if __name__ == "__main__":
    main() 