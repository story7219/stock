#!/usr/bin/env python3
"""
ğŸ—ï¸ ì½”ë“œ êµ¬ì¡° ë¶„ì„ê¸°
í”„ë¡œì íŠ¸ì˜ ì½”ë“œ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ê³  ì•„í‚¤í…ì²˜ í’ˆì§ˆì„ í‰ê°€í•˜ëŠ” ë„êµ¬
"""

import os
import sys
import json
import ast
import logging
from pathlib import Path
from typing import Dict, List, Any, Set, Tuple
from datetime import datetime
from collections import defaultdict, Counter
import re

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class CodeStructureAnalyzer:
    """ì½”ë“œ êµ¬ì¡° ë¶„ì„ê¸°"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.analysis_result = {
            'timestamp': datetime.now().isoformat(),
            'project_root': str(self.project_root),
            'modules': {},
            'dependencies': {},
            'metrics': {},
            'issues': [],
            'recommendations': []
        }
        
        logger.info(f"ğŸ—ï¸ ì½”ë“œ êµ¬ì¡° ë¶„ì„ê¸° ì´ˆê¸°í™” (í”„ë¡œì íŠ¸: {self.project_root})")
    
    def analyze_file_structure(self) -> Dict[str, Any]:
        """íŒŒì¼ êµ¬ì¡° ë¶„ì„"""
        logger.info("ğŸ“ íŒŒì¼ êµ¬ì¡° ë¶„ì„ ì¤‘...")
        
        structure = {
            'total_files': 0,
            'python_files': 0,
            'test_files': 0,
            'config_files': 0,
            'documentation_files': 0,
            'directories': [],
            'file_types': Counter(),
            'largest_files': [],
            'empty_files': []
        }
        
        for file_path in self.project_root.rglob('*'):
            if file_path.is_file():
                structure['total_files'] += 1
                
                # íŒŒì¼ í™•ì¥ìë³„ ë¶„ë¥˜
                suffix = file_path.suffix.lower()
                structure['file_types'][suffix] += 1
                
                # íŒŒì¼ í¬ê¸° í™•ì¸
                file_size = file_path.stat().st_size
                
                if suffix == '.py':
                    structure['python_files'] += 1
                    
                    # í…ŒìŠ¤íŠ¸ íŒŒì¼ í™•ì¸
                    if 'test' in file_path.name.lower() or file_path.parent.name == 'tests':
                        structure['test_files'] += 1
                    
                    # ë¹ˆ íŒŒì¼ í™•ì¸
                    if file_size == 0:
                        structure['empty_files'].append(str(file_path.relative_to(self.project_root)))
                    
                    # í° íŒŒì¼ ì¶”ì  (ìƒìœ„ 10ê°œ)
                    structure['largest_files'].append({
                        'file': str(file_path.relative_to(self.project_root)),
                        'size': file_size,
                        'lines': self._count_lines(file_path)
                    })
                
                elif suffix in ['.ini', '.conf', '.cfg', '.json', '.yaml', '.yml', '.toml']:
                    structure['config_files'] += 1
                
                elif suffix in ['.md', '.rst', '.txt']:
                    structure['documentation_files'] += 1
            
            elif file_path.is_dir():
                structure['directories'].append(str(file_path.relative_to(self.project_root)))
        
        # í° íŒŒì¼ ì •ë ¬ (ìƒìœ„ 10ê°œë§Œ)
        structure['largest_files'] = sorted(
            structure['largest_files'], 
            key=lambda x: x['size'], 
            reverse=True
        )[:10]
        
        return structure
    
    def _count_lines(self, file_path: Path) -> int:
        """íŒŒì¼ì˜ ë¼ì¸ ìˆ˜ ê³„ì‚°"""
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                return len(f.readlines())
        except Exception:
            return 0
    
    def analyze_module_dependencies(self) -> Dict[str, Any]:
        """ëª¨ë“ˆ ì˜ì¡´ì„± ë¶„ì„"""
        logger.info("ğŸ”— ëª¨ë“ˆ ì˜ì¡´ì„± ë¶„ì„ ì¤‘...")
        
        dependencies = {
            'imports': defaultdict(set),
            'internal_dependencies': defaultdict(set),
            'external_dependencies': set(),
            'circular_dependencies': [],
            'unused_imports': [],
            'dependency_graph': {}
        }
        
        # ëª¨ë“  Python íŒŒì¼ ë¶„ì„
        for py_file in self.project_root.rglob('*.py'):
            if py_file.is_file():
                module_name = self._get_module_name(py_file)
                file_imports = self._analyze_imports(py_file)
                
                dependencies['imports'][module_name] = file_imports['all_imports']
                dependencies['external_dependencies'].update(file_imports['external'])
                dependencies['internal_dependencies'][module_name] = file_imports['internal']
                
                # ì˜ì¡´ì„± ê·¸ë˜í”„ êµ¬ì¶•
                dependencies['dependency_graph'][module_name] = list(file_imports['internal'])
        
        # ìˆœí™˜ ì˜ì¡´ì„± ê²€ì‚¬
        dependencies['circular_dependencies'] = self._find_circular_dependencies(
            dependencies['dependency_graph']
        )
        
        return dependencies
    
    def _get_module_name(self, file_path: Path) -> str:
        """íŒŒì¼ ê²½ë¡œì—ì„œ ëª¨ë“ˆëª… ì¶”ì¶œ"""
        relative_path = file_path.relative_to(self.project_root)
        module_parts = list(relative_path.parts[:-1]) + [relative_path.stem]
        return '.'.join(module_parts).replace('__init__', '')
    
    def _analyze_imports(self, file_path: Path) -> Dict[str, Set[str]]:
        """íŒŒì¼ì˜ import ë¶„ì„"""
        imports = {
            'all_imports': set(),
            'internal': set(),
            'external': set(),
            'unused': set()
        }
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            tree = ast.parse(content)
            
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        import_name = alias.name
                        imports['all_imports'].add(import_name)
                        
                        if self._is_internal_module(import_name):
                            imports['internal'].add(import_name)
                        else:
                            imports['external'].add(import_name)
                
                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        module_name = node.module
                        imports['all_imports'].add(module_name)
                        
                        if self._is_internal_module(module_name):
                            imports['internal'].add(module_name)
                        else:
                            imports['external'].add(module_name)
                        
                        # from ì ˆì˜ ê°œë³„ importë“¤ë„ ì¶”ê°€
                        for alias in node.names:
                            full_name = f"{module_name}.{alias.name}"
                            imports['all_imports'].add(full_name)
        
        except Exception as e:
            logger.warning(f"âš ï¸ Import ë¶„ì„ ì˜¤ë¥˜ {file_path}: {e}")
        
        return imports
    
    def _is_internal_module(self, module_name: str) -> bool:
        """ë‚´ë¶€ ëª¨ë“ˆì¸ì§€ í™•ì¸"""
        # í”„ë¡œì íŠ¸ ë‚´ë¶€ ëª¨ë“ˆ íŒ¨í„´
        internal_patterns = ['src', 'modules', 'core', 'utils', 'config']
        
        for pattern in internal_patterns:
            if module_name.startswith(pattern):
                return True
        
        # ìƒëŒ€ ê²½ë¡œ import
        if module_name.startswith('.'):
            return True
        
        # í”„ë¡œì íŠ¸ ë£¨íŠ¸ì— í•´ë‹¹ ëª¨ë“ˆ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
        module_path = self.project_root / f"{module_name.replace('.', '/')}.py"
        if module_path.exists():
            return True
        
        return False
    
    def _find_circular_dependencies(self, dependency_graph: Dict[str, List[str]]) -> List[List[str]]:
        """ìˆœí™˜ ì˜ì¡´ì„± ì°¾ê¸°"""
        circular_deps = []
        visited = set()
        rec_stack = set()
        
        def dfs(node: str, path: List[str]) -> bool:
            if node in rec_stack:
                # ìˆœí™˜ ë°œê²¬
                cycle_start = path.index(node)
                circular_deps.append(path[cycle_start:] + [node])
                return True
            
            if node in visited:
                return False
            
            visited.add(node)
            rec_stack.add(node)
            
            for neighbor in dependency_graph.get(node, []):
                if dfs(neighbor, path + [node]):
                    return True
            
            rec_stack.remove(node)
            return False
        
        for node in dependency_graph:
            if node not in visited:
                dfs(node, [])
        
        return circular_deps
    
    def analyze_code_complexity(self) -> Dict[str, Any]:
        """ì½”ë“œ ë³µì¡ë„ ë¶„ì„"""
        logger.info("ğŸ§® ì½”ë“œ ë³µì¡ë„ ë¶„ì„ ì¤‘...")
        
        complexity = {
            'total_lines': 0,
            'total_functions': 0,
            'total_classes': 0,
            'average_function_length': 0,
            'average_class_length': 0,
            'cyclomatic_complexity': {},
            'complex_functions': [],
            'large_classes': [],
            'long_functions': []
        }
        
        all_function_lengths = []
        all_class_lengths = []
        
        for py_file in self.project_root.rglob('*.py'):
            if py_file.is_file():
                file_analysis = self._analyze_file_complexity(py_file)
                
                complexity['total_lines'] += file_analysis['lines']
                complexity['total_functions'] += file_analysis['functions']
                complexity['total_classes'] += file_analysis['classes']
                
                all_function_lengths.extend(file_analysis['function_lengths'])
                all_class_lengths.extend(file_analysis['class_lengths'])
                
                # ë³µì¡í•œ í•¨ìˆ˜ë“¤ ì¶”ê°€
                complexity['complex_functions'].extend(file_analysis['complex_functions'])
                complexity['large_classes'].extend(file_analysis['large_classes'])
                complexity['long_functions'].extend(file_analysis['long_functions'])
        
        # í‰ê·  ê³„ì‚°
        if all_function_lengths:
            complexity['average_function_length'] = sum(all_function_lengths) / len(all_function_lengths)
        
        if all_class_lengths:
            complexity['average_class_length'] = sum(all_class_lengths) / len(all_class_lengths)
        
        # ìƒìœ„ ë³µì¡ë„ í•¨ìˆ˜ë“¤ë§Œ ìœ ì§€
        complexity['complex_functions'] = sorted(
            complexity['complex_functions'], 
            key=lambda x: x['complexity'], 
            reverse=True
        )[:20]
        
        complexity['large_classes'] = sorted(
            complexity['large_classes'], 
            key=lambda x: x['lines'], 
            reverse=True
        )[:10]
        
        complexity['long_functions'] = sorted(
            complexity['long_functions'], 
            key=lambda x: x['lines'], 
            reverse=True
        )[:10]
        
        return complexity
    
    def _analyze_file_complexity(self, file_path: Path) -> Dict[str, Any]:
        """ê°œë³„ íŒŒì¼ ë³µì¡ë„ ë¶„ì„"""
        analysis = {
            'lines': 0,
            'functions': 0,
            'classes': 0,
            'function_lengths': [],
            'class_lengths': [],
            'complex_functions': [],
            'large_classes': [],
            'long_functions': []
        }
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                analysis['lines'] = len(content.split('\n'))
            
            tree = ast.parse(content)
            
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    analysis['functions'] += 1
                    func_lines = self._count_node_lines(node)
                    analysis['function_lengths'].append(func_lines)
                    
                    # ê¸´ í•¨ìˆ˜ ì²´í¬ (50ì¤„ ì´ìƒ)
                    if func_lines > 50:
                        analysis['long_functions'].append({
                            'file': str(file_path.relative_to(self.project_root)),
                            'name': node.name,
                            'lines': func_lines,
                            'start_line': node.lineno
                        })
                    
                    # ìˆœí™˜ ë³µì¡ë„ ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)
                    complexity = self._calculate_cyclomatic_complexity(node)
                    if complexity > 10:
                        analysis['complex_functions'].append({
                            'file': str(file_path.relative_to(self.project_root)),
                            'name': node.name,
                            'complexity': complexity,
                            'lines': func_lines,
                            'start_line': node.lineno
                        })
                
                elif isinstance(node, ast.ClassDef):
                    analysis['classes'] += 1
                    class_lines = self._count_node_lines(node)
                    analysis['class_lengths'].append(class_lines)
                    
                    # í° í´ë˜ìŠ¤ ì²´í¬ (200ì¤„ ì´ìƒ)
                    if class_lines > 200:
                        analysis['large_classes'].append({
                            'file': str(file_path.relative_to(self.project_root)),
                            'name': node.name,
                            'lines': class_lines,
                            'start_line': node.lineno,
                            'methods': len([n for n in node.body if isinstance(n, ast.FunctionDef)])
                        })
        
        except Exception as e:
            logger.warning(f"âš ï¸ íŒŒì¼ ë³µì¡ë„ ë¶„ì„ ì˜¤ë¥˜ {file_path}: {e}")
        
        return analysis
    
    def _count_node_lines(self, node: ast.AST) -> int:
        """AST ë…¸ë“œì˜ ë¼ì¸ ìˆ˜ ê³„ì‚°"""
        if hasattr(node, 'end_lineno') and hasattr(node, 'lineno'):
            return node.end_lineno - node.lineno + 1
        return 1
    
    def _calculate_cyclomatic_complexity(self, node: ast.FunctionDef) -> int:
        """ìˆœí™˜ ë³µì¡ë„ ê³„ì‚° (ê°„ë‹¨í•œ ë²„ì „)"""
        complexity = 1  # ê¸°ë³¸ ë³µì¡ë„
        
        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.AsyncFor)):
                complexity += 1
            elif isinstance(child, ast.ExceptHandler):
                complexity += 1
            elif isinstance(child, ast.With, ast.AsyncWith):
                complexity += 1
            elif isinstance(child, ast.BoolOp):
                if isinstance(child.op, (ast.And, ast.Or)):
                    complexity += len(child.values) - 1
        
        return complexity
    
    def analyze_code_quality_patterns(self) -> Dict[str, Any]:
        """ì½”ë“œ í’ˆì§ˆ íŒ¨í„´ ë¶„ì„"""
        logger.info("âœ¨ ì½”ë“œ í’ˆì§ˆ íŒ¨í„´ ë¶„ì„ ì¤‘...")
        
        patterns = {
            'docstring_coverage': 0,
            'type_hint_coverage': 0,
            'test_coverage_estimate': 0,
            'naming_issues': [],
            'code_smells': [],
            'best_practices': {
                'followed': [],
                'violated': []
            }
        }
        
        total_functions = 0
        documented_functions = 0
        type_hinted_functions = 0
        
        for py_file in self.project_root.rglob('*.py'):
            if py_file.is_file():
                file_patterns = self._analyze_file_patterns(py_file)
                
                total_functions += file_patterns['total_functions']
                documented_functions += file_patterns['documented_functions']
                type_hinted_functions += file_patterns['type_hinted_functions']
                
                patterns['naming_issues'].extend(file_patterns['naming_issues'])
                patterns['code_smells'].extend(file_patterns['code_smells'])
        
        # ì»¤ë²„ë¦¬ì§€ ê³„ì‚°
        if total_functions > 0:
            patterns['docstring_coverage'] = (documented_functions / total_functions) * 100
            patterns['type_hint_coverage'] = (type_hinted_functions / total_functions) * 100
        
        # í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ ì¶”ì • (í…ŒìŠ¤íŠ¸ íŒŒì¼ ìˆ˜ ê¸°ë°˜)
        test_files = len(list(self.project_root.rglob('*test*.py')))
        source_files = len(list(self.project_root.rglob('*.py'))) - test_files
        if source_files > 0:
            patterns['test_coverage_estimate'] = min((test_files / source_files) * 100, 100)
        
        return patterns
    
    def _analyze_file_patterns(self, file_path: Path) -> Dict[str, Any]:
        """ê°œë³„ íŒŒì¼ íŒ¨í„´ ë¶„ì„"""
        analysis = {
            'total_functions': 0,
            'documented_functions': 0,
            'type_hinted_functions': 0,
            'naming_issues': [],
            'code_smells': []
        }
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            tree = ast.parse(content)
            
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    analysis['total_functions'] += 1
                    
                    # Docstring ì²´í¬
                    if ast.get_docstring(node):
                        analysis['documented_functions'] += 1
                    
                    # íƒ€ì… íŒíŠ¸ ì²´í¬
                    if node.returns or any(arg.annotation for arg in node.args.args):
                        analysis['type_hinted_functions'] += 1
                    
                    # ë„¤ì´ë° ê·œì¹™ ì²´í¬
                    if not self._is_valid_function_name(node.name):
                        analysis['naming_issues'].append({
                            'file': str(file_path.relative_to(self.project_root)),
                            'type': 'function',
                            'name': node.name,
                            'line': node.lineno,
                            'issue': 'í•¨ìˆ˜ëª…ì´ snake_case ê·œì¹™ì„ ë”°ë¥´ì§€ ì•ŠìŒ'
                        })
                    
                    # í•¨ìˆ˜ ê¸¸ì´ ì²´í¬ (ì½”ë“œ ìŠ¤ë©œ)
                    func_lines = self._count_node_lines(node)
                    if func_lines > 100:
                        analysis['code_smells'].append({
                            'file': str(file_path.relative_to(self.project_root)),
                            'type': 'long_function',
                            'name': node.name,
                            'line': node.lineno,
                            'lines': func_lines,
                            'description': f'í•¨ìˆ˜ê°€ ë„ˆë¬´ ê¹€ ({func_lines}ì¤„)'
                        })
                
                elif isinstance(node, ast.ClassDef):
                    # í´ë˜ìŠ¤ ë„¤ì´ë° ê·œì¹™ ì²´í¬
                    if not self._is_valid_class_name(node.name):
                        analysis['naming_issues'].append({
                            'file': str(file_path.relative_to(self.project_root)),
                            'type': 'class',
                            'name': node.name,
                            'line': node.lineno,
                            'issue': 'í´ë˜ìŠ¤ëª…ì´ PascalCase ê·œì¹™ì„ ë”°ë¥´ì§€ ì•ŠìŒ'
                        })
        
        except Exception as e:
            logger.warning(f"âš ï¸ íŒŒì¼ íŒ¨í„´ ë¶„ì„ ì˜¤ë¥˜ {file_path}: {e}")
        
        return analysis
    
    def _is_valid_function_name(self, name: str) -> bool:
        """í•¨ìˆ˜ëª… ìœ íš¨ì„± ê²€ì‚¬ (snake_case)"""
        return re.match(r'^[a-z_][a-z0-9_]*$', name) is not None
    
    def _is_valid_class_name(self, name: str) -> bool:
        """í´ë˜ìŠ¤ëª… ìœ íš¨ì„± ê²€ì‚¬ (PascalCase)"""
        return re.match(r'^[A-Z][a-zA-Z0-9]*$', name) is not None
    
    def generate_recommendations(self, analysis: Dict[str, Any]) -> List[str]:
        """ë¶„ì„ ê²°ê³¼ ê¸°ë°˜ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # íŒŒì¼ êµ¬ì¡° ê¶Œì¥ì‚¬í•­
        structure = analysis.get('file_structure', {})
        if structure.get('empty_files'):
            recommendations.append(f"ğŸ“ {len(structure['empty_files'])}ê°œì˜ ë¹ˆ íŒŒì¼ì„ ì •ë¦¬í•˜ì„¸ìš”")
        
        if structure.get('python_files', 0) > 0 and structure.get('test_files', 0) == 0:
            recommendations.append("ğŸ§ª í…ŒìŠ¤íŠ¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤. ë‹¨ìœ„ í…ŒìŠ¤íŠ¸ë¥¼ ì¶”ê°€í•˜ì„¸ìš”")
        
        # ì˜ì¡´ì„± ê¶Œì¥ì‚¬í•­
        dependencies = analysis.get('dependencies', {})
        if dependencies.get('circular_dependencies'):
            recommendations.append(f"ğŸ”„ {len(dependencies['circular_dependencies'])}ê°œì˜ ìˆœí™˜ ì˜ì¡´ì„±ì„ í•´ê²°í•˜ì„¸ìš”")
        
        # ë³µì¡ë„ ê¶Œì¥ì‚¬í•­
        complexity = analysis.get('complexity', {})
        if complexity.get('complex_functions'):
            recommendations.append(f"ğŸ§® {len(complexity['complex_functions'])}ê°œì˜ ë³µì¡í•œ í•¨ìˆ˜ë¥¼ ë¦¬íŒ©í† ë§í•˜ì„¸ìš”")
        
        if complexity.get('large_classes'):
            recommendations.append(f"ğŸ“¦ {len(complexity['large_classes'])}ê°œì˜ í° í´ë˜ìŠ¤ë¥¼ ë¶„í• í•˜ì„¸ìš”")
        
        # í’ˆì§ˆ ê¶Œì¥ì‚¬í•­
        quality = analysis.get('quality_patterns', {})
        if quality.get('docstring_coverage', 0) < 70:
            recommendations.append(f"ğŸ“ Docstring ì»¤ë²„ë¦¬ì§€ê°€ ë‚®ìŠµë‹ˆë‹¤ ({quality.get('docstring_coverage', 0):.1f}%)")
        
        if quality.get('type_hint_coverage', 0) < 50:
            recommendations.append(f"ğŸ·ï¸ íƒ€ì… íŒíŠ¸ ì»¤ë²„ë¦¬ì§€ê°€ ë‚®ìŠµë‹ˆë‹¤ ({quality.get('type_hint_coverage', 0):.1f}%)")
        
        return recommendations
    
    def calculate_overall_score(self, analysis: Dict[str, Any]) -> int:
        """ì „ì²´ ì½”ë“œ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        score = 100
        
        # êµ¬ì¡° ì ìˆ˜ (25%)
        structure = analysis.get('file_structure', {})
        if structure.get('empty_files'):
            score -= len(structure['empty_files']) * 2
        
        # ì˜ì¡´ì„± ì ìˆ˜ (25%)
        dependencies = analysis.get('dependencies', {})
        score -= len(dependencies.get('circular_dependencies', [])) * 10
        
        # ë³µì¡ë„ ì ìˆ˜ (25%)
        complexity = analysis.get('complexity', {})
        score -= len(complexity.get('complex_functions', [])) * 2
        score -= len(complexity.get('large_classes', [])) * 5
        
        # í’ˆì§ˆ ì ìˆ˜ (25%)
        quality = analysis.get('quality_patterns', {})
        docstring_penalty = max(0, 70 - quality.get('docstring_coverage', 0)) * 0.3
        type_hint_penalty = max(0, 50 - quality.get('type_hint_coverage', 0)) * 0.2
        score -= docstring_penalty + type_hint_penalty
        
        return max(0, int(score))
    
    def run_analysis(self) -> Dict[str, Any]:
        """ì „ì²´ êµ¬ì¡° ë¶„ì„ ì‹¤í–‰"""
        logger.info("ğŸ—ï¸ ì½”ë“œ êµ¬ì¡° ë¶„ì„ ì‹œì‘")
        
        # ê° ë¶„ì„ ì‹¤í–‰
        self.analysis_result['file_structure'] = self.analyze_file_structure()
        self.analysis_result['dependencies'] = self.analyze_module_dependencies()
        self.analysis_result['complexity'] = self.analyze_code_complexity()
        self.analysis_result['quality_patterns'] = self.analyze_code_quality_patterns()
        
        # ê¶Œì¥ì‚¬í•­ ìƒì„±
        self.analysis_result['recommendations'] = self.generate_recommendations(self.analysis_result)
        
        # ì „ì²´ ì ìˆ˜ ê³„ì‚°
        self.analysis_result['overall_score'] = self.calculate_overall_score(self.analysis_result)
        
        logger.info(f"âœ… ì½”ë“œ êµ¬ì¡° ë¶„ì„ ì™„ë£Œ: ì ìˆ˜ {self.analysis_result['overall_score']}/100")
        
        return self.analysis_result
    
    def save_report(self, output_file: str = "code_structure_report.json"):
        """ë¶„ì„ ê²°ê³¼ ì €ì¥"""
        output_path = self.project_root / output_file
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_result, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"ğŸ“„ êµ¬ì¡° ë¶„ì„ ë³´ê³ ì„œ ì €ì¥: {output_path}")
        
        # ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œë„ ìƒì„±
        self.save_markdown_report(str(output_path).replace('.json', '.md'))
    
    def save_markdown_report(self, output_file: str):
        """ë§ˆí¬ë‹¤ìš´ í˜•ì‹ ë³´ê³ ì„œ ì €ì¥"""
        result = self.analysis_result
        
        md_content = f"""# ğŸ—ï¸ ì½”ë“œ êµ¬ì¡° ë¶„ì„ ë³´ê³ ì„œ

**ìƒì„± ì‹œê°„**: {result['timestamp']}  
**í”„ë¡œì íŠ¸**: {result['project_root']}  
**ì „ì²´ ì ìˆ˜**: {result['overall_score']}/100

## ğŸ“Š ìš”ì•½

### ğŸ“ íŒŒì¼ êµ¬ì¡°
- **ì´ íŒŒì¼**: {result['file_structure']['total_files']}ê°œ
- **Python íŒŒì¼**: {result['file_structure']['python_files']}ê°œ
- **í…ŒìŠ¤íŠ¸ íŒŒì¼**: {result['file_structure']['test_files']}ê°œ
- **ì„¤ì • íŒŒì¼**: {result['file_structure']['config_files']}ê°œ

### ğŸ”— ì˜ì¡´ì„±
- **ì™¸ë¶€ ì˜ì¡´ì„±**: {len(result['dependencies']['external_dependencies'])}ê°œ
- **ìˆœí™˜ ì˜ì¡´ì„±**: {len(result['dependencies']['circular_dependencies'])}ê°œ

### ğŸ§® ë³µì¡ë„
- **ì´ í•¨ìˆ˜**: {result['complexity']['total_functions']}ê°œ
- **ì´ í´ë˜ìŠ¤**: {result['complexity']['total_classes']}ê°œ
- **í‰ê·  í•¨ìˆ˜ ê¸¸ì´**: {result['complexity']['average_function_length']:.1f}ì¤„

### âœ¨ í’ˆì§ˆ
- **Docstring ì»¤ë²„ë¦¬ì§€**: {result['quality_patterns']['docstring_coverage']:.1f}%
- **íƒ€ì… íŒíŠ¸ ì»¤ë²„ë¦¬ì§€**: {result['quality_patterns']['type_hint_coverage']:.1f}%
- **í…ŒìŠ¤íŠ¸ ì»¤ë²„ë¦¬ì§€ ì¶”ì •**: {result['quality_patterns']['test_coverage_estimate']:.1f}%

## ğŸ¯ ì£¼ìš” ê¶Œì¥ì‚¬í•­

"""
        
        for rec in result['recommendations']:
            md_content += f"- {rec}\n"
        
        # ìƒì„¸ ì´ìŠˆë“¤
        if result['complexity']['complex_functions']:
            md_content += "\n## ğŸ§® ë³µì¡í•œ í•¨ìˆ˜ë“¤\n\n"
            for func in result['complexity']['complex_functions'][:10]:
                md_content += f"- **{func['file']}:{func['start_line']}** `{func['name']}()` - ë³µì¡ë„: {func['complexity']}, ê¸¸ì´: {func['lines']}ì¤„\n"
        
        if result['dependencies']['circular_dependencies']:
            md_content += "\n## ğŸ”„ ìˆœí™˜ ì˜ì¡´ì„±\n\n"
            for cycle in result['dependencies']['circular_dependencies']:
                md_content += f"- {' â†’ '.join(cycle)}\n"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        logger.info(f"ğŸ“„ ë§ˆí¬ë‹¤ìš´ ë³´ê³ ì„œ ì €ì¥: {output_file}")

def main():
    """CLI ì§„ì…ì """
    import argparse
    
    parser = argparse.ArgumentParser(description='ì½”ë“œ êµ¬ì¡° ë¶„ì„ ë„êµ¬')
    parser.add_argument('--project-root', default='.', help='í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬')
    parser.add_argument('--output', default='code_structure_report.json', help='ì¶œë ¥ íŒŒì¼ëª…')
    parser.add_argument('--verbose', '-v', action='store_true', help='ìƒì„¸ ë¡œê·¸ ì¶œë ¥')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # êµ¬ì¡° ë¶„ì„ ì‹¤í–‰
    analyzer = CodeStructureAnalyzer(args.project_root)
    result = analyzer.run_analysis()
    analyzer.save_report(args.output)
    
    # ê²°ê³¼ ì¶œë ¥
    print(f"\nğŸ—ï¸ ì½”ë“œ êµ¬ì¡° ë¶„ì„ ì™„ë£Œ!")
    print(f"ğŸ“Š ì „ì²´ ì ìˆ˜: {result['overall_score']}/100")
    print(f"ğŸ“ Python íŒŒì¼: {result['file_structure']['python_files']}ê°œ")
    print(f"ğŸ§® ë³µì¡í•œ í•¨ìˆ˜: {len(result['complexity']['complex_functions'])}ê°œ")
    print(f"ğŸ”„ ìˆœí™˜ ì˜ì¡´ì„±: {len(result['dependencies']['circular_dependencies'])}ê°œ")
    
    if result['overall_score'] < 70:
        print("âš ï¸ ì½”ë“œ êµ¬ì¡° ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤")
        sys.exit(1)
    else:
        print("âœ… ì½”ë“œ êµ¬ì¡°ê°€ ì–‘í˜¸í•©ë‹ˆë‹¤")

if __name__ == "__main__":
    main() 