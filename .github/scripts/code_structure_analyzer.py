#!/usr/bin/env python3
"""
üèóÔ∏è ÏΩîÎìú Íµ¨Ï°∞ Î∂ÑÏÑùÍ∏∞
ÌîÑÎ°úÏ†ùÌä∏Ïùò ÏΩîÎìú Íµ¨Ï°∞Î•º Î∂ÑÏÑùÌïòÍ≥† ÏïÑÌÇ§ÌÖçÏ≤ò ÌíàÏßàÏùÑ ÌèâÍ∞ÄÌïòÎäî ÎèÑÍµ¨
"""

import os
import sys
import json
import ast
import logging
from pathlib import Path
from typing import Dict, List, Any, Set, Tuple
from datetime import datetime
from collections import defaultdict, Counter
import re

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class CodeStructureAnalyzer:
    """ÏΩîÎìú Íµ¨Ï°∞ Î∂ÑÏÑùÍ∏∞"""
    
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.analysis_result = {
            'timestamp': datetime.now().isoformat(),
            'project_root': str(self.project_root),
            'modules': {},
            'dependencies': {},
            'metrics': {},
            'issues': [],
            'recommendations': []
        }
        
        logger.info(f"üèóÔ∏è ÏΩîÎìú Íµ¨Ï°∞ Î∂ÑÏÑùÍ∏∞ Ï¥àÍ∏∞Ìôî (ÌîÑÎ°úÏ†ùÌä∏: {self.project_root})")
    
    def analyze_file_structure(self) -> Dict[str, Any]:
        """ÌååÏùº Íµ¨Ï°∞ Î∂ÑÏÑù"""
        logger.info("üìÅ ÌååÏùº Íµ¨Ï°∞ Î∂ÑÏÑù Ï§ë...")
        
        structure = {
            'total_files': 0,
            'python_files': 0,
            'test_files': 0,
            'config_files': 0,
            'documentation_files': 0,
            'directories': [],
            'file_types': Counter(),
            'largest_files': [],
            'empty_files': []
        }
        
        for file_path in self.project_root.rglob('*'):
            if file_path.is_file():
                structure['total_files'] += 1
                
                # ÌååÏùº ÌôïÏû•ÏûêÎ≥Ñ Î∂ÑÎ•ò
                suffix = file_path.suffix.lower()
                structure['file_types'][suffix] += 1
                
                # ÌååÏùº ÌÅ¨Í∏∞ ÌôïÏù∏
                file_size = file_path.stat().st_size
                
                if suffix == '.py':
                    structure['python_files'] += 1
                    
                    # ÌÖåÏä§Ìä∏ ÌååÏùº ÌôïÏù∏
                    if 'test' in file_path.name.lower() or file_path.parent.name == 'tests':
                        structure['test_files'] += 1
                    
                    # Îπà ÌååÏùº ÌôïÏù∏
                    if file_size == 0:
                        structure['empty_files'].append(str(file_path.relative_to(self.project_root)))
                    
                    # ÌÅ∞ ÌååÏùº Ï∂îÏ†Å (ÏÉÅÏúÑ 10Í∞ú)
                    structure['largest_files'].append({
                        'file': str(file_path.relative_to(self.project_root)),
                        'size': file_size,
                        'lines': self._count_lines(file_path)
                    })
                
                elif suffix in ['.ini', '.conf', '.cfg', '.json', '.yaml', '.yml', '.toml']:
                    structure['config_files'] += 1
                
                elif suffix in ['.md', '.rst', '.txt']:
                    structure['documentation_files'] += 1
            
            elif file_path.is_dir():
                structure['directories'].append(str(file_path.relative_to(self.project_root)))
        
        # ÌÅ∞ ÌååÏùº Ï†ïÎ†¨ (ÏÉÅÏúÑ 10Í∞úÎßå)
        structure['largest_files'] = sorted(
            structure['largest_files'], 
            key=lambda x: x['size'], 
            reverse=True
        )[:10]
        
        return structure
    
    def _count_lines(self, file_path: Path) -> int:
        """ÌååÏùºÏùò ÎùºÏù∏ Ïàò Í≥ÑÏÇ∞"""
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
                return len(f.readlines())
        except Exception:
            return 0
    
    def analyze_module_dependencies(self) -> Dict[str, Any]:
        """Î™®Îìà ÏùòÏ°¥ÏÑ± Î∂ÑÏÑù"""
        logger.info("üîó Î™®Îìà ÏùòÏ°¥ÏÑ± Î∂ÑÏÑù Ï§ë...")
        
        dependencies = {
            'imports': defaultdict(set),
            'internal_dependencies': defaultdict(set),
            'external_dependencies': set(),
            'circular_dependencies': [],
            'unused_imports': [],
            'dependency_graph': {}
        }
        
        # Î™®Îì† Python ÌååÏùº Î∂ÑÏÑù
        for py_file in self.project_root.rglob('*.py'):
            if py_file.is_file():
                module_name = self._get_module_name(py_file)
                file_imports = self._analyze_imports(py_file)
                
                dependencies['imports'][module_name] = file_imports['all_imports']
                dependencies['external_dependencies'].update(file_imports['external'])
                dependencies['internal_dependencies'][module_name] = file_imports['internal']
                
                # ÏùòÏ°¥ÏÑ± Í∑∏ÎûòÌîÑ Íµ¨Ï∂ï
                dependencies['dependency_graph'][module_name] = list(file_imports['internal'])
        
        # ÏàúÌôò ÏùòÏ°¥ÏÑ± Í≤ÄÏÇ¨
        dependencies['circular_dependencies'] = self._find_circular_dependencies(
            dependencies['dependency_graph']
        )
        
        return dependencies
    
    def _get_module_name(self, file_path: Path) -> str:
        """ÌååÏùº Í≤ΩÎ°úÏóêÏÑú Î™®ÎìàÎ™Ö Ï∂îÏ∂ú"""
        relative_path = file_path.relative_to(self.project_root)
        module_parts = list(relative_path.parts[:-1]) + [relative_path.stem]
        return '.'.join(module_parts).replace('__init__', '')
    
    def _analyze_imports(self, file_path: Path) -> Dict[str, Set[str]]:
        """ÌååÏùºÏùò import Î∂ÑÏÑù"""
        imports = {
            'all_imports': set(),
            'internal': set(),
            'external': set(),
            'unused': set()
        }
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            tree = ast.parse(content)
            
            for node in ast.walk(tree):
                if isinstance(node, ast.Import):
                    for alias in node.names:
                        import_name = alias.name
                        imports['all_imports'].add(import_name)
                        
                        if self._is_internal_module(import_name):
                            imports['internal'].add(import_name)
                        else:
                            imports['external'].add(import_name)
                
                elif isinstance(node, ast.ImportFrom):
                    if node.module:
                        module_name = node.module
                        imports['all_imports'].add(module_name)
                        
                        if self._is_internal_module(module_name):
                            imports['internal'].add(module_name)
                        else:
                            imports['external'].add(module_name)
                        
                        # from Ï†àÏùò Í∞úÎ≥Ñ importÎì§ÎèÑ Ï∂îÍ∞Ä
                        for alias in node.names:
                            full_name = f"{module_name}.{alias.name}"
                            imports['all_imports'].add(full_name)
        
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Import Î∂ÑÏÑù Ïò§Î•ò {file_path}: {e}")
        
        return imports
    
    def _is_internal_module(self, module_name: str) -> bool:
        """ÎÇ¥Î∂Ä Î™®ÎìàÏù∏ÏßÄ ÌôïÏù∏"""
        # ÌîÑÎ°úÏ†ùÌä∏ ÎÇ¥Î∂Ä Î™®Îìà Ìå®ÌÑ¥
        internal_patterns = ['src', 'modules', 'core', 'utils', 'config']
        
        for pattern in internal_patterns:
            if module_name.startswith(pattern):
                return True
        
        # ÏÉÅÎåÄ Í≤ΩÎ°ú import
        if module_name.startswith('.'):
            return True
        
        # ÌîÑÎ°úÏ†ùÌä∏ Î£®Ìä∏Ïóê Ìï¥Îãπ Î™®Îìà ÌååÏùºÏù¥ ÏûàÎäîÏßÄ ÌôïÏù∏
        module_path = self.project_root / f"{module_name.replace('.', '/')}.py"
        if module_path.exists():
            return True
        
        return False
    
    def _find_circular_dependencies(self, dependency_graph: Dict[str, List[str]]) -> List[List[str]]:
        """ÏàúÌôò ÏùòÏ°¥ÏÑ± Ï∞æÍ∏∞"""
        circular_deps = []
        visited = set()
        rec_stack = set()
        
        def dfs(node: str, path: List[str]) -> bool:
            if node in rec_stack:
                # ÏàúÌôò Î∞úÍ≤¨
                cycle_start = path.index(node)
                circular_deps.append(path[cycle_start:] + [node])
                return True
            
            if node in visited:
                return False
            
            visited.add(node)
            rec_stack.add(node)
            
            for neighbor in dependency_graph.get(node, []):
                if dfs(neighbor, path + [node]):
                    return True
            
            rec_stack.remove(node)
            return False
        
        for node in dependency_graph:
            if node not in visited:
                dfs(node, [])
        
        return circular_deps
    
    def analyze_code_complexity(self) -> Dict[str, Any]:
        """ÏΩîÎìú Î≥µÏû°ÎèÑ Î∂ÑÏÑù"""
        logger.info("üßÆ ÏΩîÎìú Î≥µÏû°ÎèÑ Î∂ÑÏÑù Ï§ë...")
        
        complexity = {
            'total_lines': 0,
            'total_functions': 0,
            'total_classes': 0,
            'average_function_length': 0,
            'average_class_length': 0,
            'cyclomatic_complexity': {},
            'complex_functions': [],
            'large_classes': [],
            'long_functions': []
        }
        
        all_function_lengths = []
        all_class_lengths = []
        
        for py_file in self.project_root.rglob('*.py'):
            if py_file.is_file():
                file_analysis = self._analyze_file_complexity(py_file)
                
                complexity['total_lines'] += file_analysis['lines']
                complexity['total_functions'] += file_analysis['functions']
                complexity['total_classes'] += file_analysis['classes']
                
                all_function_lengths.extend(file_analysis['function_lengths'])
                all_class_lengths.extend(file_analysis['class_lengths'])
                
                # Î≥µÏû°Ìïú Ìï®ÏàòÎì§ Ï∂îÍ∞Ä
                complexity['complex_functions'].extend(file_analysis['complex_functions'])
                complexity['large_classes'].extend(file_analysis['large_classes'])
                complexity['long_functions'].extend(file_analysis['long_functions'])
        
        # ÌèâÍ∑† Í≥ÑÏÇ∞
        if all_function_lengths:
            complexity['average_function_length'] = sum(all_function_lengths) / len(all_function_lengths)
        
        if all_class_lengths:
            complexity['average_class_length'] = sum(all_class_lengths) / len(all_class_lengths)
        
        # ÏÉÅÏúÑ Î≥µÏû°ÎèÑ Ìï®ÏàòÎì§Îßå Ïú†ÏßÄ
        complexity['complex_functions'] = sorted(
            complexity['complex_functions'], 
            key=lambda x: x['complexity'], 
            reverse=True
        )[:20]
        
        complexity['large_classes'] = sorted(
            complexity['large_classes'], 
            key=lambda x: x['lines'], 
            reverse=True
        )[:10]
        
        complexity['long_functions'] = sorted(
            complexity['long_functions'], 
            key=lambda x: x['lines'], 
            reverse=True
        )[:10]
        
        return complexity
    
    def _analyze_file_complexity(self, file_path: Path) -> Dict[str, Any]:
        """Í∞úÎ≥Ñ ÌååÏùº Î≥µÏû°ÎèÑ Î∂ÑÏÑù"""
        analysis = {
            'lines': 0,
            'functions': 0,
            'classes': 0,
            'function_lengths': [],
            'class_lengths': [],
            'complex_functions': [],
            'large_classes': [],
            'long_functions': []
        }
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
                analysis['lines'] = len(content.split('\n'))
            
            tree = ast.parse(content)
            
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    analysis['functions'] += 1
                    func_lines = self._count_node_lines(node)
                    analysis['function_lengths'].append(func_lines)
                    
                    # Í∏¥ Ìï®Ïàò Ï≤¥ÌÅ¨ (50Ï§Ñ Ïù¥ÏÉÅ)
                    if func_lines > 50:
                        analysis['long_functions'].append({
                            'file': str(file_path.relative_to(self.project_root)),
                            'name': node.name,
                            'lines': func_lines,
                            'start_line': node.lineno
                        })
                    
                    # ÏàúÌôò Î≥µÏû°ÎèÑ Í≥ÑÏÇ∞ (Í∞ÑÎã®Ìïú Î≤ÑÏ†Ñ)
                    complexity = self._calculate_cyclomatic_complexity(node)
                    if complexity > 10:
                        analysis['complex_functions'].append({
                            'file': str(file_path.relative_to(self.project_root)),
                            'name': node.name,
                            'complexity': complexity,
                            'lines': func_lines,
                            'start_line': node.lineno
                        })
                
                elif isinstance(node, ast.ClassDef):
                    analysis['classes'] += 1
                    class_lines = self._count_node_lines(node)
                    analysis['class_lengths'].append(class_lines)
                    
                    # ÌÅ∞ ÌÅ¥ÎûòÏä§ Ï≤¥ÌÅ¨ (200Ï§Ñ Ïù¥ÏÉÅ)
                    if class_lines > 200:
                        analysis['large_classes'].append({
                            'file': str(file_path.relative_to(self.project_root)),
                            'name': node.name,
                            'lines': class_lines,
                            'start_line': node.lineno,
                            'methods': len([n for n in node.body if isinstance(n, ast.FunctionDef)])
                        })
        
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ÌååÏùº Î≥µÏû°ÎèÑ Î∂ÑÏÑù Ïò§Î•ò {file_path}: {e}")
        
        return analysis
    
    def _count_node_lines(self, node: ast.AST) -> int:
        """AST ÎÖ∏ÎìúÏùò ÎùºÏù∏ Ïàò Í≥ÑÏÇ∞"""
        if hasattr(node, 'end_lineno') and hasattr(node, 'lineno'):
            return node.end_lineno - node.lineno + 1
        return 1
    
    def _calculate_cyclomatic_complexity(self, node: ast.FunctionDef) -> int:
        """ÏàúÌôò Î≥µÏû°ÎèÑ Í≥ÑÏÇ∞ (Í∞ÑÎã®Ìïú Î≤ÑÏ†Ñ)"""
        complexity = 1  # Í∏∞Î≥∏ Î≥µÏû°ÎèÑ
        
        for child in ast.walk(node):
            if isinstance(child, (ast.If, ast.While, ast.For, ast.AsyncFor)):
                complexity += 1
            elif isinstance(child, ast.ExceptHandler):
                complexity += 1
            elif isinstance(child, ast.With, ast.AsyncWith):
                complexity += 1
            elif isinstance(child, ast.BoolOp):
                if isinstance(child.op, (ast.And, ast.Or)):
                    complexity += len(child.values) - 1
        
        return complexity
    
    def analyze_code_quality_patterns(self) -> Dict[str, Any]:
        """ÏΩîÎìú ÌíàÏßà Ìå®ÌÑ¥ Î∂ÑÏÑù"""
        logger.info("‚ú® ÏΩîÎìú ÌíàÏßà Ìå®ÌÑ¥ Î∂ÑÏÑù Ï§ë...")
        
        patterns = {
            'docstring_coverage': 0,
            'type_hint_coverage': 0,
            'test_coverage_estimate': 0,
            'naming_issues': [],
            'code_smells': [],
            'best_practices': {
                'followed': [],
                'violated': []
            }
        }
        
        total_functions = 0
        documented_functions = 0
        type_hinted_functions = 0
        
        for py_file in self.project_root.rglob('*.py'):
            if py_file.is_file():
                file_patterns = self._analyze_file_patterns(py_file)
                
                total_functions += file_patterns['total_functions']
                documented_functions += file_patterns['documented_functions']
                type_hinted_functions += file_patterns['type_hinted_functions']
                
                patterns['naming_issues'].extend(file_patterns['naming_issues'])
                patterns['code_smells'].extend(file_patterns['code_smells'])
        
        # Ïª§Î≤ÑÎ¶¨ÏßÄ Í≥ÑÏÇ∞
        if total_functions > 0:
            patterns['docstring_coverage'] = (documented_functions / total_functions) * 100
            patterns['type_hint_coverage'] = (type_hinted_functions / total_functions) * 100
        
        # ÌÖåÏä§Ìä∏ Ïª§Î≤ÑÎ¶¨ÏßÄ Ï∂îÏ†ï (ÌÖåÏä§Ìä∏ ÌååÏùº Ïàò Í∏∞Î∞ò)
        test_files = len(list(self.project_root.rglob('*test*.py')))
        source_files = len(list(self.project_root.rglob('*.py'))) - test_files
        if source_files > 0:
            patterns['test_coverage_estimate'] = min((test_files / source_files) * 100, 100)
        
        return patterns
    
    def _analyze_file_patterns(self, file_path: Path) -> Dict[str, Any]:
        """Í∞úÎ≥Ñ ÌååÏùº Ìå®ÌÑ¥ Î∂ÑÏÑù"""
        analysis = {
            'total_functions': 0,
            'documented_functions': 0,
            'type_hinted_functions': 0,
            'naming_issues': [],
            'code_smells': []
        }
        
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            tree = ast.parse(content)
            
            for node in ast.walk(tree):
                if isinstance(node, ast.FunctionDef):
                    analysis['total_functions'] += 1
                    
                    # Docstring Ï≤¥ÌÅ¨
                    if ast.get_docstring(node):
                        analysis['documented_functions'] += 1
                    
                    # ÌÉÄÏûÖ ÌûåÌä∏ Ï≤¥ÌÅ¨
                    if node.returns or any(arg.annotation for arg in node.args.args):
                        analysis['type_hinted_functions'] += 1
                    
                    # ÎÑ§Ïù¥Î∞ç Í∑úÏπô Ï≤¥ÌÅ¨
                    if not self._is_valid_function_name(node.name):
                        analysis['naming_issues'].append({
                            'file': str(file_path.relative_to(self.project_root)),
                            'type': 'function',
                            'name': node.name,
                            'line': node.lineno,
                            'issue': 'Ìï®ÏàòÎ™ÖÏù¥ snake_case Í∑úÏπôÏùÑ Îî∞Î•¥ÏßÄ ÏïäÏùå'
                        })
                    
                    # Ìï®Ïàò Í∏∏Ïù¥ Ï≤¥ÌÅ¨ (ÏΩîÎìú Ïä§Î©ú)
                    func_lines = self._count_node_lines(node)
                    if func_lines > 100:
                        analysis['code_smells'].append({
                            'file': str(file_path.relative_to(self.project_root)),
                            'type': 'long_function',
                            'name': node.name,
                            'line': node.lineno,
                            'lines': func_lines,
                            'description': f'Ìï®ÏàòÍ∞Ä ÎÑàÎ¨¥ ÍπÄ ({func_lines}Ï§Ñ)'
                        })
                
                elif isinstance(node, ast.ClassDef):
                    # ÌÅ¥ÎûòÏä§ ÎÑ§Ïù¥Î∞ç Í∑úÏπô Ï≤¥ÌÅ¨
                    if not self._is_valid_class_name(node.name):
                        analysis['naming_issues'].append({
                            'file': str(file_path.relative_to(self.project_root)),
                            'type': 'class',
                            'name': node.name,
                            'line': node.lineno,
                            'issue': 'ÌÅ¥ÎûòÏä§Î™ÖÏù¥ PascalCase Í∑úÏπôÏùÑ Îî∞Î•¥ÏßÄ ÏïäÏùå'
                        })
        
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è ÌååÏùº Ìå®ÌÑ¥ Î∂ÑÏÑù Ïò§Î•ò {file_path}: {e}")
        
        return analysis
    
    def _is_valid_function_name(self, name: str) -> bool:
        """Ìï®ÏàòÎ™Ö Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨ (snake_case)"""
        return re.match(r'^[a-z_][a-z0-9_]*$', name) is not None
    
    def _is_valid_class_name(self, name: str) -> bool:
        """ÌÅ¥ÎûòÏä§Î™Ö Ïú†Ìö®ÏÑ± Í≤ÄÏÇ¨ (PascalCase)"""
        return re.match(r'^[A-Z][a-zA-Z0-9]*$', name) is not None
    
    def generate_recommendations(self, analysis: Dict[str, Any]) -> List[str]:
        """Î∂ÑÏÑù Í≤∞Í≥º Í∏∞Î∞ò Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ±"""
        recommendations = []
        
        # ÌååÏùº Íµ¨Ï°∞ Í∂åÏû•ÏÇ¨Ìï≠
        structure = analysis.get('file_structure', {})
        if structure.get('empty_files'):
            recommendations.append(f"üìÅ {len(structure['empty_files'])}Í∞úÏùò Îπà ÌååÏùºÏùÑ Ï†ïÎ¶¨ÌïòÏÑ∏Ïöî")
        
        if structure.get('python_files', 0) > 0 and structure.get('test_files', 0) == 0:
            recommendations.append("üß™ ÌÖåÏä§Ìä∏ ÌååÏùºÏù¥ ÏóÜÏäµÎãàÎã§. Îã®ÏúÑ ÌÖåÏä§Ìä∏Î•º Ï∂îÍ∞ÄÌïòÏÑ∏Ïöî")
        
        # ÏùòÏ°¥ÏÑ± Í∂åÏû•ÏÇ¨Ìï≠
        dependencies = analysis.get('dependencies', {})
        if dependencies.get('circular_dependencies'):
            recommendations.append(f"üîÑ {len(dependencies['circular_dependencies'])}Í∞úÏùò ÏàúÌôò ÏùòÏ°¥ÏÑ±ÏùÑ Ìï¥Í≤∞ÌïòÏÑ∏Ïöî")
        
        # Î≥µÏû°ÎèÑ Í∂åÏû•ÏÇ¨Ìï≠
        complexity = analysis.get('complexity', {})
        if complexity.get('complex_functions'):
            recommendations.append(f"üßÆ {len(complexity['complex_functions'])}Í∞úÏùò Î≥µÏû°Ìïú Ìï®ÏàòÎ•º Î¶¨Ìå©ÌÜ†ÎßÅÌïòÏÑ∏Ïöî")
        
        if complexity.get('large_classes'):
            recommendations.append(f"üì¶ {len(complexity['large_classes'])}Í∞úÏùò ÌÅ∞ ÌÅ¥ÎûòÏä§Î•º Î∂ÑÌï†ÌïòÏÑ∏Ïöî")
        
        # ÌíàÏßà Í∂åÏû•ÏÇ¨Ìï≠
        quality = analysis.get('quality_patterns', {})
        if quality.get('docstring_coverage', 0) < 70:
            recommendations.append(f"üìù Docstring Ïª§Î≤ÑÎ¶¨ÏßÄÍ∞Ä ÎÇÆÏäµÎãàÎã§ ({quality.get('docstring_coverage', 0):.1f}%)")
        
        if quality.get('type_hint_coverage', 0) < 50:
            recommendations.append(f"üè∑Ô∏è ÌÉÄÏûÖ ÌûåÌä∏ Ïª§Î≤ÑÎ¶¨ÏßÄÍ∞Ä ÎÇÆÏäµÎãàÎã§ ({quality.get('type_hint_coverage', 0):.1f}%)")
        
        return recommendations
    
    def calculate_overall_score(self, analysis: Dict[str, Any]) -> int:
        """Ï†ÑÏ≤¥ ÏΩîÎìú ÌíàÏßà Ï†êÏàò Í≥ÑÏÇ∞"""
        score = 100
        
        # Íµ¨Ï°∞ Ï†êÏàò (25%)
        structure = analysis.get('file_structure', {})
        if structure.get('empty_files'):
            score -= len(structure['empty_files']) * 2
        
        # ÏùòÏ°¥ÏÑ± Ï†êÏàò (25%)
        dependencies = analysis.get('dependencies', {})
        score -= len(dependencies.get('circular_dependencies', [])) * 10
        
        # Î≥µÏû°ÎèÑ Ï†êÏàò (25%)
        complexity = analysis.get('complexity', {})
        score -= len(complexity.get('complex_functions', [])) * 2
        score -= len(complexity.get('large_classes', [])) * 5
        
        # ÌíàÏßà Ï†êÏàò (25%)
        quality = analysis.get('quality_patterns', {})
        docstring_penalty = max(0, 70 - quality.get('docstring_coverage', 0)) * 0.3
        type_hint_penalty = max(0, 50 - quality.get('type_hint_coverage', 0)) * 0.2
        score -= docstring_penalty + type_hint_penalty
        
        return max(0, int(score))
    
    def run_analysis(self) -> Dict[str, Any]:
        """Ï†ÑÏ≤¥ Íµ¨Ï°∞ Î∂ÑÏÑù Ïã§Ìñâ"""
        logger.info("üèóÔ∏è ÏΩîÎìú Íµ¨Ï°∞ Î∂ÑÏÑù ÏãúÏûë")
        
        # Í∞Å Î∂ÑÏÑù Ïã§Ìñâ
        self.analysis_result['file_structure'] = self.analyze_file_structure()
        self.analysis_result['dependencies'] = self.analyze_module_dependencies()
        self.analysis_result['complexity'] = self.analyze_code_complexity()
        self.analysis_result['quality_patterns'] = self.analyze_code_quality_patterns()
        
        # Í∂åÏû•ÏÇ¨Ìï≠ ÏÉùÏÑ±
        self.analysis_result['recommendations'] = self.generate_recommendations(self.analysis_result)
        
        # Ï†ÑÏ≤¥ Ï†êÏàò Í≥ÑÏÇ∞
        self.analysis_result['overall_score'] = self.calculate_overall_score(self.analysis_result)
        
        logger.info(f"‚úÖ ÏΩîÎìú Íµ¨Ï°∞ Î∂ÑÏÑù ÏôÑÎ£å: Ï†êÏàò {self.analysis_result['overall_score']}/100")
        
        return self.analysis_result
    
    def save_report(self, output_file: str = "code_structure_report.json"):
        """Î∂ÑÏÑù Í≤∞Í≥º Ï†ÄÏû•"""
        output_path = self.project_root / output_file
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.analysis_result, f, indent=2, ensure_ascii=False, default=str)
        
        logger.info(f"üìÑ Íµ¨Ï°∞ Î∂ÑÏÑù Î≥¥Í≥†ÏÑú Ï†ÄÏû•: {output_path}")
        
        # ÎßàÌÅ¨Îã§Ïö¥ Î≥¥Í≥†ÏÑúÎèÑ ÏÉùÏÑ±
        self.save_markdown_report(str(output_path).replace('.json', '.md'))
    
    def save_markdown_report(self, output_file: str):
        """ÎßàÌÅ¨Îã§Ïö¥ ÌòïÏãù Î≥¥Í≥†ÏÑú Ï†ÄÏû•"""
        result = self.analysis_result
        
        md_content = f"""# üèóÔ∏è ÏΩîÎìú Íµ¨Ï°∞ Î∂ÑÏÑù Î≥¥Í≥†ÏÑú

**ÏÉùÏÑ± ÏãúÍ∞Ñ**: {result['timestamp']}  
**ÌîÑÎ°úÏ†ùÌä∏**: {result['project_root']}  
**Ï†ÑÏ≤¥ Ï†êÏàò**: {result['overall_score']}/100

## üìä ÏöîÏïΩ

### üìÅ ÌååÏùº Íµ¨Ï°∞
- **Ï¥ù ÌååÏùº**: {result['file_structure']['total_files']}Í∞ú
- **Python ÌååÏùº**: {result['file_structure']['python_files']}Í∞ú
- **ÌÖåÏä§Ìä∏ ÌååÏùº**: {result['file_structure']['test_files']}Í∞ú
- **ÏÑ§Ï†ï ÌååÏùº**: {result['file_structure']['config_files']}Í∞ú

### üîó ÏùòÏ°¥ÏÑ±
- **Ïô∏Î∂Ä ÏùòÏ°¥ÏÑ±**: {len(result['dependencies']['external_dependencies'])}Í∞ú
- **ÏàúÌôò ÏùòÏ°¥ÏÑ±**: {len(result['dependencies']['circular_dependencies'])}Í∞ú

### üßÆ Î≥µÏû°ÎèÑ
- **Ï¥ù Ìï®Ïàò**: {result['complexity']['total_functions']}Í∞ú
- **Ï¥ù ÌÅ¥ÎûòÏä§**: {result['complexity']['total_classes']}Í∞ú
- **ÌèâÍ∑† Ìï®Ïàò Í∏∏Ïù¥**: {result['complexity']['average_function_length']:.1f}Ï§Ñ

### ‚ú® ÌíàÏßà
- **Docstring Ïª§Î≤ÑÎ¶¨ÏßÄ**: {result['quality_patterns']['docstring_coverage']:.1f}%
- **ÌÉÄÏûÖ ÌûåÌä∏ Ïª§Î≤ÑÎ¶¨ÏßÄ**: {result['quality_patterns']['type_hint_coverage']:.1f}%
- **ÌÖåÏä§Ìä∏ Ïª§Î≤ÑÎ¶¨ÏßÄ Ï∂îÏ†ï**: {result['quality_patterns']['test_coverage_estimate']:.1f}%

## üéØ Ï£ºÏöî Í∂åÏû•ÏÇ¨Ìï≠

"""
        
        for rec in result['recommendations']:
            md_content += f"- {rec}\n"
        
        # ÏÉÅÏÑ∏ Ïù¥ÏäàÎì§
        if result['complexity']['complex_functions']:
            md_content += "\n## üßÆ Î≥µÏû°Ìïú Ìï®ÏàòÎì§\n\n"
            for func in result['complexity']['complex_functions'][:10]:
                md_content += f"- **{func['file']}:{func['start_line']}** `{func['name']}()` - Î≥µÏû°ÎèÑ: {func['complexity']}, Í∏∏Ïù¥: {func['lines']}Ï§Ñ\n"
        
        if result['dependencies']['circular_dependencies']:
            md_content += "\n## üîÑ ÏàúÌôò ÏùòÏ°¥ÏÑ±\n\n"
            for cycle in result['dependencies']['circular_dependencies']:
                md_content += f"- {' ‚Üí '.join(cycle)}\n"
        
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(md_content)
        
        logger.info(f"üìÑ ÎßàÌÅ¨Îã§Ïö¥ Î≥¥Í≥†ÏÑú Ï†ÄÏû•: {output_file}")

def main():
    """CLI ÏßÑÏûÖÏ†ê"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ÏΩîÎìú Íµ¨Ï°∞ Î∂ÑÏÑù ÎèÑÍµ¨')
    parser.add_argument('--project-root', default='.', help='ÌîÑÎ°úÏ†ùÌä∏ Î£®Ìä∏ ÎîîÎ†âÌÜ†Î¶¨')
    parser.add_argument('--output', default='code_structure_report.json', help='Ï∂úÎ†• ÌååÏùºÎ™Ö')
    parser.add_argument('--verbose', '-v', action='store_true', help='ÏÉÅÏÑ∏ Î°úÍ∑∏ Ï∂úÎ†•')
    
    args = parser.parse_args()
    
    if args.verbose:
        logging.getLogger().setLevel(logging.DEBUG)
    
    # Íµ¨Ï°∞ Î∂ÑÏÑù Ïã§Ìñâ
    analyzer = CodeStructureAnalyzer(args.project_root)
    result = analyzer.run_analysis()
    analyzer.save_report(args.output)
    
    # Í≤∞Í≥º Ï∂úÎ†•
    print(f"\nüèóÔ∏è ÏΩîÎìú Íµ¨Ï°∞ Î∂ÑÏÑù ÏôÑÎ£å!")
    print(f"üìä Ï†ÑÏ≤¥ Ï†êÏàò: {result['overall_score']}/100")
    print(f"üìÅ Python ÌååÏùº: {result['file_structure']['python_files']}Í∞ú")
    print(f"üßÆ Î≥µÏû°Ìïú Ìï®Ïàò: {len(result['complexity']['complex_functions'])}Í∞ú")
    print(f"üîÑ ÏàúÌôò ÏùòÏ°¥ÏÑ±: {len(result['dependencies']['circular_dependencies'])}Í∞ú")
    
    if result['overall_score'] < 70:
        print("‚ö†Ô∏è ÏΩîÎìú Íµ¨Ï°∞ Í∞úÏÑ†Ïù¥ ÌïÑÏöîÌï©ÎãàÎã§")
        sys.exit(1)
    else:
        print("‚úÖ ÏΩîÎìú Íµ¨Ï°∞Í∞Ä ÏñëÌò∏Ìï©ÎãàÎã§")

if __name__ == "__main__":
    main() 