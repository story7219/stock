#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íŒŒì¼ëª…: memory_optimizer.py
ëª¨ë“ˆ: ë©”ëª¨ë¦¬ ìµœì í™” ì „ë¬¸ ì‹œìŠ¤í…œ
ëª©ì : ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê·¹í•œ ìµœì í™”, ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ ë°©ì§€, ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìµœì í™”

Author: World-Class AI Trading System
Created: 2025-01-27
Version: 1.0.0

ìµœì í™” ê¸°ë²•:
ğŸ§  ë©”ëª¨ë¦¬ ê´€ë¦¬:
- ë©”ëª¨ë¦¬ ë§¤í•‘ íŒŒì¼ I/O
- ì²­í¬ ë‹¨ìœ„ ìŠ¤íŠ¸ë¦¬ë° ì²˜ë¦¬
- ë©”ëª¨ë¦¬ í’€ ì¬ì‚¬ìš©
- ì•½í•œ ì°¸ì¡° í™œìš©

âš¡ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜:
- ì„¸ëŒ€ë³„ GC ìµœì í™”
- ìˆ˜ë™ GC íŠ¸ë¦¬ê±°
- ìˆœí™˜ ì°¸ì¡° ë°©ì§€
- ë©”ëª¨ë¦¬ ì„ê³„ê°’ ê´€ë¦¬

ğŸ“Š ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§:
- ì‹¤ì‹œê°„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¶”ì 
- ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ íƒì§€
- ê°ì²´ ìˆ˜ëª… ê´€ë¦¬
- ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§

ğŸ”§ ìµœì í™” ì „ëµ:
- ë°ì´í„° êµ¬ì¡° ìµœì í™”
- ë©”ëª¨ë¦¬ ì •ë ¬ ìµœì í™”
- ìºì‹œ ì¹œí™”ì  ì•Œê³ ë¦¬ì¦˜
- ë©”ëª¨ë¦¬ ì••ì¶• ê¸°ë²•

License: MIT
"""

from __future__ import annotations
import gc
import mmap
import os
import sys
import time
import threading
import weakref
from contextlib import contextmanager
from dataclasses import dataclass, field
from pathlib import Path
from typing import Dict, List, Optional, Any, Callable, Union, Iterator
import logging
import psutil
import resource
from collections import defaultdict, deque
from functools import wraps
import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
from memory_profiler import profile
import pympler
from pympler import tracker, muppy, summary
import tracemalloc
import linecache
import pickle
import joblib
import lz4.frame
import zstandard as zstd
from concurrent.futures import ThreadPoolExecutor
import asyncio

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ë©”ëª¨ë¦¬ ì„¤ì •
MEMORY_CONFIG = {
    'max_memory_gb': 16,
    'gc_threshold_ratio': 0.8,
    'chunk_size': 1024 * 1024,  # 1MB ì²­í¬
    'mmap_threshold': 100 * 1024 * 1024,  # 100MB ì´ìƒ ì‹œ mmap ì‚¬ìš©
    'weak_ref_cache_size': 10000,
    'memory_pool_size': 1000,
    'compression_threshold': 50 * 1024 * 1024,  # 50MB ì´ìƒ ì‹œ ì••ì¶•
}

@dataclass
class MemoryStats:
    """ë©”ëª¨ë¦¬ í†µê³„ í´ë˜ìŠ¤"""
    total_memory: int = 0
    available_memory: int = 0
    used_memory: int = 0
    memory_percent: float = 0.0
    process_memory: int = 0
    gc_collections: int = 0
    gc_collected: int = 0
    gc_uncollectable: int = 0
    timestamp: float = field(default_factory=time.time)

class MemoryPool:
    """ë©”ëª¨ë¦¬ í’€ ê´€ë¦¬ì"""

    def __init__(self, pool_size: int = 1000):
        self.pool_size = pool_size
        self.byte_pools = defaultdict(deque)
        self.array_pools = defaultdict(deque)
        self.dataframe_pools = deque()
        self.lock = threading.Lock()
        self.stats = {
            'allocations': 0,
            'deallocations': 0,
            'pool_hits': 0,
            'pool_misses': 0,
        }

    def get_bytes(self, size: int) -> bytearray:
        """ë°”ì´íŠ¸ ë°°ì—´ íšë“"""
        with self.lock:
            pool = self.byte_pools[size]
            if pool:
                self.stats['pool_hits'] += 1
                return pool.popleft()
            else:
                self.stats['pool_misses'] += 1
                self.stats['allocations'] += 1
                return bytearray(size)

    def return_bytes(self, data: bytearray):
        """ë°”ì´íŠ¸ ë°°ì—´ ë°˜í™˜"""
        with self.lock:
            size = len(data)
            pool = self.byte_pools[size]
            if len(pool) < self.pool_size:
                # ë°ì´í„° ì´ˆê¸°í™”
                data[:] = b'\x00' * size
                pool.append(data)
                self.stats['deallocations'] += 1

    def get_array(self, shape: tuple, dtype: np.dtype) -> np.ndarray:
        """NumPy ë°°ì—´ íšë“"""
        with self.lock:
            key = (shape, dtype)
            pool = self.array_pools[key]
            if pool:
                self.stats['pool_hits'] += 1
                return pool.popleft()
            else:
                self.stats['pool_misses'] += 1
                self.stats['allocations'] += 1
                return np.zeros(shape, dtype=dtype)

    def return_array(self, array: np.ndarray):
        """NumPy ë°°ì—´ ë°˜í™˜"""
        with self.lock:
            key = (array.shape, array.dtype)
            pool = self.array_pools[key]
            if len(pool) < self.pool_size:
                # ë°°ì—´ ì´ˆê¸°í™”
                array.fill(0)
                pool.append(array)
                self.stats['deallocations'] += 1

    def get_dataframe(self, rows: int, cols: int) -> pd.DataFrame:
        """DataFrame íšë“"""
        with self.lock:
            if self.dataframe_pools:
                df = self.dataframe_pools.popleft()
                # í¬ê¸° ì¡°ì •
                if len(df) >= rows and len(df.columns) >= cols:
                    self.stats['pool_hits'] += 1
                    return df.iloc[:rows, :cols].copy()

            self.stats['pool_misses'] += 1
            self.stats['allocations'] += 1
            return pd.DataFrame(np.zeros((rows, cols)))

    def return_dataframe(self, df: pd.DataFrame):
        """DataFrame ë°˜í™˜"""
        with self.lock:
            if len(self.dataframe_pools) < self.pool_size:
                # ë°ì´í„° ì´ˆê¸°í™”
                df.iloc[:, :] = 0
                self.dataframe_pools.append(df)
                self.stats['deallocations'] += 1

    def get_stats(self) -> Dict[str, int]:
        """í’€ í†µê³„ ë°˜í™˜"""
        with self.lock:
            return self.stats.copy()

class WeakRefCache:
    """ì•½í•œ ì°¸ì¡° ìºì‹œ"""

    def __init__(self, max_size: int = 10000):
        self.max_size = max_size
        self.cache = {}
        self.access_order = deque()
        self.lock = threading.Lock()
        self.stats = {
            'hits': 0,
            'misses': 0,
            'evictions': 0,
        }

    def get(self, key: str) -> Optional[Any]:
        """ìºì‹œì—ì„œ ê°ì²´ ì¡°íšŒ"""
        with self.lock:
            if key in self.cache:
                weak_ref = self.cache[key]
                obj = weak_ref()
                if obj is not None:
                    # ì•¡ì„¸ìŠ¤ ìˆœì„œ ì—…ë°ì´íŠ¸
                    self.access_order.remove(key)
                    self.access_order.append(key)
                    self.stats['hits'] += 1
                    return obj
                else:
                    # ê°ì²´ê°€ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ë¨
                    del self.cache[key]

            self.stats['misses'] += 1
            return None

    def set(self, key: str, obj: Any):
        """ìºì‹œì— ê°ì²´ ì €ì¥"""
        with self.lock:
            # í¬ê¸° ì œí•œ í™•ì¸
            if len(self.cache) >= self.max_size:
                self._evict_oldest()

            # ì•½í•œ ì°¸ì¡°ë¡œ ì €ì¥
            def cleanup_callback(weak_ref):
                with self.lock:
                    if key in self.cache and self.cache[key] is weak_ref:
                        del self.cache[key]
                        if key in self.access_order:
                            self.access_order.remove(key)

            self.cache[key] = weakref.ref(obj, cleanup_callback)
            self.access_order.append(key)

    def _evict_oldest(self):
        """ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°"""
        if self.access_order:
            oldest_key = self.access_order.popleft()
            if oldest_key in self.cache:
                del self.cache[oldest_key]
                self.stats['evictions'] += 1

    def get_stats(self) -> Dict[str, int]:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        with self.lock:
            return self.stats.copy()

class MemoryMappedFile:
    """ë©”ëª¨ë¦¬ ë§¤í•‘ íŒŒì¼ ê´€ë¦¬ì"""

    def __init__(self, file_path: Union[str, Path], mode: str = 'r+b'):
        self.file_path = Path(file_path)
        self.mode = mode
        self.file_handle = None
        self.mmap_handle = None
        self.size = 0

    def __enter__(self):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        self.file_handle = open(self.file_path, self.mode)
        self.size = os.path.getsize(self.file_path)

        if self.size > 0:
            self.mmap_handle = mmap.mmap(
                self.file_handle.fileno(),
                self.size,
                access=mmap.ACCESS_READ if 'r' in self.mode else mmap.ACCESS_WRITE
            )

        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        if self.mmap_handle:
            self.mmap_handle.close()
        if self.file_handle:
            self.file_handle.close()

    def read_chunk(self, offset: int, size: int) -> bytes:
        """ì²­í¬ ë‹¨ìœ„ ì½ê¸°"""
        if self.mmap_handle:
            return self.mmap_handle[offset:offset + size]
        return b''

    def write_chunk(self, offset: int, data: bytes):
        """ì²­í¬ ë‹¨ìœ„ ì“°ê¸°"""
        if self.mmap_handle and 'w' in self.mode:
            self.mmap_handle[offset:offset + len(data)] = data
            self.mmap_handle.flush()

class ChunkedDataProcessor:
    """ì²­í¬ ë‹¨ìœ„ ë°ì´í„° ì²˜ë¦¬ê¸°"""

    def __init__(self, chunk_size: int = 1024 * 1024):
        self.chunk_size = chunk_size
        self.memory_pool = MemoryPool()
        self.compressor = zstd.ZstdCompressor(level=3)
        self.decompressor = zstd.ZstdDecompressor()

    def process_large_file(self, file_path: Path, processor: Callable) -> Iterator[Any]:
        """ëŒ€ìš©ëŸ‰ íŒŒì¼ ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬"""
        file_size = file_path.stat().st_size

        if file_size > MEMORY_CONFIG['mmap_threshold']:
            # ë©”ëª¨ë¦¬ ë§¤í•‘ ì‚¬ìš©
            with MemoryMappedFile(file_path) as mmap_file:
                offset = 0
                while offset < file_size:
                    chunk_size = min(self.chunk_size, file_size - offset)
                    chunk_data = mmap_file.read_chunk(offset, chunk_size)

                    result = processor(chunk_data)
                    if result is not None:
                        yield result

                    offset += chunk_size
        else:
            # ì¼ë°˜ íŒŒì¼ ì½ê¸°
            with open(file_path, 'rb') as f:
                while True:
                    chunk_data = f.read(self.chunk_size)
                    if not chunk_data:
                        break

                    result = processor(chunk_data)
                    if result is not None:
                        yield result

    def process_dataframe_chunks(self, df: pd.DataFrame, processor: Callable) -> Iterator[pd.DataFrame]:
        """DataFrame ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬"""
        total_rows = len(df)
        rows_per_chunk = self.chunk_size // (df.memory_usage(deep=True).sum() // total_rows)

        for start_idx in range(0, total_rows, rows_per_chunk):
            end_idx = min(start_idx + rows_per_chunk, total_rows)
            chunk_df = df.iloc[start_idx:end_idx].copy()

            result = processor(chunk_df)
            if result is not None:
                yield result

            # ë©”ëª¨ë¦¬ ì •ë¦¬
            del chunk_df
            gc.collect()

    def compress_data(self, data: bytes) -> bytes:
        """ë°ì´í„° ì••ì¶•"""
        if len(data) > MEMORY_CONFIG['compression_threshold']:
            return self.compressor.compress(data)
        return data

    def decompress_data(self, compressed_data: bytes) -> bytes:
        """ë°ì´í„° ì••ì¶• í•´ì œ"""
        try:
            return self.decompressor.decompress(compressed_data)
        except:
            return compressed_data

class GarbageCollectionOptimizer:
    """ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìµœì í™”"""

    def __init__(self):
        self.gc_stats = {
            'collections': [0, 0, 0],
            'collected': [0, 0, 0],
            'uncollectable': [0, 0, 0],
        }
        self.last_gc_time = time.time()
        self.gc_threshold = MEMORY_CONFIG['gc_threshold_ratio']

    def optimize_gc_settings(self):
        """GC ì„¤ì • ìµœì í™”"""
        # GC ì„ê³„ê°’ ì¡°ì •
        gc.set_threshold(700, 10, 10)  # ë” ì ê·¹ì ì¸ GC

        # ë””ë²„ê·¸ í”Œë˜ê·¸ ì„¤ì •
        gc.set_debug(gc.DEBUG_STATS)

        logger.info("ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì„¤ì • ìµœì í™” ì™„ë£Œ")

    def should_collect(self) -> bool:
        """GC ìˆ˜í–‰ ì—¬ë¶€ ê²°ì •"""
        memory_percent = psutil.virtual_memory().percent / 100
        time_since_last_gc = time.time() - self.last_gc_time

        return (memory_percent > self.gc_threshold or
                time_since_last_gc > 60)  # 1ë¶„ë§ˆë‹¤ ë˜ëŠ” ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ

    def collect_garbage(self) -> Dict[str, int]:
        """ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìˆ˜í–‰"""
        start_time = time.time()

        # ì „ì²´ ì„¸ëŒ€ GC ìˆ˜í–‰
        collected = gc.collect()

        # í†µê³„ ì—…ë°ì´íŠ¸
        self.last_gc_time = time.time()
        gc_time = self.last_gc_time - start_time

        # GC í†µê³„ ìˆ˜ì§‘
        stats = gc.get_stats()
        for i, stat in enumerate(stats):
            self.gc_stats['collections'][i] += stat.get('collections', 0)
            self.gc_stats['collected'][i] += stat.get('collected', 0)
            self.gc_stats['uncollectable'][i] += stat.get('uncollectable', 0)

        result = {
            'collected_objects': collected,
            'gc_time_seconds': gc_time,
            'generation_0_collections': self.gc_stats['collections'][0],
            'generation_1_collections': self.gc_stats['collections'][1],
            'generation_2_collections': self.gc_stats['collections'][2],
        }

        logger.info(f"ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì™„ë£Œ: {result}")
        return result

    def find_memory_leaks(self) -> List[Dict[str, Any]]:
        """ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ íƒì§€"""
        # ì°¸ì¡°ë˜ì§€ ì•ŠëŠ” ê°ì²´ ì°¾ê¸°
        unreachable = gc.garbage

        leaks = []
        for obj in unreachable:
            leak_info = {
                'type': type(obj).__name__,
                'id': id(obj),
                'size': sys.getsizeof(obj),
                'referrers': len(gc.get_referrers(obj)),
                'referents': len(gc.get_referents(obj)),
            }
            leaks.append(leak_info)

        return leaks

class MemoryProfiler:
    """ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ëŸ¬"""

    def __init__(self):
        self.tracker = tracker.SummaryTracker()
        self.snapshots = []
        self.start_time = time.time()

    def start_profiling(self):
        """í”„ë¡œíŒŒì¼ë§ ì‹œì‘"""
        tracemalloc.start()
        self.start_time = time.time()
        logger.info("ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§ ì‹œì‘")

    def take_snapshot(self, name: str = None):
        """ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ìƒì„±"""
        if not tracemalloc.is_tracing():
            return

        snapshot = tracemalloc.take_snapshot()
        self.snapshots.append({
            'name': name or f"snapshot_{len(self.snapshots)}",
            'timestamp': time.time(),
            'snapshot': snapshot,
        })

        logger.info(f"ë©”ëª¨ë¦¬ ìŠ¤ëƒ…ìƒ· ìƒì„±: {name}")

    def get_top_memory_usage(self, limit: int = 10) -> List[Dict[str, Any]]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìƒìœ„ í•­ëª© ë°˜í™˜"""
        if not self.snapshots:
            return []

        latest_snapshot = self.snapshots[-1]['snapshot']
        top_stats = latest_snapshot.statistics('lineno')

        memory_usage = []
        for stat in top_stats[:limit]:
            memory_usage.append({
                'filename': stat.traceback.format()[0],
                'size_mb': stat.size / 1024 / 1024,
                'count': stat.count,
                'average_size': stat.size / stat.count if stat.count > 0 else 0,
            })

        return memory_usage

    def compare_snapshots(self, name1: str, name2: str) -> List[Dict[str, Any]]:
        """ìŠ¤ëƒ…ìƒ· ë¹„êµ"""
        snapshot1 = next((s for s in self.snapshots if s['name'] == name1), None)
        snapshot2 = next((s for s in self.snapshots if s['name'] == name2), None)

        if not snapshot1 or not snapshot2:
            return []

        top_stats = snapshot2['snapshot'].compare_to(snapshot1['snapshot'], 'lineno')

        differences = []
        for stat in top_stats[:10]:
            differences.append({
                'filename': stat.traceback.format()[0],
                'size_diff_mb': stat.size_diff / 1024 / 1024,
                'count_diff': stat.count_diff,
                'size_mb': stat.size / 1024 / 1024,
            })

        return differences

    def stop_profiling(self):
        """í”„ë¡œíŒŒì¼ë§ ì¢…ë£Œ"""
        if tracemalloc.is_tracing():
            tracemalloc.stop()
        logger.info("ë©”ëª¨ë¦¬ í”„ë¡œíŒŒì¼ë§ ì¢…ë£Œ")

class MemoryOptimizer:
    """ë©”ëª¨ë¦¬ ìµœì í™” ë©”ì¸ í´ë˜ìŠ¤"""

    def __init__(self):
        self.memory_pool = MemoryPool()
        self.weak_cache = WeakRefCache()
        self.chunked_processor = ChunkedDataProcessor()
        self.gc_optimizer = GarbageCollectionOptimizer()
        self.profiler = MemoryProfiler()

        # ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ
        self.monitoring_thread = None
        self.is_monitoring = False

        # í†µê³„
        self.stats = {
            'optimizations_performed': 0,
            'memory_saved_mb': 0,
            'gc_collections': 0,
            'start_time': time.time(),
        }

    def __enter__(self):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        self.start_optimization()
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        self.stop_optimization()

    def start_optimization(self):
        """ìµœì í™” ì‹œì‘"""
        logger.info("ë©”ëª¨ë¦¬ ìµœì í™” ì‹œì‘")

        # GC ì„¤ì • ìµœì í™”
        self.gc_optimizer.optimize_gc_settings()

        # í”„ë¡œíŒŒì¼ë§ ì‹œì‘
        self.profiler.start_profiling()

        # ëª¨ë‹ˆí„°ë§ ì‹œì‘
        self.is_monitoring = True
        self.monitoring_thread = threading.Thread(target=self._monitor_memory)
        self.monitoring_thread.daemon = True
        self.monitoring_thread.start()

    def stop_optimization(self):
        """ìµœì í™” ì¢…ë£Œ"""
        logger.info("ë©”ëª¨ë¦¬ ìµœì í™” ì¢…ë£Œ")

        # ëª¨ë‹ˆí„°ë§ ì¤‘ì§€
        self.is_monitoring = False
        if self.monitoring_thread:
            self.monitoring_thread.join(timeout=5)

        # í”„ë¡œíŒŒì¼ë§ ì¢…ë£Œ
        self.profiler.stop_profiling()

        # ìµœì¢… GC ìˆ˜í–‰
        self.gc_optimizer.collect_garbage()

    def _monitor_memory(self):
        """ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ìŠ¤ë ˆë“œ"""
        while self.is_monitoring:
            try:
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
                memory_stats = self.get_memory_stats()

                # ì„ê³„ê°’ ì´ˆê³¼ ì‹œ ìµœì í™” ìˆ˜í–‰
                if memory_stats.memory_percent > MEMORY_CONFIG['gc_threshold_ratio']:
                    self.perform_optimization()

                time.sleep(10)  # 10ì´ˆë§ˆë‹¤ í™•ì¸

            except Exception as e:
                logger.error(f"ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ì˜¤ë¥˜: {e}")
                time.sleep(5)

    def get_memory_stats(self) -> MemoryStats:
        """ë©”ëª¨ë¦¬ í†µê³„ ì¡°íšŒ"""
        virtual_memory = psutil.virtual_memory()
        process = psutil.Process()

        return MemoryStats(
            total_memory=virtual_memory.total,
            available_memory=virtual_memory.available,
            used_memory=virtual_memory.used,
            memory_percent=virtual_memory.percent,
            process_memory=process.memory_info().rss,
            gc_collections=sum(self.gc_optimizer.gc_stats['collections']),
            gc_collected=sum(self.gc_optimizer.gc_stats['collected']),
            gc_uncollectable=sum(self.gc_optimizer.gc_stats['uncollectable']),
        )

    def perform_optimization(self):
        """ìµœì í™” ìˆ˜í–‰"""
        logger.info("ë©”ëª¨ë¦¬ ìµœì í™” ìˆ˜í–‰")

        start_memory = psutil.Process().memory_info().rss

        # ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ìˆ˜í–‰
        if self.gc_optimizer.should_collect():
            gc_result = self.gc_optimizer.collect_garbage()
            self.stats['gc_collections'] += 1

        # ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ íƒì§€
        leaks = self.gc_optimizer.find_memory_leaks()
        if leaks:
            logger.warning(f"ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ íƒì§€: {len(leaks)}ê°œ")

        # ìµœì í™” í›„ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰
        end_memory = psutil.Process().memory_info().rss
        memory_saved = (start_memory - end_memory) / 1024 / 1024  # MB

        self.stats['optimizations_performed'] += 1
        self.stats['memory_saved_mb'] += memory_saved

        logger.info(f"ìµœì í™” ì™„ë£Œ: {memory_saved:.2f}MB ì ˆì•½")

    @contextmanager
    def optimized_processing(self, name: str = "processing"):
        """ìµœì í™”ëœ ì²˜ë¦¬ ì»¨í…ìŠ¤íŠ¸"""
        # ì‹œì‘ ìŠ¤ëƒ…ìƒ·
        self.profiler.take_snapshot(f"{name}_start")
        start_memory = psutil.Process().memory_info().rss

        try:
            yield
        finally:
            # ì¢…ë£Œ ìŠ¤ëƒ…ìƒ·
            self.profiler.take_snapshot(f"{name}_end")
            end_memory = psutil.Process().memory_info().rss

            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¶„ì„
            memory_diff = (end_memory - start_memory) / 1024 / 1024
            logger.info(f"{name} ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {memory_diff:.2f}MB")

            # í•„ìš”ì‹œ ìµœì í™” ìˆ˜í–‰
            if memory_diff > 100:  # 100MB ì´ìƒ ì¦ê°€ ì‹œ
                self.perform_optimization()

    def optimize_dataframe(self, df: pd.DataFrame) -> pd.DataFrame:
        """DataFrame ë©”ëª¨ë¦¬ ìµœì í™”"""
        logger.info(f"DataFrame ìµœì í™” ì‹œì‘: {df.memory_usage(deep=True).sum() / 1024 / 1024:.2f}MB")

        # ë°ì´í„° íƒ€ì… ìµœì í™”
        optimized_df = df.copy()

        for col in optimized_df.columns:
            col_type = optimized_df[col].dtype

            if col_type == 'object':
                # ë¬¸ìì—´ ì¹´í…Œê³ ë¦¬í™”
                unique_ratio = optimized_df[col].nunique() / len(optimized_df)
                if unique_ratio < 0.5:  # ê³ ìœ ê°’ì´ 50% ë¯¸ë§Œì´ë©´ ì¹´í…Œê³ ë¦¬í™”
                    optimized_df[col] = optimized_df[col].astype('category')

            elif col_type in ['int64', 'int32']:
                # ì •ìˆ˜ íƒ€ì… ìµœì í™”
                col_min = optimized_df[col].min()
                col_max = optimized_df[col].max()

                if col_min >= 0:
                    if col_max < 255:
                        optimized_df[col] = optimized_df[col].astype('uint8')
                    elif col_max < 65535:
                        optimized_df[col] = optimized_df[col].astype('uint16')
                    elif col_max < 4294967295:
                        optimized_df[col] = optimized_df[col].astype('uint32')
                else:
                    if col_min > -128 and col_max < 127:
                        optimized_df[col] = optimized_df[col].astype('int8')
                    elif col_min > -32768 and col_max < 32767:
                        optimized_df[col] = optimized_df[col].astype('int16')
                    elif col_min > -2147483648 and col_max < 2147483647:
                        optimized_df[col] = optimized_df[col].astype('int32')

            elif col_type == 'float64':
                # ì‹¤ìˆ˜ íƒ€ì… ìµœì í™”
                optimized_df[col] = pd.to_numeric(optimized_df[col], downcast='float')

        optimized_size = optimized_df.memory_usage(deep=True).sum() / 1024 / 1024
        original_size = df.memory_usage(deep=True).sum() / 1024 / 1024
        saved_memory = original_size - optimized_size

        logger.info(f"DataFrame ìµœì í™” ì™„ë£Œ: {saved_memory:.2f}MB ì ˆì•½ ({optimized_size:.2f}MB)")

        return optimized_df

    def get_optimization_stats(self) -> Dict[str, Any]:
        """ìµœì í™” í†µê³„ ë°˜í™˜"""
        runtime = time.time() - self.stats['start_time']
        memory_stats = self.get_memory_stats()

        return {
            'runtime_seconds': runtime,
            'optimizations_performed': self.stats['optimizations_performed'],
            'memory_saved_mb': self.stats['memory_saved_mb'],
            'gc_collections': self.stats['gc_collections'],
            'current_memory_usage_mb': memory_stats.process_memory / 1024 / 1024,
            'memory_usage_percent': memory_stats.memory_percent,
            'pool_stats': self.memory_pool.get_stats(),
            'cache_stats': self.weak_cache.get_stats(),
            'top_memory_usage': self.profiler.get_top_memory_usage(),
        }

# ë©”ëª¨ë¦¬ ìµœì í™” ë°ì½”ë ˆì´í„°
def memory_optimized(func):
    """ë©”ëª¨ë¦¬ ìµœì í™” ë°ì½”ë ˆì´í„°"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        with MemoryOptimizer() as optimizer:
            with optimizer.optimized_processing(func.__name__):
                return func(*args, **kwargs)
    return wrapper

# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    """ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸"""
    try:
        with MemoryOptimizer() as optimizer:

            # ëŒ€ìš©ëŸ‰ ë°ì´í„° ìƒì„±
            logger.info("ëŒ€ìš©ëŸ‰ ë°ì´í„° ìƒì„±")
            large_df = pd.DataFrame({
                'id': range(1000000),
                'value': np.random.randn(1000000),
                'category': np.random.choice(['A', 'B', 'C'], 1000000),
                'timestamp': pd.date_range('2020-01-01', periods=1000000, freq='1min')
            })

            # ë©”ëª¨ë¦¬ ìµœì í™”
            logger.info("DataFrame ë©”ëª¨ë¦¬ ìµœì í™”")
            optimized_df = optimizer.optimize_dataframe(large_df)

            # ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬
            logger.info("ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬")
            chunk_results = []
            for chunk in optimizer.chunked_processor.process_dataframe_chunks(:
                optimized_df, :
                lambda x: x.groupby('category').agg({'value': 'mean'})
            ):
                chunk_results.append(chunk)

            # í†µê³„ ì¶œë ¥
            stats = optimizer.get_optimization_stats()
            logger.info(f"ìµœì í™” í†µê³„: {stats}")

    except Exception as e:
        logger.error(f"ë©”ëª¨ë¦¬ ìµœì í™” í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        raise

if __name__ == "__main__":
    asyncio.run(main())
