#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íŒŒì¼ëª…: krx_ultimate_web_crawler.py
ëª¨ë“ˆ: KRX ì›¹ì‚¬ì´íŠ¸ ì™„ì „ ìë™í™” í¬ë¡¤ë§ ì‹œìŠ¤í…œ
ëª©ì : 45ë…„ê°„ ì£¼ì‹ ë°ì´í„° + 29ë…„ê°„ ì„ ë¬¼/ì˜µì…˜ ë°ì´í„° + 23ë…„ê°„ ETF ë°ì´í„° ì™„ì „ ìˆ˜ì§‘

Author: World-Class Trading AI System
Created: 2025-01-13
Version: 1.0.0

ì£¼ìš” ê¸°ëŠ¥:
ğŸ¯ ë°ì´í„° ìˆ˜ì§‘ ë²”ìœ„:
- ì£¼ì‹ ë°ì´í„°: 1980ë…„ 1ì›” 4ì¼ ~ í˜„ì¬ (45ë…„ê°„)
- ì„ ë¬¼ ë°ì´í„°: 1996ë…„ 5ì›” 3ì¼ ~ í˜„ì¬ (29ë…„ê°„)
- ì˜µì…˜ ë°ì´í„°: 1997ë…„ 7ì›” 7ì¼ ~ í˜„ì¬ (28ë…„ê°„)
- ETF ë°ì´í„°: 2002ë…„ 10ì›” 14ì¼ ~ í˜„ì¬ (23ë…„ê°„)

ğŸš€ ê³ ê¸‰ í¬ë¡¤ë§ ê¸°ëŠ¥:
- OTP í† í° ìë™ ë°œê¸‰ ë° CSV ë‹¤ìš´ë¡œë“œ
- ë¶„í•  ìš”ì²­ (ì—°ë„ë³„/ì›”ë³„/ì¼ë³„ ìë™ ë¶„í• )
- IP ìš°íšŒ (í”„ë¡ì‹œ ë¡œí…Œì´ì…˜, User-Agent ë³€ê²½)
- ë™ì  ë”œë ˆì´ ì¡°ì ˆ (ë´‡ íƒì§€ ìš°íšŒ)
- Selenium ë™ì  í˜ì´ì§€ ì²˜ë¦¬
- ìë™ ì¬ì‹œë„ ë° ì˜¤ë¥˜ ë³µêµ¬
- ì‹¤ì‹œê°„ ì§„í–‰ë¥  ì‹œê°í™”
- ë°ì´í„° í’ˆì§ˆ ê²€ì¦
- ìë™ ë°±ì—… ë° ë³µêµ¬

ğŸ›¡ï¸ ë³´ì•ˆ ë° ì•ˆì •ì„±:
- robots.txt ì¤€ìˆ˜
- ì„œë²„ ë¶€í•˜ ë°©ì§€
- ì˜ˆì™¸ ì²˜ë¦¬ ë° ë¡œê¹…
- ë©”ëª¨ë¦¬ ìµœì í™”
- ë©€í‹°í”„ë¡œì„¸ì‹± ì§€ì›

Performance:
- ì²˜ë¦¬ ì†ë„: 1,000+ ì¢…ëª©/ì‹œê°„
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: < 2GB
- ë™ì‹œ ì—°ê²°: ìµœëŒ€ 20ê°œ
- ì„±ê³µë¥ : 95%+
"""

from __future__ import annotations
import asyncio
import logging
import random
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional, Any, Tuple, Union
import os
import json
import csv
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from contextlib import asynccontextmanager
import warnings
import gc
import psutil
import re
from urllib.parse import urljoin, urlparse, parse_qs
from io import StringIO
import ssl
import socket
from itertools import cycle

import pandas as pd
import numpy as np
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry
import aiohttp
from bs4 import BeautifulSoup
import selenium
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait, Select
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.common.keys import Keys
from selenium.common.exceptions import TimeoutException, NoSuchElementException, WebDriverException
from tqdm.asyncio import tqdm
import cloudscraper
import fake_useragent
from fake_useragent import UserAgent
import diskcache
from scipy import stats
from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
from sklearn.impute import SimpleImputer, KNNImputer
from sklearn.ensemble import IsolationForest
from sklearn.cluster import DBSCAN

# ë¶„ë¦¬ëœ ì „ì²˜ë¦¬ê¸° ì„í¬íŠ¸
from data_engine.processors.ml_preprocessor import MLOptimizedPreprocessor

warnings.filterwarnings('ignore')

# ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
log_dir = Path("../../logs")
log_dir.mkdir(parents=True, exist_ok=True)

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(log_dir / 'krx_ultimate_crawler.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ìºì‹œ ì„¤ì •
CACHE_DIR = Path("../../cache/krx_ultimate")
CACHE_DIR.mkdir(parents=True, exist_ok=True)
cache = diskcache.Cache(str(CACHE_DIR))

# ë°ì´í„° ë””ë ‰í† ë¦¬ ì„¤ì •
DATA_DIR = Path("../../data/krx_ultimate_data")
DATA_DIR.mkdir(parents=True, exist_ok=True)

# ë°±ì—… ë””ë ‰í† ë¦¬ ì„¤ì •
BACKUP_DIR = Path("../../backup/krx_ultimate_data")
BACKUP_DIR.mkdir(parents=True, exist_ok=True)

# KRX ì›¹ì‚¬ì´íŠ¸ URL ì„¤ì •
KRX_BASE_URL = "http://data.krx.co.kr"
KRX_OTP_URL = f"{KRX_BASE_URL}/comm/fileDn/GenerateOTP/generate.cmd"
KRX_DOWNLOAD_URL = f"{KRX_BASE_URL}/comm/fileDn/download_csv/download.cmd"
KRX_STOCK_URL = f"{KRX_BASE_URL}/contents/MDC/MDI/mdiLoader"
KRX_DERIVATIVES_URL = f"{KRX_BASE_URL}/contents/MDC/MDI/mdiLoader"
KRX_ETF_URL = f"{KRX_BASE_URL}/contents/MDC/MDI/mdiLoader"

# í”„ë¡ì‹œ ì„œë²„ ë¦¬ìŠ¤íŠ¸ (ë¬´ë£Œ í”„ë¡ì‹œ ì˜ˆì‹œ)
PROXY_LIST = [
    # ì‹¤ì œ ìš´ì˜ ì‹œì—ëŠ” ìœ ë£Œ í”„ë¡ì‹œ ì„œë¹„ìŠ¤ ì‚¬ìš© ê¶Œì¥
    "http://proxy1.example.com:8080",
    "http://proxy2.example.com:8080",
    "http://proxy3.example.com:8080",
    # ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš© (í”„ë¡ì‹œ ì—†ì´)
    None
]

# User-Agent ë¦¬ìŠ¤íŠ¸
USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
]

# ì‹œì¥ ID ë§µ (ì‹ ê·œ ì¶”ê°€)
MARKET_ID_MAP = {
    'KOSPI': 'STK',
    'KOSDAQ': 'KSQ',
    'KONEX': 'KNX',
    'ETF': 'ETF',
    'FUTURES': 'DRV',
    'OPTIONS': 'DRV',
}

@dataclass
class CrawlConfig:
    """í¬ë¡¤ë§ ì„¤ì • í´ë˜ìŠ¤"""
    # ê¸°ë³¸ ì„¤ì •
    start_date: str = "1980-01-04"
    end_date: str = datetime.now().strftime("%Y-%m-%d")
    max_workers: int = 30  # ë³‘ë ¬ ì›Œì»¤ ìˆ˜ ìƒí–¥
    batch_size: int = 15   # ë°°ì¹˜ í¬ê¸° ìƒí–¥

    # ë¶„í•  ìš”ì²­ ì„¤ì •
    split_by_year: bool = True
    split_by_month: bool = False
    split_by_day: bool = False
    max_days_per_request: int = 365

    # ë”œë ˆì´ ì„¤ì •
    min_delay: float = 1.0
    max_delay: float = 5.0
    request_delay: float = 2.0
    retry_delay: float = 10.0

    # ì¬ì‹œë„ ì„¤ì •
    max_retries: int = 5
    retry_backoff: float = 2.0

    # í”„ë¡ì‹œ ì„¤ì •
    use_proxy: bool = True
    proxy_rotation: bool = True
    proxy_list: List[str] = field(default_factory=lambda: PROXY_LIST)

    # User-Agent ì„¤ì •
    rotate_user_agent: bool = True
    user_agents: List[str] = field(default_factory=lambda: USER_AGENTS)

    # Selenium ì„¤ì •
    use_selenium: bool = True
    headless: bool = True
    page_load_timeout: int = 30
    implicit_wait: int = 10

    # ë°ì´í„° í’ˆì§ˆ ì„¤ì •
    min_data_points: int = 100
    data_quality_threshold: float = 0.8

    # ìºì‹œ ì„¤ì •
    use_cache: bool = True
    cache_expiry: int = 3600  # 1ì‹œê°„

    # ë¡œê¹… ì„¤ì •
    log_level: str = "INFO"
    save_raw_data: bool = True
    save_processed_data: bool = True

@dataclass
class StockInfo:
    """ì£¼ì‹ ì •ë³´ í´ë˜ìŠ¤"""
    code: str
    name: str
    market: str
    sector: str
    data_type: str  # 'STOCK', 'ETF', 'FUTURES', 'OPTIONS'
    start_date: str
    end_date: str
    retry_count: int = 0
    last_error: Optional[str] = None

@dataclass
class CrawlResult:
    """í¬ë¡¤ë§ ê²°ê³¼ í´ë˜ìŠ¤"""
    stock_info: StockInfo
    data: Optional[pd.DataFrame] = None
    success: bool = False
    error_message: Optional[str] = None
    data_points: int = 0
    processing_time: float = 0.0
    quality_score: float = 0.0

class ProxyRotator:
    """í”„ë¡ì‹œ ë¡œí…Œì´ì…˜ í´ë˜ìŠ¤"""

    def __init__(self, proxy_list: List[str]):
        self.proxy_list = [p for p in proxy_list if p is not None]
        self.proxy_cycle = cycle(self.proxy_list) if self.proxy_list else None
        self.current_proxy = None
        self.failed_proxies = set()

    def get_proxy(self) -> Optional[str]:
        """ë‹¤ìŒ í”„ë¡ì‹œ ë°˜í™˜"""
        if not self.proxy_cycle:
            return None

        for _ in range(len(self.proxy_list)):
            proxy = next(self.proxy_cycle)
            if proxy not in self.failed_proxies:
                self.current_proxy = proxy
                return proxy

        # ëª¨ë“  í”„ë¡ì‹œê°€ ì‹¤íŒ¨í•œ ê²½ìš° ì´ˆê¸°í™”
        self.failed_proxies.clear()
        self.current_proxy = next(self.proxy_cycle)
        return self.current_proxy

    def mark_proxy_failed(self, proxy: str):
        """í”„ë¡ì‹œë¥¼ ì‹¤íŒ¨ë¡œ í‘œì‹œ"""
        self.failed_proxies.add(proxy)

    def get_proxy_dict(self) -> Optional[Dict[str, str]]:
        """í”„ë¡ì‹œ ë”•ì…”ë„ˆë¦¬ ë°˜í™˜"""
        proxy = self.get_proxy()
        if proxy:
            return {'http': proxy, 'https': proxy}
        return None

class UserAgentRotator:
    """User-Agent ë¡œí…Œì´ì…˜ í´ë˜ìŠ¤"""

    def __init__(self, user_agents: List[str]):
        self.user_agents = user_agents
        self.ua_cycle = cycle(self.user_agents)
        try:
            self.ua = UserAgent()
        except:
            self.ua = None

    def get_user_agent(self) -> str:
        """ëœë¤ User-Agent ë°˜í™˜"""
        if self.ua:
            try:
                return self.ua.random
            except:
                pass

        return next(self.ua_cycle)

class AdvancedDataCleaner:
    """ê³ ê¸‰ ë°ì´í„° ì •ì œ ë° ì „ì²˜ë¦¬ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.scaler = StandardScaler()
        self.imputer = KNNImputer(n_neighbors=5)
        # IsolationForest ì´ˆê¸°í™” ë¬¸ì œ í•´ê²°
        try:
            self.outlier_detector = IsolationForest(contamination='0.1', random_state=42)
        except:
            self.outlier_detector = None

    def comprehensive_data_cleaning(self, df: pd.DataFrame, stock_info: StockInfo) -> pd.DataFrame:
        """ì¢…í•©ì ì¸ ë°ì´í„° ì •ì œ"""
        if df is None or df.empty:
            logger.warning(f"ë¹ˆ ë°ì´í„°í”„ë ˆì„: {stock_info.code}")
            return pd.DataFrame()
        try:
            logger.info(f"ë°ì´í„° ì •ì œ ì‹œì‘: {stock_info.code} ({len(df)}í–‰)")

            # 1. ê¸°ë³¸ ì •ì œ
            df = self._basic_cleaning(df, stock_info)

            # 2. ê²°ì¸¡ê°’ ì²˜ë¦¬
            df = self._handle_missing_values(df, stock_info)

            # 3. ì´ìƒì¹˜ ì²˜ë¦¬
            df = self._handle_outliers(df, stock_info)

            # 4. ì¤‘ë³µ ë°ì´í„° ì œê±°
            df = self._remove_duplicates(df, stock_info)

            # 5. ë°ì´í„° íƒ€ì… ìµœì í™”
            df = self._optimize_data_types(df, stock_info)

            # 6. ë°ì´í„° ê²€ì¦
            df = self._validate_data(df, stock_info)

            logger.info(f"ë°ì´í„° ì •ì œ ì™„ë£Œ: {stock_info.code} ({len(df)}í–‰)")
            return df

        except Exception as e:
            logger.error(f"ë°ì´í„° ì •ì œ ì‹¤íŒ¨ ({stock_info.code}): {e}")
            return df

    def _basic_cleaning(self, df: pd.DataFrame, stock_info: StockInfo) -> pd.DataFrame:
        if df is None or df.empty:
            return pd.DataFrame()
        try:
            # ì»¬ëŸ¼ëª… í‘œì¤€í™”
            column_mapping = {
                'ì¼ì': 'ë‚ ì§œ', 'ë‚ ì§œ': 'ë‚ ì§œ', 'Date': 'ë‚ ì§œ', 'TRD_DD': 'ë‚ ì§œ',
                'ì¢…ê°€': 'ì¢…ê°€', 'Close': 'ì¢…ê°€', 'CLSPRC': 'ì¢…ê°€',
                'ì‹œê°€': 'ì‹œê°€', 'Open': 'ì‹œê°€', 'OPNPRC': 'ì‹œê°€',
                'ê³ ê°€': 'ê³ ê°€', 'High': 'ê³ ê°€', 'HGPRC': 'ê³ ê°€',
                'ì €ê°€': 'ì €ê°€', 'Low': 'ì €ê°€', 'LWPRC': 'ì €ê°€',
                'ê±°ë˜ëŸ‰': 'ê±°ë˜ëŸ‰', 'Volume': 'ê±°ë˜ëŸ‰', 'ACC_TRDVOL': 'ê±°ë˜ëŸ‰',
                'ê±°ë˜ëŒ€ê¸ˆ': 'ê±°ë˜ëŒ€ê¸ˆ', 'Amount': 'ê±°ë˜ëŒ€ê¸ˆ', 'ACC_TRDVAL': 'ê±°ë˜ëŒ€ê¸ˆ',
                'ì‹œê°€ì´ì•¡': 'ì‹œê°€ì´ì•¡', 'MarketCap': 'ì‹œê°€ì´ì•¡', 'MKTCAP': 'ì‹œê°€ì´ì•¡'
            }

            # ì»¬ëŸ¼ëª… ë³€ê²½
            for old_col, new_col in column_mapping.items():
                if old_col in df.columns:
                    df = df.rename(columns={old_col: new_col})

            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸
            required_columns = ['ë‚ ì§œ', 'ì¢…ê°€']
            available_columns = [col for col in required_columns if col in df.columns]

            if not available_columns:
                logger.warning(f"í•„ìˆ˜ ì»¬ëŸ¼ ì—†ìŒ: {stock_info.code}")
                return pd.DataFrame()

            # ë‚ ì§œ ì»¬ëŸ¼ ì²˜ë¦¬
            if 'ë‚ ì§œ' in df.columns:
                # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
                df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'], errors='coerce', infer_datetime_format=True)

                # ë‚ ì§œê°€ Noneì¸ í–‰ ì œê±°
                df = df.dropna(subset=['ë‚ ì§œ'])

            # ìˆ«ì ì»¬ëŸ¼ ì²˜ë¦¬
            numeric_columns = ['ì‹œê°€', 'ê³ ê°€', 'ì €ê°€', 'ì¢…ê°€', 'ê±°ë˜ëŸ‰', 'ê±°ë˜ëŒ€ê¸ˆ', 'ì‹œê°€ì´ì•¡']
            for col in numeric_columns:
                if col in df.columns:
                    # ë¬¸ìì—´ì„ ìˆ«ìë¡œ ë³€í™˜ (ì‰¼í‘œ, í•˜ì´í”ˆ ì²˜ë¦¬)
                    df[col] = df[col].astype(str).str.replace(',', '').str.replace('-', '0').str.replace('', '0')
                    df[col] = pd.to_numeric(df[col], errors='coerce')

            return df

        except Exception as e:
            logger.error(f"ê¸°ë³¸ ì •ì œ ì‹¤íŒ¨ ({stock_info.code}): {e}")
            return df

    def _handle_missing_values(self, df: pd.DataFrame, stock_info: StockInfo) -> pd.DataFrame:
        if df is None or df.empty:
            return pd.DataFrame()
        try:
            # ê²°ì¸¡ê°’ í˜„í™© ë¶„ì„
            missing_info = df.isnull().sum()
            total_rows = len(df)

            for col, missing_count in missing_info.items():
                if missing_count > 0:
                    missing_ratio = missing_count / total_rows
                    logger.info(f"{stock_info.code} {col} ê²°ì¸¡ê°’: {missing_count}ê°œ ({missing_ratio:.2%})")

            # ê°€ê²© ë°ì´í„° ê²°ì¸¡ê°’ ì²˜ë¦¬
            price_columns = ['ì‹œê°€', 'ê³ ê°€', 'ì €ê°€', 'ì¢…ê°€']
            for col in price_columns:
                if col in df.columns:
                    if df[col].isnull().sum() > 0:
                        # ì „ì¼ ì¢…ê°€ë¡œ ëŒ€ì²´ (Forward Fill)
                        df[col] = df[col].fillna(method='ffill')
                        # ì—¬ì „íˆ ê²°ì¸¡ê°’ì´ ìˆìœ¼ë©´ ë‹¤ìŒì¼ ì¢…ê°€ë¡œ ëŒ€ì²´ (Backward Fill)
                        df[col] = df[col].fillna(method='bfill')
                        # ê·¸ë˜ë„ ê²°ì¸¡ê°’ì´ ìˆìœ¼ë©´ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´
                        df[col] = df[col].fillna(df[col].mean())

            # ê±°ë˜ëŸ‰/ê±°ë˜ëŒ€ê¸ˆ ê²°ì¸¡ê°’ ì²˜ë¦¬
            volume_columns = ['ê±°ë˜ëŸ‰', 'ê±°ë˜ëŒ€ê¸ˆ']
            for col in volume_columns:
                if col in df.columns:
                    if df[col].isnull().sum() > 0:
                        # 0ìœ¼ë¡œ ëŒ€ì²´ (ê±°ë˜ê°€ ì—†ì—ˆë˜ ê²ƒìœ¼ë¡œ ê°„ì£¼)
                        df[col] = df[col].fillna(0)

            # ì‹œê°€ì´ì•¡ ê²°ì¸¡ê°’ ì²˜ë¦¬
            if 'ì‹œê°€ì´ì•¡' in df.columns and 'ì¢…ê°€' in df.columns and 'ê±°ë˜ëŸ‰' in df.columns:
                # ì¢…ê°€ * ìƒì¥ì£¼ì‹ìˆ˜ë¡œ ê³„ì‚° (ê±°ë˜ëŸ‰ ê¸°ë°˜ ì¶”ì •)
                df['ì‹œê°€ì´ì•¡'] = df['ì‹œê°€ì´ì•¡'].fillna(df['ì¢…ê°€'] * df['ê±°ë˜ëŸ‰'] * 1000)

            # ì—¬ì „íˆ ê²°ì¸¡ê°’ì´ ìˆëŠ” í–‰ ì œê±°
            essential_columns = ['ë‚ ì§œ', 'ì¢…ê°€']
            available_essential = [col for col in essential_columns if col in df.columns]
            if available_essential:
                df = df.dropna(subset=available_essential)

            return df

        except Exception as e:
            logger.error(f"ê²°ì¸¡ê°’ ì²˜ë¦¬ ì‹¤íŒ¨ ({stock_info.code}): {e}")
            return df

    def _handle_outliers(self, df: pd.DataFrame, stock_info: StockInfo) -> pd.DataFrame:
        if df is None or df.empty:
            return pd.DataFrame()
        try:
            if df.empty or len(df) < 10:
                return df

            # ê°€ê²© ì´ìƒì¹˜ ì²˜ë¦¬
            price_columns = ['ì‹œê°€', 'ê³ ê°€', 'ì €ê°€', 'ì¢…ê°€']
            for col in price_columns:
                if col in df.columns and df[col].notna().sum() > 0:
                    # IQR ë°©ë²•ìœ¼ë¡œ ì´ìƒì¹˜ íƒì§€
                    Q1 = df[col].quantile(0.25)
                    Q3 = df[col].quantile(0.75)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR

                    # ì´ìƒì¹˜ ê°œìˆ˜ í™•ì¸
                    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]
                    if len(outliers) > 0:
                        logger.info(f"{stock_info.code} {col} ì´ìƒì¹˜: {len(outliers)}ê°œ")

                        # ì´ìƒì¹˜ë¥¼ ê²½ê³„ê°’ìœ¼ë¡œ ëŒ€ì²´ (Winsorization)
                        df.loc[df[col] < lower_bound, col] = lower_bound
                        df.loc[df[col] > upper_bound, col] = upper_bound

            # ê±°ë˜ëŸ‰ ì´ìƒì¹˜ ì²˜ë¦¬
            if 'ê±°ë˜ëŸ‰' in df.columns and df['ê±°ë˜ëŸ‰'].notna().sum() > 0:
                # ê±°ë˜ëŸ‰ì€ 0 ì´ìƒì´ì–´ì•¼ í•¨
                df.loc[df['ê±°ë˜ëŸ‰'] < 0, 'ê±°ë˜ëŸ‰'] = 0

                # ê·¹ë‹¨ì ìœ¼ë¡œ ë†’ì€ ê±°ë˜ëŸ‰ ì²˜ë¦¬
                volume_99th = df['ê±°ë˜ëŸ‰'].quantile(0.99)
                extreme_volume = df['ê±°ë˜ëŸ‰'] > volume_99th * 10
                if extreme_volume.sum() > 0:
                    logger.info(f"{stock_info.code} ê·¹ë‹¨ ê±°ë˜ëŸ‰: {extreme_volume.sum()}ê°œ")
                    df.loc[extreme_volume, 'ê±°ë˜ëŸ‰'] = volume_99th

            # ê°€ê²© ë…¼ë¦¬ ê²€ì¦ (ê³ ê°€ >= ì €ê°€, ì‹œê°€/ì¢…ê°€ëŠ” ê³ ê°€-ì €ê°€ ë²”ìœ„ ë‚´)
            price_check_columns = ['ì‹œê°€', 'ê³ ê°€', 'ì €ê°€', 'ì¢…ê°€']
            if all(col in df.columns for col in price_check_columns):
                # ê³ ê°€ < ì €ê°€ì¸ ê²½ìš° ìˆ˜ì •
                invalid_price = df['ê³ ê°€'] < df['ì €ê°€']
                if invalid_price.sum() > 0:
                    logger.info(f"{stock_info.code} ê°€ê²© ë…¼ë¦¬ ì˜¤ë¥˜: {invalid_price.sum()}ê°œ")
                    # ê³ ê°€ì™€ ì €ê°€ë¥¼ í‰ê· ê°’ìœ¼ë¡œ ëŒ€ì²´
                    avg_price = (df.loc[invalid_price, 'ê³ ê°€'] + df.loc[invalid_price, 'ì €ê°€']) / 2
                    df.loc[invalid_price, 'ê³ ê°€'] = avg_price
                    df.loc[invalid_price, 'ì €ê°€'] = avg_price

                # ì‹œê°€/ì¢…ê°€ê°€ ê³ ê°€-ì €ê°€ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ê²½ìš° ìˆ˜ì •
                for price_col in ['ì‹œê°€', 'ì¢…ê°€']:
                    out_of_range = (df[price_col] < df['ì €ê°€']) | (df[price_col] > df['ê³ ê°€'])
                    if out_of_range.sum() > 0:
                        logger.info(f"{stock_info.code} {price_col} ë²”ìœ„ ì´ˆê³¼: {out_of_range.sum()}ê°œ")
                        # ë²”ìœ„ ë‚´ë¡œ ì¡°ì •
                        df.loc[df[price_col] < df['ì €ê°€'], price_col] = df.loc[df[price_col] < df['ì €ê°€'], 'ì €ê°€']
                        df.loc[df[price_col] > df['ê³ ê°€'], price_col] = df.loc[df[price_col] > df['ê³ ê°€'], 'ê³ ê°€']

            return df

        except Exception as e:
            logger.error(f"ì´ìƒì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨ ({stock_info.code}): {e}")
            return df

    def _remove_duplicates(self, df: pd.DataFrame, stock_info: StockInfo) -> pd.DataFrame:
        if df is None or df.empty:
            return pd.DataFrame()
        try:
            if df.empty:
                return df

            initial_count = len(df)

            # ë‚ ì§œ ê¸°ì¤€ ì¤‘ë³µ ì œê±° (ê°™ì€ ë‚ ì§œì˜ ì—¬ëŸ¬ ë°ì´í„°ê°€ ìˆëŠ” ê²½ìš°)
            if 'ë‚ ì§œ' in df.columns:
                df = df.drop_duplicates(subset=['ë‚ ì§œ'], keep='last')
            else:
                # ë‚ ì§œ ì»¬ëŸ¼ì´ ì—†ìœ¼ë©´ ì „ì²´ í–‰ ê¸°ì¤€ ì¤‘ë³µ ì œê±°
                df = df.drop_duplicates()

            removed_count = initial_count - len(df)
            if removed_count > 0:
                logger.info(f"{stock_info.code} ì¤‘ë³µ ì œê±°: {removed_count}ê°œ")

            return df

        except Exception as e:
            logger.error(f"ì¤‘ë³µ ì œê±° ì‹¤íŒ¨ ({stock_info.code}): {e}")
            return df

    def _optimize_data_types(self, df: pd.DataFrame, stock_info: StockInfo) -> pd.DataFrame:
        if df is None or df.empty:
            return pd.DataFrame()
        try:
            if df.empty:
                return df

            # ì •ìˆ˜í˜• ì»¬ëŸ¼ ìµœì í™”
            int_columns = ['ê±°ë˜ëŸ‰']
            for col in int_columns:
                if col in df.columns:
                    # ê²°ì¸¡ê°’ì´ ì—†ê³  ëª¨ë‘ ì •ìˆ˜ì¸ ê²½ìš°
                    if df[col].notna().all() and df[col].apply(lambda x: float(x).is_integer()).all():
                        df[col] = df[col].astype('int64')

            # ì‹¤ìˆ˜í˜• ì»¬ëŸ¼ ìµœì í™”
            float_columns = ['ì‹œê°€', 'ê³ ê°€', 'ì €ê°€', 'ì¢…ê°€', 'ê±°ë˜ëŒ€ê¸ˆ', 'ì‹œê°€ì´ì•¡']
            for col in float_columns:
                if col in df.columns:
                    # float32ë¡œ ë©”ëª¨ë¦¬ ì ˆì•½ (ì •ë°€ë„ ì¶©ë¶„)
                    df[col] = df[col].astype('float32')

            # ë‚ ì§œ ì»¬ëŸ¼ ìµœì í™”
            if 'ë‚ ì§œ' in df.columns:
                df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])

            return df

        except Exception as e:
            logger.error(f"ë°ì´í„° íƒ€ì… ìµœì í™” ì‹¤íŒ¨ ({stock_info.code}): {e}")
            return df

    def _validate_data(self, df: pd.DataFrame, stock_info: StockInfo) -> pd.DataFrame:
        if df is None or df.empty:
            return pd.DataFrame()
        try:
            if df.empty:
                return df

            # ìµœì†Œ ë°ì´í„° ìš”êµ¬ì‚¬í•­ í™•ì¸
            if len(df) < 10:
                logger.warning(f"{stock_info.code} ë°ì´í„° ë¶€ì¡±: {len(df)}ê°œ")
                return pd.DataFrame()

            # ê°€ê²© ë°ì´í„° ìœ íš¨ì„± ê²€ì¦
            price_columns = ['ì‹œê°€', 'ê³ ê°€', 'ì €ê°€', 'ì¢…ê°€']
            for col in price_columns:
                if col in df.columns:
                    # 0 ì´í•˜ ê°€ê²© ì œê±°
                    invalid_price = df[col] <= 0
                    if invalid_price.any():
                        logger.warning(f"{stock_info.code} {col} ë¬´íš¨ ê°€ê²©: {invalid_price.sum()}ê°œ")
                        df = df[~invalid_price]

            # ë‚ ì§œ ì—°ì†ì„± í™•ì¸
            if 'ë‚ ì§œ' in df.columns and len(df) > 1:
                df = df.sort_values('ë‚ ì§œ').reset_index(drop=True)

                # ë¯¸ë˜ ë‚ ì§œ ì œê±°
                future_dates = df['ë‚ ì§œ'] > datetime.now()
                if future_dates.any():
                    logger.warning(f"{stock_info.code} ë¯¸ë˜ ë‚ ì§œ: {future_dates.sum()}ê°œ")
                    df = df[~future_dates]

            return df

        except Exception as e:
            logger.error(f"ë°ì´í„° ê²€ì¦ ì‹¤íŒ¨ ({stock_info.code}): {e}")
            return df

class DataPreprocessor:
    """ë°ì´í„° ì „ì²˜ë¦¬ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.scaler = RobustScaler()  # ì´ìƒì¹˜ì— ê°•í•œ ìŠ¤ì¼€ì¼ëŸ¬

    def advanced_preprocessing(self, df: pd.DataFrame, stock_info: StockInfo) -> pd.DataFrame:
        """ê³ ê¸‰ ë°ì´í„° ì „ì²˜ë¦¬"""
        if df is None or df.empty:
            return pd.DataFrame()
        try:
            logger.info(f"ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘: {stock_info.code}")

            # 1. ê¸°ìˆ ì  ì§€í‘œ ìƒì„±
            df = self._create_technical_indicators(df, stock_info)

            # 2. ì‹œê°„ ê¸°ë°˜ íŠ¹ì„± ìƒì„±
            df = self._create_temporal_features(df, stock_info)

            # 3. ìˆ˜ìµë¥  ê³„ì‚°
            df = self._calculate_returns(df, stock_info)

            # 4. ë³€ë™ì„± ì§€í‘œ ìƒì„±
            df = self._create_volatility_indicators(df, stock_info)

            # 5. ë°ì´í„° ì •ê·œí™”
            df = self._normalize_features(df, stock_info)

            logger.info(f"ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: {stock_info.code}")
            return df

        except Exception as e:
            logger.error(f"ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤íŒ¨ ({stock_info.code}): {e}")
            return df

    def _create_technical_indicators(self, df: pd.DataFrame, stock_info: StockInfo) -> pd.DataFrame:
        """ê¸°ìˆ ì  ì§€í‘œ ìƒì„±"""
        if df is None or df.empty:
            return pd.DataFrame()
        try:
            if 'ì¢…ê°€' not in df.columns or len(df) < 20:
                return df

            # ì´ë™í‰ê· 
            df['MA5'] = df['ì¢…ê°€'].rolling(window=5).mean()
            df['MA20'] = df['ì¢…ê°€'].rolling(window=20).mean()
            df['MA60'] = df['ì¢…ê°€'].rolling(window=60).mean()

            # RSI (Relative Strength Index)
            if len(df) >= 14:
                delta = df['ì¢…ê°€'].diff()
                gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
                rs = gain / loss
                df['RSI'] = 100 - (100 / (1 + rs))

            # ë³¼ë¦°ì € ë°´ë“œ
            if len(df) >= 20:
                ma20 = df['ì¢…ê°€'].rolling(window=20).mean()
                std20 = df['ì¢…ê°€'].rolling(window=20).std()
                df['BB_Upper'] = ma20 + (std20 * 2)
                df['BB_Lower'] = ma20 - (std20 * 2)
                df['BB_Width'] = df['BB_Upper'] - df['BB_Lower']

            return df

        except Exception as e:
            logger.error(f"ê¸°ìˆ ì  ì§€í‘œ ìƒì„± ì‹¤íŒ¨ ({stock_info.code}): {e}")
            return df

    def _create_temporal_features(self, df: pd.DataFrame, stock_info: StockInfo) -> pd.DataFrame:
        """ì‹œê°„ ê¸°ë°˜ íŠ¹ì„± ìƒì„±"""
        if df is None or df.empty:
            return pd.DataFrame()
        try:
            if 'ë‚ ì§œ' not in df.columns:
                return df

            # ìš”ì¼, ì›”, ë¶„ê¸° ë“±
            df['ìš”ì¼'] = df['ë‚ ì§œ'].dt.dayofweek  # 0=ì›”ìš”ì¼, 6=ì¼ìš”ì¼
            df['ì›”'] = df['ë‚ ì§œ'].dt.month
            df['ë¶„ê¸°'] = df['ë‚ ì§œ'].dt.quarter
            df['ì—°ë„'] = df['ë‚ ì§œ'].dt.year

            # ì›”ë§/ì›”ì´ˆ ì—¬ë¶€
            df['ì›”ë§'] = df['ë‚ ì§œ'].dt.is_month_end
            df['ì›”ì´ˆ'] = df['ë‚ ì§œ'].dt.is_month_start

            return df

        except Exception as e:
            logger.error(f"ì‹œê°„ íŠ¹ì„± ìƒì„± ì‹¤íŒ¨ ({stock_info.code}): {e}")
            return df

    def _calculate_returns(self, df: pd.DataFrame, stock_info: StockInfo) -> pd.DataFrame:
        """ìˆ˜ìµë¥  ê³„ì‚°"""
        if df is None or df.empty:
            return pd.DataFrame()
        try:
            if 'ì¢…ê°€' not in df.columns or len(df) < 2:
                return df

            # ì¼ì¼ ìˆ˜ìµë¥ 
            df['ì¼ì¼ìˆ˜ìµë¥ '] = df['ì¢…ê°€'].pct_change()

            # ëˆ„ì  ìˆ˜ìµë¥ 
            df['ëˆ„ì ìˆ˜ìµë¥ '] = (1 + df['ì¼ì¼ìˆ˜ìµë¥ ']).cumprod() - 1

            # ë¡œê·¸ ìˆ˜ìµë¥ 
            df['ë¡œê·¸ìˆ˜ìµë¥ '] = np.log(df['ì¢…ê°€'] / df['ì¢…ê°€'].shift(1))

            return df

        except Exception as e:
            logger.error(f"ìˆ˜ìµë¥  ê³„ì‚° ì‹¤íŒ¨ ({stock_info.code}): {e}")
            return df

    def _create_volatility_indicators(self, df: pd.DataFrame, stock_info: StockInfo) -> pd.DataFrame:
        """ë³€ë™ì„± ì§€í‘œ ìƒì„±"""
        if df is None or df.empty:
            return pd.DataFrame()
        try:
            if 'ì¼ì¼ìˆ˜ìµë¥ ' not in df.columns or len(df) < 20:
                return df

            # ë³€ë™ì„± (20ì¼ ì´ë™ í‘œì¤€í¸ì°¨)
            df['ë³€ë™ì„±'] = df['ì¼ì¼ìˆ˜ìµë¥ '].rolling(window=20).std()

            # VIX ìŠ¤íƒ€ì¼ ë³€ë™ì„±
            if 'ê³ ê°€' in df.columns and 'ì €ê°€' in df.columns:
                df['ì¼ì¤‘ë³€ë™ì„±'] = (df['ê³ ê°€'] - df['ì €ê°€']) / df['ì¢…ê°€']
                df['í‰ê· ì¼ì¤‘ë³€ë™ì„±'] = df['ì¼ì¤‘ë³€ë™ì„±'].rolling(window=20).mean()

            return df

        except Exception as e:
            logger.error(f"ë³€ë™ì„± ì§€í‘œ ìƒì„± ì‹¤íŒ¨ ({stock_info.code}): {e}")
            return df

    def _normalize_features(self, df: pd.DataFrame, stock_info: StockInfo) -> pd.DataFrame:
        """íŠ¹ì„± ì •ê·œí™”"""
        if df is None or df.empty:
            return pd.DataFrame()
        try:
            # ì •ê·œí™”í•  ì»¬ëŸ¼ ì„ íƒ (ê°€ê²©, ê±°ë˜ëŸ‰ ë“±)
            normalize_columns = ['ê±°ë˜ëŸ‰', 'ê±°ë˜ëŒ€ê¸ˆ', 'ë³€ë™ì„±']
            available_columns = [col for col in normalize_columns if col in df.columns]

            if not available_columns:
                return df

            # ë¡œê·¸ ë³€í™˜ (ê±°ë˜ëŸ‰, ê±°ë˜ëŒ€ê¸ˆ)
            for col in ['ê±°ë˜ëŸ‰', 'ê±°ë˜ëŒ€ê¸ˆ']:
                if col in df.columns:
                    # 0ê°’ ì²˜ë¦¬ í›„ ë¡œê·¸ ë³€í™˜
                    df[f'{col}_log'] = np.log1p(df[col])  # log(1+x)

            return df

        except Exception as e:
            logger.error(f"íŠ¹ì„± ì •ê·œí™” ì‹¤íŒ¨ ({stock_info.code}): {e}")
            return df

class KRXUltimateWebCrawler:
    """KRX ê¶ê·¹ì˜ ì›¹ í¬ë¡¤ë§ ì‹œìŠ¤í…œ"""

    def __init__(self, config: CrawlConfig):
        self.config = config
        self.session = None
        self.driver = None
        self.proxy_rotator = ProxyRotator(config.proxy_list) if config.use_proxy else None
        self.user_agent_rotator = UserAgentRotator(config.user_agents) if config.rotate_user_agent else None
        self.cleaner = AdvancedDataCleaner()
        self.preprocessor = DataPreprocessor()
        # ë¶„ë¦¬ëœ í´ë˜ìŠ¤ì˜ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
        # self.ml_preprocessor = MLOptimizedPreprocessor() # MLOptimizedPreprocessorëŠ” ì´ íŒŒì¼ì— ì—†ì–´ì•¼ í•©ë‹ˆë‹¤.
        self.start_time = time.time()
        self.statistics = {
            "total_requests": 0, "successful_requests": 0, "failed_requests": 0,
            "total_data_points": 0, "total_processing_time": 0.0,
        }

    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        await self.init_session()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        await self.close_session()

    async def init_session(self):
        """ì„¸ì…˜ ì´ˆê¸°í™”"""
        try:
            # cloudscraper ì„¸ì…˜ ìƒì„± (Cloudflare ìš°íšŒ)
            self.session = cloudscraper.create_scraper()

            # requests ì„¸ì…˜ ì„¤ì •
            retry_strategy = Retry(
                total=self.config.max_retries,
                backoff_factor=self.config.retry_backoff,
                status_forcelist=[429, 500, 502, 503, 504]
            )
            adapter = HTTPAdapter(max_retries=retry_strategy)
            self.session.mount("http://", adapter)
            self.session.mount("https://", adapter)

            # Selenium WebDriver ì´ˆê¸°í™”
            if self.config.use_selenium:
                await self.init_webdriver()

            logger.info("KRX ê¶ê·¹ í¬ë¡¤ë§ ì‹œìŠ¤í…œ ì´ˆê¸°í™” ì™„ë£Œ")

        except Exception as e:
            logger.error(f"ì„¸ì…˜ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    async def init_webdriver(self):
        """Selenium WebDriver ì´ˆê¸°í™”"""
        try:
            chrome_options = Options()

            if self.config.headless:
                chrome_options.add_argument('--headless')

            # ë´‡ íƒì§€ ìš°íšŒ ì˜µì…˜
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-blink-features=AutomationControlled')
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--window-size=1920,1080')
            chrome_options.add_argument('--disable-web-security')
            chrome_options.add_argument('--disable-features=VizDisplayCompositor')

            # User-Agent ì„¤ì •
            user_agent = self.user_agent_rotator.get_user_agent() if self.user_agent_rotator else None
            if user_agent:
                chrome_options.add_argument(f'--user-agent={user_agent}')

            # í”„ë¡ì‹œ ì„¤ì •
            if self.proxy_rotator:
                proxy = self.proxy_rotator.get_proxy()
                if proxy:
                    chrome_options.add_argument(f'--proxy-server={proxy}')

            # WebDriver ìƒì„±
            self.driver = webdriver.Chrome(options=chrome_options)
            self.driver.set_page_load_timeout(self.config.page_load_timeout)
            self.driver.implicitly_wait(self.config.implicit_wait)

            # ë´‡ íƒì§€ ìš°íšŒ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")

            logger.info("Selenium WebDriver ì´ˆê¸°í™” ì™„ë£Œ")

        except Exception as e:
            logger.error(f"WebDriver ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.driver = None

    async def close_session(self):
        """ì„¸ì…˜ ì¢…ë£Œ"""
        try:
            if self.session:
                self.session.close()

            if self.driver:
                self.driver.quit()

            logger.info("ì„¸ì…˜ ì¢…ë£Œ ì™„ë£Œ")

        except Exception as e:
            logger.error(f"ì„¸ì…˜ ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")

    def generate_date_ranges(self, start_date: str, end_date: str) -> List[Tuple[str, str]]:
        """ë‚ ì§œ ë²”ìœ„ë¥¼ ë¶„í• í•˜ì—¬ ìƒì„±"""
        try:
            start_dt = datetime.strptime(start_date, "%Y-%m-%d")
            end_dt = datetime.strptime(end_date, "%Y-%m-%d")

            ranges = []
            current_dt = start_dt

            while current_dt < end_dt:
                if self.config.split_by_year:
                    # ì—°ë„ë³„ ë¶„í• 
                    next_dt = datetime(current_dt.year + 1, 1, 1)
                elif self.config.split_by_month:
                    # ì›”ë³„ ë¶„í• 
                    if current_dt.month == 12:
                        next_dt = datetime(current_dt.year + 1, 1, 1)
                    else:
                        next_dt = datetime(current_dt.year, current_dt.month + 1, 1)
                else:
                    # ì¼ë³„ ë¶„í• 
                    next_dt = current_dt + timedelta(days=self.config.max_days_per_request)

                range_end = min(next_dt - timedelta(days=1), end_dt)
                ranges.append((
                    current_dt.strftime("%Y-%m-%d"),
                    range_end.strftime("%Y-%m-%d")
                ))

                current_dt = next_dt

            logger.info(f"ë‚ ì§œ ë²”ìœ„ ë¶„í•  ì™„ë£Œ: {len(ranges)}ê°œ êµ¬ê°„")
            return ranges

        except Exception as e:
            logger.error(f"ë‚ ì§œ ë²”ìœ„ ìƒì„± ì‹¤íŒ¨: {e}")
            return [(start_date, end_date)]

    async def get_otp_token(self, params: Dict[str, Any]) -> Optional[str]:
        """OTP í† í° ë°œê¸‰"""
        try:
            await asyncio.sleep(random.uniform(self.config.min_delay, self.config.max_delay))
            headers = {
                'User-Agent': self.user_agent_rotator.get_user_agent() if self.user_agent_rotator else None,
                'Referer': KRX_BASE_URL,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive'
            }
            proxies = self.proxy_rotator.get_proxy_dict() if self.proxy_rotator else None
            if self.session is None:
                logger.error("ì„¸ì…˜ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return None
            # ë™ê¸° requests.postë¥¼ asyncio.to_threadë¡œ ë³‘ë ¬í™”
            response = await asyncio.to_thread(
                self.session.post,
                KRX_OTP_URL,
                data=params,
                headers=headers,
                proxies=proxies,
                timeout=30
            )
            if response.status_code == 200:
                otp_token = response.text.strip()
                logger.debug(f"OTP í† í° ë°œê¸‰ ì„±ê³µ: {otp_token[:20]}...")
                return otp_token
            else:
                logger.error(f"OTP í† í° ë°œê¸‰ ì‹¤íŒ¨: HTTP {response.status_code}")
                return None
        except Exception as e:
            logger.error(f"OTP í† í° ë°œê¸‰ ì˜¤ë¥˜: {e}")
            return None

    async def download_csv_data(self, otp_token: str) -> Optional[pd.DataFrame]:
        """OTP í† í°ìœ¼ë¡œ CSV ë°ì´í„° ë‹¤ìš´ë¡œë“œ"""
        try:
            await asyncio.sleep(random.uniform(self.config.min_delay, self.config.max_delay))
            headers = {
                'User-Agent': self.user_agent_rotator.get_user_agent() if self.user_agent_rotator else None,
                'Referer': KRX_BASE_URL,
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive'
            }
            proxies = self.proxy_rotator.get_proxy_dict() if self.proxy_rotator else None
            if self.session is None:
                logger.error("ì„¸ì…˜ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return None
            # ë™ê¸° requests.postë¥¼ asyncio.to_threadë¡œ ë³‘ë ¬í™”
            response = await asyncio.to_thread(
                self.session.post,
                KRX_DOWNLOAD_URL,
                data={'code': otp_token},
                headers=headers,
                proxies=proxies,
                timeout=60
            )
            if response.status_code == 200:
                csv_data = pd.read_csv(StringIO(response.text))
                logger.debug(f"CSV ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì„±ê³µ: {len(csv_data)} rows")
                return csv_data
            else:
                logger.error(f"CSV ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: HTTP {response.status_code}")
                return None
        except Exception as e:
            logger.error(f"CSV ë°ì´í„° ë‹¤ìš´ë¡œë“œ ì˜¤ë¥˜: {e}")
            return None

    async def crawl_stock_data(self, stock_info: StockInfo) -> CrawlResult:
        """ì£¼ì‹ ë°ì´í„° í¬ë¡¤ë§"""
        start_time = time.time()
        result = CrawlResult(stock_info=stock_info)

        try:
            # ìºì‹œ í™•ì¸
            cache_key = f"stock_{stock_info.code}_{stock_info.start_date}_{stock_info.end_date}"
            if self.config.use_cache and cache_key in cache:
                cached_data = cache[cache_key]
                result.data = pd.DataFrame(cached_data)
                result.success = True
                result.data_points = len(result.data)
                result.processing_time = time.time() - start_time
                logger.info(f"ìºì‹œì—ì„œ ë°ì´í„° ë¡œë“œ: {stock_info.code}")
                return result

            # ë‚ ì§œ ë²”ìœ„ ë¶„í• 
            date_ranges = self.generate_date_ranges(stock_info.start_date, stock_info.end_date)

            all_data = []
            for start_date, end_date in date_ranges:

                # OTP íŒŒë¼ë¯¸í„° ì„¤ì •
                otp_params = {
                    'locale': 'ko_KR',
                    'mktId': MARKET_ID_MAP.get(stock_info.market, 'ALL'),
                    'trdDd': end_date.replace('-', ''),
                    'isuCd': stock_info.code,
                    'strtDd': start_date.replace('-', ''),
                    'endDd': end_date.replace('-', ''),
                    'share': '1',
                    'money': '1',
                    'csvxls_isNo': 'false',
                    'name': 'fileDown',
                    'url': 'dbms/MDC/STAT/standard/MDCSTAT01501'
                }

                # ì¬ì‹œë„ ë¡œì§
                for attempt in range(self.config.max_retries):
                    try:
                        # OTP í† í° ë°œê¸‰
                        otp_token = await self.get_otp_token(otp_params)
                        if not otp_token:
                            continue

                        # CSV ë°ì´í„° ë‹¤ìš´ë¡œë“œ
                        data = await self.download_csv_data(otp_token)
                        if data is not None:
                            # ë°ì´í„° ì •ì œ
                            cleaned_data = self.clean_stock_data(data, stock_info)
                            if not cleaned_data.empty:
                                all_data.append(cleaned_data)
                            break

                    except Exception as e:
                        logger.warning(f"ì¬ì‹œë„ {attempt + 1}/{self.config.max_retries}: {e}")
                        if attempt < self.config.max_retries - 1:
                            await asyncio.sleep(self.config.retry_delay * (2 ** attempt))
                        else:
                            result.error_message = str(e)

            # ë°ì´í„° í†µí•©
            if all_data:
                combined_data = pd.concat(all_data, ignore_index=True)
                combined_data = combined_data.drop_duplicates().sort_values('ë‚ ì§œ').reset_index(drop=True)

                result.data = combined_data
                result.success = True
                result.data_points = len(combined_data)
                result.quality_score = self.calculate_data_quality(combined_data)

                # ìºì‹œ ì €ì¥
                if self.config.use_cache:
                    cache.set(cache_key, combined_data.to_dict('records'), expire=self.config.cache_expiry)

                logger.info(f"ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {stock_info.code} ({result.data_points}ì¼)")
            else:
                result.error_message = "ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨"
                logger.error(f"ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {stock_info.code}")

        except Exception as e:
            result.error_message = str(e)
            logger.error(f"ì£¼ì‹ ë°ì´í„° í¬ë¡¤ë§ ì˜¤ë¥˜ ({stock_info.code}): {e}")

        result.processing_time = time.time() - start_time
        return result

    def clean_stock_data(self, df: pd.DataFrame, stock_info: StockInfo) -> pd.DataFrame:
        """ìˆ˜ì§‘ëœ ë°ì´í„°ë¥¼ ì •ì œí•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤. (ê³ ê¸‰ ë²„ì „)"""
        if df is None or df.empty:
            logger.warning(f"Data for {stock_info.code} is empty. Skipping cleaning.")
            return pd.DataFrame()

        # 1. ê¸°ë³¸ì ì¸ ë°ì´í„° í´ë¦¬ë‹ (AdvancedDataCleaner)
        df_cleaned = self.cleaner.comprehensive_data_cleaning(df, stock_info)
        if df_cleaned.empty:
            return df_cleaned

        # 2. ê¸°ìˆ ì  ì§€í‘œ ë“± ê¸°ë³¸ ì „ì²˜ë¦¬ (DataPreprocessor)
        df_preprocessed = self.preprocessor.advanced_preprocessing(df_cleaned, stock_info)
        if df_preprocessed.empty:
            return df_preprocessed

        # 3. ë¨¸ì‹ ëŸ¬ë‹ì„ ìœ„í•œ ê³ ê¸‰ ì „ì²˜ë¦¬ (MLOptimizedPreprocessor)
        # ì¶©ë¶„í•œ ë°ì´í„°ê°€ ìˆì„ ë•Œë§Œ ML ì „ì²˜ë¦¬ ìˆ˜í–‰ (ìµœì†Œ 1ë…„ì¹˜ ë°ì´í„°)
        if len(df_preprocessed) > 250:
             logger.info(f"Applying advanced ML preprocessing for {stock_info.code}...")
             # df_ml_preprocessed = self.ml_preprocessor.comprehensive_ml_preprocessing(df_preprocessed, stock_info) # MLOptimizedPreprocessorëŠ” ì´ íŒŒì¼ì— ì—†ì–´ì•¼ í•©ë‹ˆë‹¤.
             return df_preprocessed
        else:
             logger.info(f"Skipping advanced ML preprocessing for {stock_info.code} due to insufficient data ({len(df_preprocessed)} rows).")
             return df_preprocessed

    def _basic_clean_fallback(self, df: pd.DataFrame, stock_info: StockInfo) -> pd.DataFrame:
        """ê¸°ë³¸ ì •ì œ (í´ë°± ë°©ì‹)"""
        if df is None or df.empty:
            return pd.DataFrame()
        try:
            if df.empty:
                return df

            # ê¸°ë³¸ ì»¬ëŸ¼ëª… ë§¤í•‘
            column_mapping = {
                'ì¼ì': 'ë‚ ì§œ',
                'ì¢…ê°€': 'ì¢…ê°€',
                'ì‹œê°€': 'ì‹œê°€',
                'ê³ ê°€': 'ê³ ê°€',
                'ì €ê°€': 'ì €ê°€',
                'ê±°ë˜ëŸ‰': 'ê±°ë˜ëŸ‰',
                'ê±°ë˜ëŒ€ê¸ˆ': 'ê±°ë˜ëŒ€ê¸ˆ',
                'ì‹œê°€ì´ì•¡': 'ì‹œê°€ì´ì•¡'
            }

            # ì»¬ëŸ¼ëª… ë³€ê²½
            for old_col, new_col in column_mapping.items():
                if old_col in df.columns:
                    df = df.rename(columns={old_col: new_col})

            # ë‚ ì§œ ë³€í™˜
            if 'ë‚ ì§œ' in df.columns:
                df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'], errors='coerce')

            # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
            numeric_columns = ['ì‹œê°€', 'ê³ ê°€', 'ì €ê°€', 'ì¢…ê°€', 'ê±°ë˜ëŸ‰', 'ê±°ë˜ëŒ€ê¸ˆ', 'ì‹œê°€ì´ì•¡']
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col].astype(str).str.replace(',', ''), errors='coerce')

            # ë©”íƒ€ë°ì´í„° ì¶”ê°€
            df['ì¢…ëª©ì½”ë“œ'] = stock_info.code
            df['ì¢…ëª©ëª…'] = stock_info.name
            df['ì‹œì¥'] = stock_info.market
            df['ì„¹í„°'] = stock_info.sector
            df['ë°ì´í„°íƒ€ì…'] = stock_info.data_type
            df['ìˆ˜ì§‘ì¼ì‹œ'] = datetime.now()

            # í•„ìˆ˜ ì»¬ëŸ¼ í™•ì¸ í›„ ê²°ì¸¡ê°’ ì œê±°
            essential_columns = []
            if 'ë‚ ì§œ' in df.columns:
                essential_columns.append('ë‚ ì§œ')
            if 'ì¢…ê°€' in df.columns:
                essential_columns.append('ì¢…ê°€')

            if essential_columns:
                df = df.dropna(subset=essential_columns)

            return df

        except Exception as e:
            logger.error(f"ê¸°ë³¸ ì •ì œ ì‹¤íŒ¨ ({stock_info.code}): {e}")
            return df

    def calculate_data_quality(self, df: pd.DataFrame) -> float:
        """ë°ì´í„° í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        if df is None or df.empty:
            return 0.0
        try:
            if df.empty:
                return 0.0

            quality_score = 1.0

            # ê²°ì¸¡ê°’ ë¹„ìœ¨
            missing_ratio = df.isnull().sum().sum() / (len(df) * len(df.columns))
            quality_score -= missing_ratio * 0.5

            # ì¤‘ë³µê°’ ë¹„ìœ¨
            duplicate_ratio = df.duplicated().sum() / len(df)
            quality_score -= duplicate_ratio * 0.3

            # í•„ìˆ˜ ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€
            required_columns = ['ë‚ ì§œ', 'ì‹œê°€', 'ê³ ê°€', 'ì €ê°€', 'ì¢…ê°€', 'ê±°ë˜ëŸ‰']
            missing_columns = [col for col in required_columns if col not in df.columns]
            quality_score -= len(missing_columns) * 0.1

            return max(0.0, min(1.0, quality_score))

        except Exception as e:
            logger.error(f"í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0

    async def crawl_all_stocks(self) -> List[CrawlResult]:
        """
        ëª¨ë“  ì‹œì¥ì˜ ì „ì²´ ì¢…ëª©ì— ëŒ€í•´ ë°ì´í„°ë¥¼ í¬ë¡¤ë§í•©ë‹ˆë‹¤.
        """
        logger.info("Starting ultimate crawl for all markets dynamically...")
        target_markets = ['KOSPI', 'KOSDAQ', 'ETF', 'FUTURES', 'OPTIONS']

        all_stock_infos = []
        for market in target_markets:
            market_stock_infos = await self.get_all_stock_infos(market)
            all_stock_infos.extend(market_stock_infos)
            await asyncio.sleep(self.config.request_delay)

        if not all_stock_infos:
            logger.critical("Failed to fetch any stock information. Terminating.")
            return []

        logger.info(f"Total stocks to crawl: {len(all_stock_infos)}")

        results = []
        semaphore = asyncio.Semaphore(self.config.max_workers)

        async def crawl_with_semaphore(stock_info):
            logger.info(f"[WORKER-START] {stock_info.code} {stock_info.name}")
            async with semaphore:
                try:
                    result = await self.crawl_stock_data(stock_info)
                    logger.info(f"[WORKER-END] {stock_info.code} {stock_info.name} success={result.success}")
                    return result
                except Exception as e:
                    logger.error(f"[WORKER-ERROR] {stock_info.code} {stock_info.name}: {e}")
                    return CrawlResult(stock_info=stock_info, success=False, error_message=str(e))

        tasks = [crawl_with_semaphore(info) for info in all_stock_infos]

        for future in tqdm.as_completed(tasks, total=len(tasks), desc="Crawling all markets"):
            result = await future
            results.append(result)
            if result.success:
                self.statistics["successful_requests"] += 1
                self.statistics["total_data_points"] += result.data_points
            else:
                self.statistics["failed_requests"] += 1
            self.statistics["total_processing_time"] += result.processing_time

        return results

    async def save_results(self, results: List[CrawlResult]):
        """ê²°ê³¼ ì €ì¥"""
        try:
            successful_results = [r for r in results if r.success and r.data is not None]

            if not successful_results:
                logger.warning("ì €ì¥í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return

            # ê°œë³„ íŒŒì¼ ì €ì¥
            for result in successful_results:
                try:
                    # ë°ì´í„° íƒ€ì… í™•ì¸
                    if result.data is None:
                        continue

                    # íŒŒì¼ëª… ìƒì„±
                    filename = f"{result.stock_info.data_type}_{result.stock_info.code}_{result.stock_info.start_date}_{result.stock_info.end_date}.csv"
                    filepath = DATA_DIR / filename

                    # CSV ì €ì¥
                    result.data.to_csv(filepath, index=False, encoding='utf-8-sig')

                    # ì••ì¶• ì €ì¥
                    compressed_filepath = filepath.with_suffix('.csv.gz')
                    result.data.to_csv(compressed_filepath, index=False, compression='gzip', encoding='utf-8-sig')

                    logger.info(f"ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filename}")

                except Exception as e:
                    logger.error(f"ê°œë³„ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨ ({result.stock_info.code}): {e}")

            # í†µí•© íŒŒì¼ ì €ì¥
            try:
                # Noneì´ ì•„ë‹Œ ë°ì´í„°ë§Œ í•„í„°ë§
                valid_data = [r.data for r in successful_results if r.data is not None]

                if valid_data:
                    all_data = pd.concat(valid_data, ignore_index=True)

                    # í†µí•© CSV ì €ì¥
                    unified_filename = f"krx_unified_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
                    unified_filepath = DATA_DIR / unified_filename
                    all_data.to_csv(unified_filepath, index=False, encoding='utf-8-sig')

                    # í†µí•© ì••ì¶• ì €ì¥
                    unified_compressed_filepath = unified_filepath.with_suffix('.csv.gz')
                    all_data.to_csv(unified_compressed_filepath, index=False, compression='gzip', encoding='utf-8-sig')

                    logger.info(f"í†µí•© ë°ì´í„° ì €ì¥ ì™„ë£Œ: {unified_filename}")
                else:
                    logger.warning("í†µí•©í•  ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

            except Exception as e:
                logger.error(f"í†µí•© íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

            # ìˆ˜ì§‘ ë¦¬í¬íŠ¸ ì €ì¥
            await self.save_collection_report(results)

        except Exception as e:
            logger.error(f"ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨: {e}")

    async def save_collection_report(self, results: List[CrawlResult]):
        """í¬ë¡¤ë§ ê²°ê³¼ì— ëŒ€í•œ ìƒì„¸ ë³´ê³ ì„œë¥¼ ì €ì¥í•©ë‹ˆë‹¤."""
        try:
            total_time_seconds = (datetime.now() - datetime.fromtimestamp(self.start_time)).total_seconds()
            report = {
                'collection_info': {
                    'start_time': datetime.fromtimestamp(self.start_time).strftime("%Y-%m-%d %H:%M:%S"),
                    'end_time': datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    'total_processing_time_seconds': total_time_seconds,
                    'total_stocks_crawled': len(results),
                    'successful_collections': len([r for r in results if r.success]),
                    'failed_collections': len([r for r in results if not r.success]),
                    'total_data_points': sum(r.data_points for r in results if r.success),
                    'average_quality_score': np.mean([r.quality_score for r in results if r.success]),
                    'success_rate': self.statistics["successful_requests"] / self.statistics["total_requests"] * 100 if self.statistics["total_requests"] > 0 else 0,
                },
                'failed_stocks': [
                    {
                        'code': r.stock_info.code,
                        'error': r.error_message
                    }
                    for r in results if not r.success
                ]
            }:
:
            report_filename = f"krx_collection_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json":
            report_filepath = DATA_DIR / report_filename:
            :
            with open(report_filepath, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)

            logger.info(f"ìˆ˜ì§‘ ë¦¬í¬íŠ¸ ì €ì¥ ì™„ë£Œ: {report_filename}")

        except Exception as e:
            logger.error(f"ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")

    def print_statistics(self):
        """í¬ë¡¤ë§ í†µê³„ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
        if not self.statistics or self.statistics.get("total_requests", 0) == 0:
            logger.info("No requests were made. Statistics are empty.")
            return

        try:
            print("\n" + "="*80)
            print("ğŸ¯ KRX Ultimate Crawler - Collection Report")
            print("="*80)

            total_time = (datetime.now() - datetime.fromtimestamp(self.start_time)).total_seconds()
            total_requests = self.statistics.get("successful_requests", 0) + self.statistics.get("failed_requests", 0)
            success_rate = (self.statistics.get("successful_requests", 0) / total_requests * 100) if total_requests > 0 else 0.0

            print(f"ğŸ“Š Total Requests: {total_requests:,}")
            print(f"âœ… Successful: {self.statistics.get('successful_requests', 0):,}")
            print(f"âŒ Failed: {self.statistics.get('failed_requests', 0):,}")
            print(f"ğŸ“ˆ Success Rate: {success_rate:.1f}%")
            print(f"ğŸ“‹ Total Data Points: {self.statistics.get('total_data_points', 0):,}")
            print(f"â±ï¸ Total Elapsed Time: {total_time:.1f} seconds")

            if total_time > 0:
                dps = self.statistics.get('total_data_points', 0) / total_time
                print(f"ğŸš€ Average Speed: {dps:.1f} data points/sec")

            print("\n" + "="*80)
        except Exception as e:
            logger.error(f"Failed to print statistics: {e}", exc_info=True)

    async def get_all_stock_infos(self, market: str) -> List[StockInfo]:
        """
        KRX ì •ë³´ë°ì´í„°ì‹œìŠ¤í…œì—ì„œ ì§€ì •ëœ ì‹œì¥ì˜ ì „ì²´ ì¢…ëª© ë¦¬ìŠ¤íŠ¸ë¥¼ ë™ì ìœ¼ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.
        """
        logger.info(f"Fetching all stock codes for market: {market}...")

        bld_map = {
            'KOSPI': 'MDCSTAT015/MDCSTAT01501/MDCSTAT01501',
            'KOSDAQ': 'MDCSTAT015/MDCSTAT01501/MDCSTAT01501',
            'KONEX': 'MDCSTAT015/MDCSTAT01501/MDCSTAT01501',
            'ETF': 'MDCSTAT043/MDCSTAT04301/MDCSTAT04301',
            'FUTURES': 'MDCSTAT053/MDCSTAT05301/MDCSTAT05301',
            'OPTIONS': 'MDCSTAT059/MDCSTAT05901/MDCSTAT05901',
        }
        bld = bld_map.get(market)
        if not bld:
            logger.error(f"Invalid market specified: {market}")
            return []

        params = {'bld': bld, 'mktId': MARKET_ID_MAP.get(market, 'ALL')}
        if market in ['KOSPI', 'KOSDAQ', 'KONEX']:
            params['segTpCd'] = MARKET_ID_MAP[market]

        try:
            otp_token = await self.get_otp_token(params)
            if not otp_token:
                return []

            csv_data = await self.download_csv_data(otp_token)
            if csv_data is None:
                return []

            stock_infos = []
            for _, row in csv_data.iterrows():
                code = row.get('ì¢…ëª©ì½”ë“œ') or row.get('ISU_CD')
                name = row.get('ì¢…ëª©ëª…') or row.get('ISU_ABBRV')
                market_name = row.get('ì‹œì¥êµ¬ë¶„') or market
                start_date = row.get('ìƒì¥ì¼') or '1980-01-01'
                # sectorê°€ Noneì¼ ê²½ìš° ë¹ˆ ë¬¸ìì—´ë¡œ ì²˜ë¦¬
                sector = row.get('ì—…ì¢…', '') or ''

                if code and name:
                    stock_infos.append(StockInfo(
                        code=str(code).strip(),
                        name=str(name).strip(),
                        market=market_name,
                        sector=str(sector),
                        data_type=market,
                        start_date=pd.to_datetime(start_date).strftime('%Y-%m-%d'),
                        end_date=datetime.now().strftime('%Y-%m-%d')
                    ))

            logger.info(f"Successfully fetched {len(stock_infos)} codes for {market}")
            return stock_infos
        except Exception as e:
            logger.critical(f"Error in get_all_stock_infos for {market}: {e}", exc_info=True)
            return []

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    config = CrawlConfig()
    async with KRXUltimateWebCrawler(config) as crawler:
        results = await crawler.crawl_all_stocks()
        if results:
            await crawler.save_results(results)
            await crawler.save_collection_report(results)
        crawler.print_statistics()

if __name__ == "__main__":
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("Crawler stopped by user.")
    except Exception as e:
        logger.critical(f"An unexpected error occurred in main: {e}", exc_info=True)
