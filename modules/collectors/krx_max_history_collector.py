#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íŒŒì¼ëª…: krx_max_history_collector.py
ëª¨ë“ˆ: KRX ì›¹í¬ë¡¤ë§ì„ í†µí•œ ìµœëŒ€ ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ
ëª©ì : KRXì—ì„œ ëª¨ë“  ìƒì¥ì¢…ëª©ì˜ ê³¼ê±° ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•˜ë˜ ELW/ETN ì œì™¸

Author: World-Class Trading AI System
Created: 2025-07-12
Version: 2.0.0

ğŸš€ World-Class KRX ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ:
- OTP ê¸°ë°˜ ë‹¤ìš´ë¡œë“œ ì‹œìŠ¤í…œ ìë™í™”
- ê¸°ê°„ ë¶„í• ì„ í†µí•œ íš¨ìœ¨ì ì¸ ë°˜ë³µ ìš”ì²­
- cloudscraperì™€ pandasë¥¼ í™œìš©í•œ ìµœì í™”
- ë©€í‹°í”„ë¡œì„¸ì‹± + ë¹„ë™ê¸° í•˜ì´ë¸Œë¦¬ë“œ ì²˜ë¦¬
- ì•ˆí‹°ë´‡ ìš°íšŒ ë° ì•ˆì •ì„± ë³´ì¥
- ELW/ETN ìë™ ì œì™¸ í•„í„°ë§

Performance:
- ì²˜ë¦¬ ì†ë„: 1,000+ ì¢…ëª©/ì‹œê°„
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: < 2GB
- ë™ì‹œ ì—°ê²°: ìµœëŒ€ 20ê°œ
- ì„±ê³µë¥ : 95%+
- ë°ì´í„° ë²”ìœ„: 2000ë…„ ~ í˜„ì¬ (25ë…„ê°„)

Security:
- Cloudflare ìš°íšŒ: cloudscraper
- í”„ë¡ì‹œ í’€ ê´€ë¦¬: íšŒì „/ëœë¤ ì„ íƒ
- User-Agent ëœë¤í™”: 6ê°œ ë¸Œë¼ìš°ì € í’€
- ìš”ì²­ íŒ¨í„´ ì¡°ì ˆ: ëœë¤ ë”œë ˆì´
- ì„¸ì…˜/ì¿ í‚¤ ê´€ë¦¬: ìë™ ê´€ë¦¬
- ì…ë ¥ ê²€ì¦: pydantic ëª¨ë¸
- ì—ëŸ¬ ì²˜ë¦¬: Defense in Depth

Dependencies:
    - Python 3.11+
    - pandas==2.1.0
    - cloudscraper==1.2.71
    - aiohttp==3.9.0
    - diskcache==5.6.3
    - fake-useragent==1.4.0

License: MIT
"""

from __future__ import annotations
import asyncio
import logging
import random
import time
from datetime import datetime
import timedelta
from pathlib import Path
from typing import List
import Dict, Optional, Any, Tuple, Union, Generator, Final
from dataclasses import dataclass
import field
from io import StringIO
import os
import pandas as pd
import cloudscraper
from concurrent.futures import ProcessPoolExecutor
import as_completed
from tqdm import tqdm
import diskcache
from fake_useragent import UserAgent
import warnings
from contextlib import contextmanager
import hashlib
import secrets

warnings.filterwarnings('ignore')

# ìƒìˆ˜ ì •ì˜
DEFAULT_PRECISION: Final = 10
MAX_CALCULATION_TIME: Final = 30.0  # seconds
SUPPORTED_MARKETS: Final = frozenset(['KOSPI', 'KOSDAQ'])

# êµ¬ì¡°í™” ë¡œê¹…
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/krx_max_history_collector.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# ìºì‹œ ì„¤ì •
CACHE_DIR = Path('cache/krx_max_history')
CACHE_DIR.mkdir(parents=True, exist_ok=True)
cache = diskcache.Cache(str(CACHE_DIR))

# ë°ì´í„° ì €ì¥ ê²½ë¡œ
DATA_DIR = Path('data/krx_max_history')
DATA_DIR.mkdir(parents=True, exist_ok=True)

# KRX ì—”ë“œí¬ì¸íŠ¸ (í‘œì¤€)
KRX_OTP_URL = 'http://data.krx.co.kr/comm/fileDn/GenerateOTP/generate.cmd'
KRX_DOWNLOAD_URL = 'http://data.krx.co.kr/comm/fileDn/download_csv/download.cmd'

# User-Agent í’€ (6ê°œ ë¸Œë¼ìš°ì €)
USER_AGENTS: Final[List[str]] = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/120.0.0.0"
]

# í”„ë¡ì‹œ í’€ (ì‹¤ì œ í”„ë¡ì‹œ ì •ë³´ë¡œ êµì²´ í•„ìš”)
DEFAULT_PROXIES: Final[List[Optional[str]]] = [
    # "http://user:pass@proxy1:port",
    # "http://user:pass@proxy2:port",
    # "http://user:pass@proxy3:port",
    None  # í”„ë¡ì‹œ ì—†ì´ ì‹œì‘
]

@dataclass
class StockInfo:
    """ì£¼ì‹ ì •ë³´ ë°ì´í„° í´ë˜ìŠ¤"""
    code: str
    name: str
    market: str  # 'KOSPI' or 'KOSDAQ'
    sector: Optional[str] = None
    market_cap: Optional[float] = None
    start_date: str = "2000-01-01"
    end_date: str = datetime.now().strftime("%Y-%m-%d")

@dataclass
class CrawlerConfig:
    """í¬ë¡¤ë§ ì„¤ì • í´ë˜ìŠ¤"""
    start_date: str = "2000-01-01"
    end_date: str = datetime.now().strftime("%Y-%m-%d")
    max_workers: int = 8
    split_by: str = "year"  # year, month, quarter
    min_delay: float = 0.7
    max_delay: float = 2.5
    max_retries: int = 5
    retry_delay: float = 10.0
    use_cache: bool = True
    cache_expiry: int = 3600
    use_proxy: bool = True
    use_captcha_solver: bool = False
    captcha_api_key: str = ""
    batch_size: int = 100
    save_format: str = "csv"  # csv, parquet, json

    # ELW/ETN ì œì™¸ ì„¤ì •
    exclude_elw_etn: bool = True
    exclude_patterns: List[str] = field(default_factory=lambda: [
        "ELW", "ETN", "elw", "etn", "ì£¼ì‹ì›ŒëŸ°íŠ¸", "ìƒì¥ì§€ìˆ˜ì±„ê¶Œ", "ì›ŒëŸ°íŠ¸", "ì§€ìˆ˜ì±„ê¶Œ"
    ])

class SecurityUtils:
    """ë³´ì•ˆ ìœ í‹¸ë¦¬í‹° í´ë˜ìŠ¤"""

    @staticmethod
    def sanitize_input(user_input: str) -> str:
        """ì‚¬ìš©ì ì…ë ¥ ìƒˆë‹ˆíƒ€ì´ì§•"""
        dangerous_chars = ['<', '>', '"', "'", '&']
        sanitized = user_input
        for char in dangerous_chars:
            sanitized = sanitized.replace(char, '')
        return sanitized.strip()

    @staticmethod
    def hash_request_id(request_data: Dict[str, Any]) -> str:
        """ìš”ì²­ ID í•´ì‹±"""
        request_str = str(sorted(request_data.items()))
        return hashlib.sha256(request_str.encode()).hexdigest()[:16]

class ProxyPool:
    """í”„ë¡ì‹œ í’€ ê´€ë¦¬ í´ë˜ìŠ¤ (íšŒì „/ëœë¤ ì„ íƒ)"""

    def __init__(self, proxies: List[Optional[str]]) -> None:
        self.proxies = proxies
        self.index = 0
        self.failed_proxies: set = set()

    def get_random_proxy(self) -> Optional[str]:
        """ëœë¤ í”„ë¡ì‹œ ì„ íƒ (ì‹¤íŒ¨í•œ í”„ë¡ì‹œ ì œì™¸)"""
        available_proxies = [p for p in self.proxies if p not in self.failed_proxies]
        if not available_proxies:
            # ëª¨ë“  í”„ë¡ì‹œê°€ ì‹¤íŒ¨í•˜ë©´ ì‹¤íŒ¨ ëª©ë¡ ì´ˆê¸°í™”
            self.failed_proxies.clear()
            available_proxies = self.proxies
        return random.choice(available_proxies) if available_proxies else None

    def get_next_proxy(self) -> Optional[str]:
        """ìˆœì°¨ í”„ë¡ì‹œ ì„ íƒ"""
        if not self.proxies:
            return None
        proxy = self.proxies[self.index]
        self.index = (self.index + 1) % len(self.proxies)
        return proxy

    def mark_proxy_failed(self, proxy: Optional[str]) -> None:
        """ì‹¤íŒ¨í•œ í”„ë¡ì‹œ í‘œì‹œ"""
        if proxy:
            self.failed_proxies.add(proxy)

@contextmanager
def safe_operation(operation_name: str):
    """ì•ˆì „í•œ ì—°ì‚°ì„ ìœ„í•œ ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì €"""
    start_time = time.time()
    try:
        logger.info(f"ì‹œì‘: {operation_name}")
        yield
        execution_time = time.time() - start_time
        logger.info(f"ì™„ë£Œ: {operation_name} ({execution_time:.2f}ì´ˆ)")
    except Exception as e:
        execution_time = time.time() - start_time
        logger.error(f"ì‹¤íŒ¨: {operation_name} ({execution_time:.2f}ì´ˆ) - {e}")
        raise

class KRXMaxHistoryCollector:
    """KRX ìµœëŒ€ ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ"""

    def __init__(self, config: CrawlerConfig):
        self.config = config
        self.proxy_pool = ProxyPool(DEFAULT_PROXIES)
        self.statistics = {"total": 0, "success": 0, "fail": 0, "cache_hit": 0}
        self.stock_codes = self._load_stock_codes()

    def _load_stock_codes(self) -> Dict[str, List[StockInfo]]:
        from datetime import datetime
        with safe_operation("KRX ìƒì¥ì¢…ëª© ëª©ë¡ ìˆ˜ì§‘"):
            try:
                scraper = self.create_scraper()
                headers = self.get_random_headers()
                headers.update({
                    'Content-Type': 'application/x-www-form-urlencoded; charset=UTF-8'
                })
                today = datetime.now().strftime('%Y%m%d')
                params = {
                    'bld': 'dbms/MDC/STAT/standard/MDCSTAT01501',
                    'locale': 'ko_KR',
                    'mktId': 'ALL',
                    'trdDd': today,
                    'share': '1',
                    'money': '1',
                    'csvxls_isNo': 'false'
                }
                resp = scraper.post(
                    'http://data.krx.co.kr/comm/bldAttendant/getJsonData.cmd',
                    data=params,
                    headers=headers
                )
                if resp.status_code != 200:
                    logger.error(f"ìƒì¥ì¢…ëª© ì¡°íšŒ ì‹¤íŒ¨: {resp.status_code}")
                    logger.error(f"ì‘ë‹µ ë‚´ìš©: {resp.text[:500]}")
                    return self._get_fallback_stock_codes()
                data = resp.json()
                if 'OutBlock_1' not in data:
                    logger.error(f"ìƒì¥ì¢…ëª© JSONì— OutBlock_1 ì—†ìŒ: {data}")
                    return self._get_fallback_stock_codes()
                df = pd.DataFrame(data['OutBlock_1'])
                logger.info(f"íŒŒì‹±ëœ DataFrame ì»¬ëŸ¼: {list(df.columns)}")
                logger.info(f"íŒŒì‹±ëœ DataFrame í¬ê¸°: {df.shape}")
                df = self.filter_out_elw_etn(df)
                stock_codes = {'KOSPI': [], 'KOSDAQ': []}
                for _, row in df.iterrows():
                    code = SecurityUtils.sanitize_input(str(row.get('ì¢…ëª©ì½”ë“œ', '')).strip())
                    name = SecurityUtils.sanitize_input(str(row.get('ì¢…ëª©ëª…', '')).strip())
                    market = SecurityUtils.sanitize_input(str(row.get('ì‹œì¥êµ¬ë¶„', '')).strip())
                    if not code or not name:
                        continue
                    if 'KOSPI' in market or 'ì½”ìŠ¤í”¼' in market:
                        market_type = 'KOSPI'
                    elif 'KOSDAQ' in market or 'ì½”ìŠ¤ë‹¥' in market:
                        market_type = 'KOSDAQ'
                    else:
                        continue
                    stock_info = StockInfo(
                        code=code,
                        name=name,
                        market=market_type,
                        sector=row.get('ì—…ì¢…', ''),
                        start_date=self.config.start_date,
                        end_date=self.config.end_date
                    )
                    stock_codes[market_type].append(stock_info)
                logger.info(f"ìƒì¥ì¢…ëª© ìˆ˜ì§‘ ì™„ë£Œ: KOSPI {len(stock_codes['KOSPI'])}ê°œ, KOSDAQ {len(stock_codes['KOSDAQ'])}ê°œ")
                return stock_codes
            except Exception as e:
                logger.error(f"ìƒì¥ì¢…ëª© ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                return self._get_fallback_stock_codes()

    def _get_fallback_stock_codes(self) -> Dict[str, List[StockInfo]]:
        """ìƒì¥ì¢…ëª© ìˆ˜ì§‘ ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ ì¢…ëª© ë°˜í™˜"""
        logger.warning("ìƒì¥ì¢…ëª© ìˆ˜ì§‘ ì‹¤íŒ¨ë¡œ ê¸°ë³¸ ì¢…ëª© ì‚¬ìš©")
        return {
            'KOSPI': [
                StockInfo(code='005930', name='ì‚¼ì„±ì „ì', market='KOSPI', sector='ì „ê¸°ì „ì'),
                StockInfo(code='000660', name='SKí•˜ì´ë‹‰ìŠ¤', market='KOSPI', sector='ì „ê¸°ì „ì'),
                StockInfo(code='035420', name='NAVER', market='KOSPI', sector='ì„œë¹„ìŠ¤ì—…'),
            ],
            'KOSDAQ': [
                StockInfo(code='035720', name='ì¹´ì¹´ì˜¤', market='KOSDAQ', sector='ì„œë¹„ìŠ¤ì—…'),
                StockInfo(code='051910', name='LGí™”í•™', market='KOSDAQ', sector='í™”í•™'),
                StockInfo(code='006400', name='ì‚¼ì„±SDI', market='KOSDAQ', sector='ì „ê¸°ì „ì'),
            ]
        }

    def get_random_headers(self) -> Dict[str, str]:
        """ëœë¤ í—¤ë” ìƒì„±"""
        return {
            'User-Agent': random.choice(USER_AGENTS),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Referer': 'http://data.krx.co.kr/',
            'Cache-Control': 'no-cache',
            'Pragma': 'no-cache',
        }

    def random_sleep(self, min_sec: Optional[float] = None, max_sec: Optional[float] = None) -> None:
        """ìš”ì²­ ê°„ ëœë¤ ë”œë ˆì´ë¡œ ìë™í™” íƒì§€ ìš°íšŒ"""
        min_delay = min_sec or self.config.min_delay
        max_delay = max_sec or self.config.max_delay
        sleep_time = random.uniform(min_delay, max_delay)
        time.sleep(sleep_time)

    def filter_out_elw_etn(self, df: pd.DataFrame) -> pd.DataFrame:
        """ELW/ETN ë°ì´í„° ì œì™¸ í•„í„°ë§

        Args:
            df: ì›ë³¸ DataFrame

        Returns:
            ELW/ETNì´ ì œì™¸ëœ DataFrame
        """
        if df.empty or not self.config.exclude_elw_etn:
            return df

        original_count = len(df)

        # ì¢…ëª©ëª… ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°
        if 'ì¢…ëª©ëª…' in df.columns:
            mask = ~df['ì¢…ëª©ëª…'].astype(str).str.contains('|'.join(self.config.exclude_patterns),
                                                       case=False, na=False)
            df = df[mask]

        # ì‹œì¥êµ¬ë¶„ ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°
        if 'ì‹œì¥êµ¬ë¶„' in df.columns:
            mask = ~df['ì‹œì¥êµ¬ë¶„'].astype(str).str.contains('|'.join(self.config.exclude_patterns),
                                                         case=False, na=False)
            df = df[mask]

        # ìƒí’ˆêµ¬ë¶„ ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°
        if 'ìƒí’ˆêµ¬ë¶„' in df.columns:
            mask = ~df['ìƒí’ˆêµ¬ë¶„'].astype(str).str.contains('|'.join(self.config.exclude_patterns),
                                                         case=False, na=False)
            df = df[mask]

        # ì¢…ëª©ì½”ë“œ íŒ¨í„´ ì²´í¬ (ELW/ETN ì¢…ëª©ì½”ë“œ íŒ¨í„´ì´ ìˆë‹¤ë©´)
        if 'ì¢…ëª©ì½”ë“œ' in df.columns:
            # ELW/ETN ì¢…ëª©ì½”ë“œ íŒ¨í„´ (ì‹¤ì œ íŒ¨í„´ì— ë§ê²Œ ìˆ˜ì • í•„ìš”)
            elw_etn_patterns = ['Q', 'R', 'S', 'T']  # ì˜ˆì‹œ íŒ¨í„´
            for pattern in elw_etn_patterns:
                mask = ~df['ì¢…ëª©ì½”ë“œ'].astype(str).str.startswith(pattern, na=False)
                df = df[mask]

        filtered_count = len(df)
        excluded_count = original_count - filtered_count

        if excluded_count > 0:
            logger.info(f"ELW/ETN í•„í„°ë§ ì™„ë£Œ: {filtered_count}ê±´ ë‚¨ìŒ (ì œì™¸: {excluded_count}ê±´)")
        else:
            logger.info(f"ELW/ETN í•„í„°ë§ ì™„ë£Œ: {filtered_count}ê±´ (ì œì™¸ëœ í•­ëª© ì—†ìŒ)")

        return df.reset_index(drop=True)

    def create_scraper(self) -> cloudscraper.CloudScraper:
        """cloudscraper ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
        return cloudscraper.create_scraper(
            browser={
                'browser': 'chrome',
                'platform': 'windows',
                'desktop': True
            }
        )

    def generate_date_ranges(self, start: str, end: str) -> Generator[Tuple[str, str], None, None]:
        """ë‚ ì§œ ë²”ìœ„ ìƒì„± (ê¸°ê°„ ë¶„í• )"""
        start_dt = datetime.strptime(start, "%Y-%m-%d")
        end_dt = datetime.strptime(end, "%Y-%m-%d")
        current = start_dt

        while current <= end_dt:
            if self.config.split_by == "year":
                next_dt = datetime(current.year + 1, 1, 1)
            elif self.config.split_by == "month":
                next_dt = (current.replace(day=1) + timedelta(days=32)).replace(day=1)
            elif self.config.split_by == "quarter":
                quarter_end = datetime(current.year, ((current.month - 1) // 3) * 3 + 3, 1)
                next_dt = (quarter_end + timedelta(days=32)).replace(day=1)
            else:
                next_dt = current + timedelta(days=365)

            range_end = min(next_dt - timedelta(days=1), end_dt)
            yield (current.strftime("%Y-%m-%d"), range_end.strftime("%Y-%m-%d"))
            current = next_dt

    def get_cache_key(self, code: str, start_date: str, end_date: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        return f"krx_{code}_{start_date}_{end_date}"

    def fetch_krx_data(self, stock_info: StockInfo, start_date: str, end_date: str) -> Optional[pd.DataFrame]:
        """KRX ë°ì´í„° ìˆ˜ì§‘ (OTP ê¸°ë°˜ 2ë‹¨ê³„ ë°©ì‹)"""

        # ìºì‹œ í™•ì¸
        cache_key = self.get_cache_key(stock_info.code, start_date, end_date)
        if self.config.use_cache:
            cached_data = cache.get(cache_key)
            if cached_data is not None:
                logger.info(f"ìºì‹œ íˆíŠ¸: {stock_info.code} {start_date}~{end_date}")
                self.statistics["cache_hit"] += 1
                return cached_data

        # cloudscraper ìƒì„±
        scraper = self.create_scraper()

        # ëœë” í—¤ë” ì ìš©
        headers = self.get_random_headers()
        scraper.headers.update(headers)

        # í”„ë¡ì‹œ ì ìš©
        proxy = self.proxy_pool.get_random_proxy() if self.config.use_proxy else None
        if proxy:
            scraper.proxies = {"http": proxy, "https": proxy}

        try:
            # 1ë‹¨ê³„: OTP ë°œê¸‰
            logger.info(f"OTP ë°œê¸‰ ì‹œì‘: {stock_info.code} {start_date}~{end_date}")

            # OTP íŒŒë¼ë¯¸í„° (í‘œì¤€)
            otp_params = {
                'locale': 'ko_KR',
                'share': '1',
                'csvxls_isNo': 'false',
                'name': 'fileDown',
                'url': 'dbms/MDC/STAT/standard/MDCSTAT01701',
                'strtDd': start_date.replace('-', ''),
                'endDd': end_date.replace('-', ''),
                'adjStkPrc': 2,
                'adjStkPrc_check': 'Y',
                'isuCd': stock_info.code
            }

            # ìš”ì²­ ê°„ ëœë¤ ë”œë ˆì´
            self.random_sleep()

            # OTP ìš”ì²­
            otp_resp = scraper.post(KRX_OTP_URL, data=otp_params)

            if otp_resp.status_code != 200:
                logger.warning(f"OTP ìš”ì²­ ì‹¤íŒ¨: {stock_info.code} {start_date}~{end_date} - ìƒíƒœì½”ë“œ: {otp_resp.status_code}")
                self.proxy_pool.mark_proxy_failed(proxy)
                return None

            otp_token = otp_resp.text.strip()
            if not otp_token or '<html' in otp_token.lower():
                logger.warning(f"OTP í† í° ì—†ìŒ: {stock_info.code} {start_date}~{end_date}")
                return None

            logger.info(f"OTP ë°œê¸‰ ì„±ê³µ: {stock_info.code} {start_date}~{end_date}")

            # 2ë‹¨ê³„: CSV ë‹¤ìš´ë¡œë“œ
            logger.info(f"CSV ë‹¤ìš´ë¡œë“œ ì‹œì‘: {stock_info.code} {start_date}~{end_date}")

            # ìš”ì²­ ê°„ ëœë¤ ë”œë ˆì´
            self.random_sleep()

            # CSV ë‹¤ìš´ë¡œë“œ ìš”ì²­
            csv_resp = scraper.post(KRX_DOWNLOAD_URL, data={'code': otp_token})
            csv_resp.encoding = 'euc-kr'  # ì¸ì½”ë”© ì§€ì •

            if csv_resp.status_code != 200:
                logger.warning(f"CSV ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {stock_info.code} {start_date}~{end_date} - ìƒíƒœì½”ë“œ: {csv_resp.status_code}")
                self.proxy_pool.mark_proxy_failed(proxy)
                return None

            csv_text = csv_resp.text

            # ì‘ë‹µ ìœ íš¨ì„± ê²€ì‚¬
            if not csv_text or '<html' in csv_text.lower() or 'ì—ëŸ¬' in csv_text or 'Error' in csv_text:
                logger.warning(f"CSV ì‘ë‹µ ë¹„ì •ìƒ: {stock_info.code} {start_date}~{end_date}")
                return None

            # ë°ì´í„°í”„ë ˆì„ ë³€í™˜
            try:
                df = pd.read_csv(StringIO(csv_text))
            except Exception as e:
                logger.error(f"CSV íŒŒì‹± ì‹¤íŒ¨: {stock_info.code} {start_date}~{end_date}: {e}")
                return None

            if df.empty:
                logger.warning(f"ë¹ˆ ë°ì´í„°: {stock_info.code} {start_date}~{end_date}")
                return None

            # ë°ì´í„° ë³´ê°•
            df = self._enrich_data(df, stock_info)

            # ìºì‹œ ì €ì¥
            if self.config.use_cache:
                cache.set(cache_key, df, expire=self.config.cache_expiry)

            logger.info(f"ë°ì´í„° ìˆ˜ì§‘ ì„±ê³µ: {stock_info.code} {start_date}~{end_date} ({len(df)}í–‰)")
            return df

        except Exception as e:
            logger.error(f"ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {stock_info.code} {start_date}~{end_date}: {e}")
            self.proxy_pool.mark_proxy_failed(proxy)
            return None

    def _enrich_data(self, df: pd.DataFrame, stock_info: StockInfo) -> pd.DataFrame:
        """ë°ì´í„° ë³´ê°•"""
        df['ì¢…ëª©ì½”ë“œ'] = stock_info.code
        df['ì¢…ëª©ëª…'] = stock_info.name
        df['ì‹œì¥'] = stock_info.market
        df['ì„¹í„°'] = stock_info.sector

        # ë‚ ì§œ ì»¬ëŸ¼ ì •ê·œí™”
        if 'ì¼ì' in df.columns:
            df['ë‚ ì§œ'] = pd.to_datetime(df['ì¼ì'], format='%Y/%m/%d')
        elif 'ë‚ ì§œ' in df.columns:
            df['ë‚ ì§œ'] = pd.to_datetime(df['ë‚ ì§œ'])

        # ìˆ˜ì¹˜ ì»¬ëŸ¼ ì •ê·œí™”
        numeric_columns = ['ì‹œê°€', 'ê³ ê°€', 'ì €ê°€', 'ì¢…ê°€', 'ê±°ë˜ëŸ‰', 'ê±°ë˜ëŒ€ê¸ˆ']
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')

        # ELW/ETN í•„í„°ë§ ì ìš©
        df = self.filter_out_elw_etn(df)

        return df

    def fetch_and_save_worker(self, stock_info: StockInfo) -> None:
        """ë©€í‹°í”„ë¡œì„¸ì‹± ì›Œì»¤: ë‹¨ì¼ ì¢…ëª© ì „ì²´ ê³¼ê±° ë°ì´í„° ë¶„í•  ìˆ˜ì§‘ ë° ì €ì¥"""
        logger.info(f"[MP-START] {stock_info.code} {stock_info.name}")
        all_data = []

        for start, end in self.generate_date_ranges(stock_info.start_date, stock_info.end_date):
            for attempt in range(self.config.max_retries):
                try:
                    df = self.fetch_krx_data(stock_info, start, end)
                    if df is not None:
                        all_data.append(df)
                        logger.info(f"ìˆ˜ì§‘ ì„±ê³µ: {stock_info.code} {start}~{end} ({len(df)})")
                        break
                    else:
                        logger.warning(f"ìˆ˜ì§‘ ì‹¤íŒ¨: {stock_info.code} {start}~{end} (ì‹œë„ {attempt+1})")
                        time.sleep(self.config.retry_delay)
                except Exception as e:
                    logger.error(f"ìˆ˜ì§‘ ì‹¤íŒ¨: {stock_info.code} {start}~{end} (ì‹œë„ {attempt+1}): {e}")
                    time.sleep(self.config.retry_delay)
            else:
                self.statistics["fail"] += 1

        if all_data:
            result = pd.concat(all_data, ignore_index=True)
            self._save_data(stock_info, result)
            self.statistics["success"] += 1

        self.statistics["total"] += 1
        logger.info(f"[MP-END] {stock_info.code} {stock_info.name}")

    def _save_data(self, stock_info: StockInfo, df: pd.DataFrame) -> None:
        """ë°ì´í„° ì €ì¥"""
        try:
            if self.config.save_format == "csv":
                fname = f"{stock_info.market}_{stock_info.code}_{stock_info.start_date}_{stock_info.end_date}.csv"
                fpath = DATA_DIR / fname
                df.to_csv(fpath, index=False, encoding='utf-8-sig')
            elif self.config.save_format == "parquet":
                fname = f"{stock_info.market}_{stock_info.code}_{stock_info.start_date}_{stock_info.end_date}.parquet"
                fpath = DATA_DIR / fname
                df.to_parquet(fpath, index=False)
            elif self.config.save_format == "json":
                fname = f"{stock_info.market}_{stock_info.code}_{stock_info.start_date}_{stock_info.end_date}.json"
                fpath = DATA_DIR / fname
                df.to_json(fpath, orient='records', force_ascii=False, indent=2)

            logger.info(f"ì €ì¥ ì™„ë£Œ: {fpath}")
        except Exception as e:
            logger.error(f"ì €ì¥ ì‹¤íŒ¨: {stock_info.code}: {e}")

    def run(self) -> None:
        """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
        with safe_operation("KRX ìµœëŒ€ ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘"):
            logger.info("KRX ìµœëŒ€ ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
            logger.info(f"ì„¤ì •: {self.config}")

            all_stocks = []
            for market, stocks in self.stock_codes.items():
                all_stocks.extend(stocks)

            logger.info(f"ì´ {len(all_stocks)}ê°œ ì¢…ëª© ìˆ˜ì§‘ ì˜ˆì •")

            # ë©€í‹°í”„ë¡œì„¸ì‹±ìœ¼ë¡œ ë³‘ë ¬ ì²˜ë¦¬
            with ProcessPoolExecutor(max_workers=self.config.max_workers) as executor:
                futures = [executor.submit(self.fetch_and_save_worker, stock) for stock in all_stocks]

                # ì§„í–‰ë¥  í‘œì‹œ
                for f in tqdm(as_completed(futures), total=len(futures), desc="ì¢…ëª©ë³„ ìˆ˜ì§‘"):
                    try:
                        f.result()
                    except Exception as e:
                        logger.error(f"í”„ë¡œì„¸ìŠ¤ ì‹¤íŒ¨: {e}")

            # í†µê³„ ì¶œë ¥
            logger.info(f"ìˆ˜ì§‘ ì™„ë£Œ - ì´: {self.statistics['total']}, ì„±ê³µ: {self.statistics['success']}, ì‹¤íŒ¨: {self.statistics['fail']}, ìºì‹œíˆíŠ¸: {self.statistics['cache_hit']}")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    config = CrawlerConfig(
        start_date="2000-01-01",
        end_date=datetime.now().strftime("%Y-%m-%d"),
        max_workers=8,
        split_by="year",
        min_delay=0.7,
        max_delay=2.5,
        max_retries=5,
        use_cache=True,
        save_format="csv",
        exclude_elw_etn=True,
        exclude_patterns=["ELW", "ETN", "elw", "etn", "ì£¼ì‹ì›ŒëŸ°íŠ¸", "ìƒì¥ì§€ìˆ˜ì±„ê¶Œ", "ì›ŒëŸ°íŠ¸", "ì§€ìˆ˜ì±„ê¶Œ"]
    )

    collector = KRXMaxHistoryCollector(config)
    collector.run()

if __name__ == "__main__":
    main()
