#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íŒŒì¼ëª…: crawling_strategy_guide.py
ëª©ì : World-Class í¬ë¡¤ë§ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„ íƒ ê°€ì´ë“œ ë° ìµœì í™” ì „ëµ
Author: World-Class Trading AI System
Created: 2025-07-12
Version: 1.0.0

ğŸ¯ í¬ë¡¤ë§ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„ íƒ ê°€ì´ë“œ:
- ì •ì  í˜ì´ì§€: requests + BeautifulSoup (ë¹ ë¥´ê³  ê°„ë‹¨)
- ë™ì  í˜ì´ì§€: Selenium (JavaScript ë Œë”ë§ í•„ìˆ˜)
- ëŒ€ê·œëª¨ í¬ë¡¤ë§: Scrapy (ë¹„ë™ê¸° ì²˜ë¦¬, í™•ì¥ì„±)
- ë³´ì•ˆ ìš°íšŒ: cloudscraper (Cloudflare ë“± ìš°íšŒ)
- ê³ ì„±ëŠ¥ ë¹„ë™ê¸°: aiohttp (ë™ì‹œ 100ê°œ+ ìš”ì²­)
"""

from __future__ import annotations
import asyncio
import logging
import random
import time
from datetime import datetime
from pathlib import Path
from typing import Dict
import List, Optional, Any, Union, Tuple
from dataclasses import dataclass
import field
from enum import Enum

import requests
from bs4 import BeautifulSoup
import selenium
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
import aiohttp
import cloudscraper
import pandas as pd

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class PageType(Enum):
    """í˜ì´ì§€ íƒ€ì… ë¶„ë¥˜"""
    STATIC = "static"           # ì •ì  HTML
    DYNAMIC = "dynamic"         # JavaScript ë Œë”ë§
    PROTECTED = "protected"     # Cloudflare ë“± ë³´í˜¸
    API = "api"                 # REST API
    MASS = "mass"               # ëŒ€ê·œëª¨ ìˆ˜ì§‘

class CrawlingLibrary(Enum):
    """í¬ë¡¤ë§ ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¶„ë¥˜"""
    REQUESTS_BS4 = "requests_beautifulsoup"
    SELENIUM = "selenium"
    CLOUDSCRAPER = "cloudscraper"
    AIOHTTP = "aiohttp"
    SCRAPY = "scrapy"

@dataclass
class LibraryStrategy:
    """ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„ íƒ ì „ëµ"""
    primary: CrawlingLibrary
    fallback: Optional[CrawlingLibrary] = None
    performance_mode: str = "single"  # single, async, multiprocessing
    anti_bot: List[str] = field(default_factory=lambda: ["headers_randomization"])

class CrawlingStrategyGuide:
    """World-Class í¬ë¡¤ë§ ì „ëµ ê°€ì´ë“œ"""

    def __init__(self):
        self.strategies = self._initialize_strategies()
        self.user_agents = [
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0",
            "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15",
        ]

    def _initialize_strategies(self) -> Dict[PageType, LibraryStrategy]:
        """ì „ëµ ì´ˆê¸°í™”"""
        return {
            PageType.STATIC: LibraryStrategy(
                primary=CrawlingLibrary.REQUESTS_BS4,
                fallback=CrawlingLibrary.AIOHTTP,
                performance_mode="async",
                anti_bot=["headers_randomization", "rate_limiting"]
            ),
            PageType.DYNAMIC: LibraryStrategy(
                primary=CrawlingLibrary.SELENIUM,
                fallback=CrawlingLibrary.CLOUDSCRAPER,
                performance_mode="single",
                anti_bot=["user_agent_rotation", "proxy_rotation", "headless_mode"]
            ),
            PageType.PROTECTED: LibraryStrategy(
                primary=CrawlingLibrary.CLOUDSCRAPER,
                fallback=CrawlingLibrary.SELENIUM,
                performance_mode="single",
                anti_bot=["cloudflare_bypass", "proxy_rotation", "session_management"]
            ),
            PageType.API: LibraryStrategy(
                primary=CrawlingLibrary.REQUESTS_BS4,
                fallback=CrawlingLibrary.AIOHTTP,
                performance_mode="async",
                anti_bot=["rate_limiting", "retry_logic"]
            ),
            PageType.MASS: LibraryStrategy(
                primary=CrawlingLibrary.AIOHTTP,
                fallback=CrawlingLibrary.SCRAPY,
                performance_mode="multiprocessing",
                anti_bot=["proxy_pool", "session_rotation", "distributed_crawling"]
            )
        }

    def select_strategy(self, url: str, page_type: PageType, volume: str = "low") -> LibraryStrategy:
        """ìš©ë„ë³„ ìµœì  ì „ëµ ì„ íƒ"""

        # URL íŒ¨í„´ ë¶„ì„
        if "cloudflare" in url.lower() or "krx.co.kr" in url:
            page_type = PageType.PROTECTED
        elif "api" in url.lower() or "json" in url.lower():
            page_type = PageType.API
        elif volume == "high" or "mass" in url.lower():
            page_type = PageType.MASS

        strategy = self.strategies[page_type]

        # ë³¼ë¥¨ì— ë”°ë¥¸ ì„±ëŠ¥ ëª¨ë“œ ì¡°ì •
        if volume == "high":
            strategy.performance_mode = "multiprocessing"
        elif volume == "medium":
            strategy.performance_mode = "async"

        return strategy

    def get_implementation_guide(self, strategy: LibraryStrategy) -> Dict[str, Any]:
        """êµ¬í˜„ ê°€ì´ë“œ ì œê³µ"""

        guides = {
            CrawlingLibrary.REQUESTS_BS4: {
                "description": "ì •ì  í˜ì´ì§€ í¬ë¡¤ë§ - ë¹ ë¥´ê³  ê°„ë‹¨",
                "pros": ["ë¹ ë¥¸ ì†ë„", "ê°„ë‹¨í•œ ì‚¬ìš©ë²•", "ì•ˆì •ì„±"],
                "cons": ["ë™ì  ì½˜í…ì¸  ë¶ˆê°€", "JavaScript ë Œë”ë§ ì—†ìŒ"],
                "best_for": ["API í˜¸ì¶œ", "ì •ì  HTML", "ê°„ë‹¨í•œ í¬ë¡¤ë§"],
                "code_example": """

def crawl_static_page(url: str) -> str:
    headers = {'User-Agent': 'Mozilla/5.0...'}
    response = requests.get(url, headers=headers)
    soup = BeautifulSoup(response.text, 'html.parser')
    return soup.get_text()
                """
            },
            CrawlingLibrary.SELENIUM: {
                "description": "ë™ì  í˜ì´ì§€ í¬ë¡¤ë§ - ì™„ì „í•œ ë¸Œë¼ìš°ì € ì œì–´",
                "pros": ["JavaScript ë Œë”ë§", "ì™„ì „í•œ ì œì–´", "ë³µì¡í•œ ìƒí˜¸ì‘ìš©"],
                "cons": ["ëŠë¦° ì†ë„", "ë†’ì€ ë¦¬ì†ŒìŠ¤ ì‚¬ìš©", "ë³µì¡í•œ ì„¤ì •"],
                "best_for": ["SPA", "ë³µì¡í•œ OTP", "ë™ì  ì½˜í…ì¸ "],
                "code_example": """

def crawl_dynamic_page(url: str) -> str:
    options = Options()
    options.add_argument('--headless')
    driver = webdriver.Chrome(options=options)
    driver.get(url)
    content = driver.page_source
    driver.quit()
    return content
                """
            },
            CrawlingLibrary.CLOUDSCRAPER: {
                "description": "ë³´ì•ˆ ìš°íšŒ í¬ë¡¤ë§ - Cloudflare ë“± ìš°íšŒ",
                "pros": ["ë³´ì•ˆ ìš°íšŒ", "requestsì™€ ìœ ì‚¬í•œ API", "ì•ˆì •ì„±"],
                "cons": ["ì¼ë¶€ ì‚¬ì´íŠ¸ì—ì„œë§Œ íš¨ê³¼", "ì†ë„ ì œí•œ"],
                "best_for": ["Cloudflare ë³´í˜¸ ì‚¬ì´íŠ¸", "KRX", "ê¸ˆìœµ ë°ì´í„°"],
                "code_example": """

def crawl_protected_page(url: str) -> str:
    scraper = cloudscraper.create_scraper()
    response = scraper.get(url)
    return response.text
                """
            },
            CrawlingLibrary.AIOHTTP: {
                "description": "ë¹„ë™ê¸° ê³ ì„±ëŠ¥ í¬ë¡¤ë§ - ë™ì‹œ 100ê°œ+ ìš”ì²­",
                "pros": ["ë§¤ìš° ë¹ ë¥¸ ì†ë„", "í™•ì¥ì„±", "ë¹„ë™ê¸° ì²˜ë¦¬"],
                "cons": ["ë³µì¡í•œ ì„¤ì •", "ë™ì  ì½˜í…ì¸  ì œí•œ"],
                "best_for": ["ëŒ€ëŸ‰ API í˜¸ì¶œ", "ê³ ì„±ëŠ¥ í¬ë¡¤ë§", "ì‹¤ì‹œê°„ ë°ì´í„°"],
                "code_example": """

async def crawl_async(urls: List[str]) -> List[str]:
    async with aiohttp.ClientSession() as session:
        tasks = [session.get(url) for url in urls]
        responses = await asyncio.gather(*tasks)
        return [await resp.text() for resp in responses]
                """
            },
            CrawlingLibrary.SCRAPY: {
                "description": "ëŒ€ê·œëª¨ í¬ë¡¤ë§ í”„ë ˆì„ì›Œí¬ - ì²´ê³„ì  ê´€ë¦¬",
                "pros": ["í™•ì¥ì„±", "ì²´ê³„ì  ê´€ë¦¬", "ê³ ê¸‰ ê¸°ëŠ¥"],
                "cons": ["ë†’ì€ í•™ìŠµê³¡ì„ ", "ë³µì¡í•œ ì„¤ì •"],
                "best_for": ["ëŒ€ê·œëª¨ í”„ë¡œì íŠ¸", "ìë™í™”", "ì—”í„°í”„ë¼ì´ì¦ˆ"],
                "code_example": """
# scrapy spider ì˜ˆì‹œ
class MySpider(scrapy.Spider):
    name = 'myspider'
    start_urls = ['http://example.com']

    def parse(self, response):
        yield {'title': response.css('h1::text').get()}
                """
            }
        }

        return guides.get(strategy.primary, {})

    def get_anti_bot_strategies(self, strategy: LibraryStrategy) -> Dict[str, str]:
        """ì•ˆí‹°ë´‡ ìš°íšŒ ì „ëµ"""

        strategies = {
            "headers_randomization": """
def get_random_headers() -> Dict[str, str]:
    return {
        'User-Agent': random.choice(USER_AGENTS),
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
        'Referer': 'http://example.com/',
        'Cache-Control': 'no-cache'
    }
            """,
            "rate_limiting": """
def rate_limited_request(url: str, delay: float = 1.0) -> requests.Response:
    time.sleep(delay)
    return requests.get(url, headers=get_random_headers())
            """,
            "proxy_rotation": """
class ProxyPool:
    def __init__(self, proxies: List[str]):
        self.proxies = proxies
        self.current = 0

    def get_proxy(self) -> str:
        proxy = self.proxies[self.current]
        self.current = (self.current + 1) % len(self.proxies)
        return proxy
            """,
            "session_management": """
def create_session() -> requests.Session:
    session = requests.Session()
    session.headers.update(get_random_headers())
    return session
            """
        }

        return {strategy: strategies.get(strategy, "") for strategy in strategy.anti_bot}

    def get_performance_optimization(self, strategy: LibraryStrategy) -> Dict[str, str]:
        """ì„±ëŠ¥ ìµœì í™” ê°€ì´ë“œ"""

        optimizations = {
            "single": """
# ë‹¨ì¼ ìŠ¤ë ˆë“œ ìµœì í™”
def single_thread_optimization():
    # ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì²˜ë¦¬
    for item in large_dataset:
        process_item(item)
        del item  # ë©”ëª¨ë¦¬ í•´ì œ
            """,
            "async": """
# ë¹„ë™ê¸° ìµœì í™”
async def async_optimization():
    async with aiohttp.ClientSession() as session:
        tasks = [fetch_data(session, url) for url in urls]
        results = await asyncio.gather(*tasks)
        return results
            """,
            "multiprocessing": """
# ë©€í‹°í”„ë¡œì„¸ì‹± ìµœì í™”
from concurrent.futures import ProcessPoolExecutor

def multiprocessing_optimization():
    with ProcessPoolExecutor(max_workers=8) as executor:
        futures = [executor.submit(process_data, chunk) for chunk in data_chunks]
        results = [f.result() for f in futures]
        return results
            """
        }

        return optimizations.get(strategy.performance_mode, {})

class WorldClassCrawler:
    """World-Class í¬ë¡¤ë§ ì‹œìŠ¤í…œ"""

    def __init__(self):
        self.strategy_guide = CrawlingStrategyGuide()
        self.session = None
        self.driver = None

    def crawl(self, url: str, page_type: PageType = PageType.STATIC,
              volume: str = "low") -> Optional[str]:
        """í†µí•© í¬ë¡¤ë§ ë©”ì„œë“œ"""

        strategy = self.strategy_guide.select_strategy(url, page_type, volume)
        guide = self.strategy_guide.get_implementation_guide(strategy)

        logger.info(f"ì„ íƒëœ ì „ëµ: {strategy.primary.value}")
        logger.info(f"í˜ì´ì§€ íƒ€ì…: {page_type.value}")
        logger.info(f"ë³¼ë¥¨: {volume}")

        try:
            if strategy.primary == CrawlingLibrary.REQUESTS_BS4:
                return self._crawl_with_requests(url)
            elif strategy.primary == CrawlingLibrary.SELENIUM:
                return self._crawl_with_selenium(url)
            elif strategy.primary == CrawlingLibrary.CLOUDSCRAPER:
                return self._crawl_with_cloudscraper(url)
            elif strategy.primary == CrawlingLibrary.AIOHTTP:
                return asyncio.run(self._crawl_with_aiohttp(url))
            else:
                logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬: {strategy.primary}")
                return None

        except Exception as e:
            logger.error(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            # Fallback ì „ëµ ì‹œë„
            if strategy.fallback:
                logger.info(f"Fallback ì „ëµ ì‹œë„: {strategy.fallback}")
                return self._try_fallback(url, strategy.fallback)
            return None

    def _crawl_with_requests(self, url: str) -> Optional[str]:
        """requests + BeautifulSoup í¬ë¡¤ë§"""
        try:
            headers = {
                'User-Agent': random.choice(self.strategy_guide.user_agents),
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
                'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            }
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()
            return response.text
        except Exception as e:
            logger.error(f"requests í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return None

    def _crawl_with_selenium(self, url: str) -> Optional[str]:
        """Selenium í¬ë¡¤ë§"""
        try:
            options = Options()
            options.add_argument('--headless')
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')

            driver = webdriver.Chrome(options=options)
            driver.get(url)
            content = driver.page_source
            driver.quit()
            return content
        except Exception as e:
            logger.error(f"Selenium í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return None

    def _crawl_with_cloudscraper(self, url: str) -> Optional[str]:
        """cloudscraper í¬ë¡¤ë§"""
        try:
            scraper = cloudscraper.create_scraper()
            response = scraper.get(url)
            return response.text
        except Exception as e:
            logger.error(f"cloudscraper í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return None

    async def _crawl_with_aiohttp(self, url: str) -> Optional[str]:
        """aiohttp ë¹„ë™ê¸° í¬ë¡¤ë§"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url) as response:
                    return await response.text()
        except Exception as e:
            logger.error(f"aiohttp í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return None

    def _try_fallback(self, url: str, fallback_library: CrawlingLibrary) -> Optional[str]:
        """Fallback ì „ëµ ì‹œë„"""
        try:
            if fallback_library == CrawlingLibrary.REQUESTS_BS4:
                return self._crawl_with_requests(url)
            elif fallback_library == CrawlingLibrary.SELENIUM:
                return self._crawl_with_selenium(url)
            elif fallback_library == CrawlingLibrary.CLOUDSCRAPER:
                return self._crawl_with_cloudscraper(url)
            else:
                return None
        except Exception as e:
            logger.error(f"Fallback ì „ëµ ì‹¤íŒ¨: {e}")
            return None

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - í¬ë¡¤ë§ ì „ëµ ê°€ì´ë“œ"""

    crawler = WorldClassCrawler()

    # í…ŒìŠ¤íŠ¸ URLë“¤
    test_urls = [
        ("https://example.com", PageType.STATIC, "low"),
        ("https://krx.co.kr", PageType.PROTECTED, "medium"),
        ("https://api.example.com/data", PageType.API, "high"),
    ]

    for url, page_type, volume in test_urls:
        print(f"\n{'='*50}")
        print(f"URL: {url}")
        print(f"í˜ì´ì§€ íƒ€ì…: {page_type.value}")
        print(f"ë³¼ë¥¨: {volume}")

        strategy = crawler.strategy_guide.select_strategy(url, page_type, volume)
        guide = crawler.strategy_guide.get_implementation_guide(strategy)

        print(f"ì„ íƒëœ ë¼ì´ë¸ŒëŸ¬ë¦¬: {strategy.primary.value}")
        print(f"ì„±ëŠ¥ ëª¨ë“œ: {strategy.performance_mode}")
        print(f"ì•ˆí‹°ë´‡ ì „ëµ: {strategy.anti_bot}")
        print(f"ì„¤ëª…: {guide.get('description', 'N/A')}")
        print(f"ì¥ì : {guide.get('pros', [])}")
        print(f"ë‹¨ì : {guide.get('cons', [])}")

if __name__ == "__main__":
    main()
