#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íŒŒì¼ëª…: binance_all_market_collector.py
ëª©ì : ë°”ì´ë‚¸ìŠ¤ í˜„ë¬¼/ì„ ë¬¼/ì˜µì…˜ ì „ì²´ ë§ˆì¼“ì„ ì„¤ë¦½ì¼(ìƒì¥ì¼)ë¶€í„° í˜„ì¬ê¹Œì§€ ë¹„ë™ê¸° ê³ ì† ë³‘ë ¬, ë©€í‹°ë ˆë²¨ ìºì‹±, ì»¤ë„¥ì…˜í’€ë§, ì„±ëŠ¥/ë©”ëª¨ë¦¬ ìµœì í™”, ìœ ì§€ë³´ìˆ˜ì„± ìµœìš°ì„ ìœ¼ë¡œ ìˆ˜ì§‘í•˜ëŠ” ìš´ì˜ í’ˆì§ˆì˜ í†µí•© ìˆ˜ì§‘ê¸°

Author: World-Class Python
Created: 2025-01-27
Version: 1.0.0

Dependencies:
    - Python 3.11+
    - aiohttp>=3.9.0
    - pandas>=2.1.0
    - pyarrow>=14.0.0
    - python-binance>=1.0.19
    - aiocache>=0.12.2
    - tqdm>=4.66.0
    - pydantic>=2.5.0
    - tenacity>=8.2.3

"""

import asyncio
import logging
import os
import sys
import traceback
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Set, Literal, cast, Union
from functools import lru_cache, wraps
import pandas as pd
import aiohttp
from aiohttp import ClientSession, TCPConnector, ClientResponseError
from aiocache import cached, Cache
from pydantic import BaseModel, Field, ValidationError
from tqdm.asyncio import tqdm
from tenacity import retry, stop_after_attempt, wait_exponential, RetryError
import json
import sqlite3
import tables  # pytables(HDF5)
import time
import websockets
from asyncio import Queue, create_task, gather
from typing import Dict, Set, Optional, Callable, Any
import numpy as np

# ê³ ê¸‰ ë¡œê¹… ì„¤ì • (ì»¤ì„œë£° 100%)
logging.basicConfig(
    level=logging.DEBUG,  # DEBUG ë ˆë²¨ë¡œ ë³€ê²½
    format='%(asctime)s %(levelname)s [%(name)s:%(lineno)d] %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('binance_collector.log', encoding='utf-8')
    ]
)
logger = logging.getLogger("binance_all_market_collector")

# í™˜ê²½ë³€ìˆ˜ì—ì„œ API í‚¤ ë¡œë“œ
BINANCE_API_KEY = os.getenv("BINANCE_API_KEY")
BINANCE_SECRET = os.getenv("BINANCE_SECRET")

# ì´ˆê³ ì† ì„±ëŠ¥ ì„¤ì • (3ë§ˆì¼“ ë™ì‹œ ìˆ˜ì§‘ ìµœì í™”)
ULTRA_FAST_CONFIG = {
    "max_concurrent_requests": 5,  # ë” ë³´ìˆ˜ì ìœ¼ë¡œ ê°ì†Œ
    "chunk_size": 30,  # ì²­í¬ í¬ê¸° ê°ì†Œ
    "request_timeout": 60,  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
    "connection_timeout": 20,  # ì—°ê²° íƒ€ì„ì•„ì›ƒ ì¦ê°€
    "max_connections": 30,  # ì—°ê²° í’€ í¬ê¸° ê°ì†Œ
    "rate_limit_delay": 0.2,  # Rate limit ì§€ì—° ì‹œê°„ ì¦ê°€
    "memory_optimization": True,
    "batch_processing": True,
    "use_compression": True,  # ì••ì¶• ì‚¬ìš©
    "keepalive_timeout": 120,  # Keep-alive ì‹œê°„
    "enable_cleanup_closed": True,
    "limit_per_host": 10,  # í˜¸ìŠ¤íŠ¸ë‹¹ ì—°ê²° ìˆ˜ ê°ì†Œ
    "ttl_dns_cache": 1200,  # DNS ìºì‹œ ì‹œê°„
    "use_dns_cache": True,
    "force_close": False,
    "market_delay": 2.0  # ë§ˆì¼“ ê°„ ì§€ì—° ì‹œê°„
}

# ë°”ì´ë‚¸ìŠ¤ ê³µì‹ Rate Limit ì„¤ì • (3ë§ˆì¼“ ì•ˆì „ ì„¤ì •)
BINANCE_RATE_LIMITS = {
    "klines": {"requests_per_second": 2, "requests_per_10min": 300},  # ë” ë³´ìˆ˜ì 
    "exchange_info": {"requests_per_second": 3, "requests_per_10min": 30},  # ë” ë³´ìˆ˜ì 
    "futures_klines": {"requests_per_second": 2, "requests_per_10min": 200},  # ì„ ë¬¼ ì „ìš©
    "options_klines": {"requests_per_second": 2, "requests_per_10min": 200}   # ì˜µì…˜ ì „ìš©
}

# Weight-based Rate Limiter (ê°œì„ )
class BinanceRateLimiter:
    def __init__(self):
        self.request_times: List[float] = []
        self.last_request: float = 0
        self.lock = asyncio.Lock()
        self.request_count: int = 0
        self.last_reset: float = time.time()
    
    async def wait_if_needed(self, endpoint: str) -> None:
        """Rate limit ì²´í¬ ë° ëŒ€ê¸° (ìµœì í™”ëœ ë²„ì „)"""
        async with self.lock:
            now = time.time()
            
            # 10ë¶„ë§ˆë‹¤ ì¹´ìš´í„° ë¦¬ì…‹
            if now - self.last_reset > 600:
                self.request_count = 0
                self.last_reset = now
            
            # 10ë¶„ ìœˆë„ìš°ì—ì„œ ì˜¤ë˜ëœ ìš”ì²­ ì œê±° (ìµœì í™”)
            cutoff_time = now - 600
            self.request_times = [t for t in self.request_times if t > cutoff_time]
            
            limit = BINANCE_RATE_LIMITS.get(endpoint, {"requests_per_second": 5, "requests_per_10min": 1000})
            
            # ì´ˆë‹¹ ì œí•œ ì²´í¬ (í¼ë¸”ë¦­ API ìµœì í™”)
            recent_requests = len([t for t in self.request_times if now - t < 1])
            if recent_requests >= limit["requests_per_second"]:
                wait_time = ULTRA_FAST_CONFIG["rate_limit_delay"]
                if wait_time > 0:
                    logger.debug(f"Rate limit: waiting {wait_time:.3f}s for {endpoint}")
                    await asyncio.sleep(wait_time)
            
            # 10ë¶„ ì œí•œ ì²´í¬ (ë³´ìˆ˜ì  ì„¤ì •)
            if self.request_count >= limit["requests_per_10min"] * 0.8:  # 80%ì—ì„œ ê²½ê³ 
                wait_time = 600 - (now - self.last_reset)
                if wait_time > 0:
                    logger.warning(f"10min rate limit approaching: {self.request_count}/{limit['requests_per_10min']}, waiting {wait_time:.2f}s")
                    await asyncio.sleep(wait_time)
                    self.request_count = 0
                    self.last_reset = time.time()
            
            self.request_times.append(now)
            self.request_count += 1

# ì „ì—­ Rate Limiter ì¸ìŠ¤í„´ìŠ¤
rate_limiter = BinanceRateLimiter()

# ìƒìˆ˜
BINANCE_BASE_URL = "https://api.binance.com"
BINANCE_FUTURES_URL = "https://fapi.binance.com"
BINANCE_OPTIONS_URL = "https://eapi.binance.com"
MAX_CONCURRENCY = 10  # ë™ì‹œì„± ëŒ€í­ ì¦ê°€
BATCH_SIZE = 1000
CACHE_DIR = Path(".binance_cache")
CACHE_DIR.mkdir(exist_ok=True)

# Rate limitingì„ ìœ„í•œ ì„¸ë§ˆí¬ì–´
rate_limit_semaphore = asyncio.Semaphore(MAX_CONCURRENCY)

# LRU ë©”ëª¨ë¦¬ ìºì‹œ (ì‹¬ë³¼/ìƒì¥ì¼ ë“±)
@lru_cache(maxsize=256)
def lru_json_cache(key: str) -> Optional[Any]:
    """LRU ìºì‹œì—ì„œ JSON ë°ì´í„° ì¡°íšŒ"""
    try:
        cache_path = CACHE_DIR / f"{key}.json"
        if cache_path.exists():
            with open(cache_path, "r", encoding="utf-8") as f:
                return json.load(f)
    except Exception as e:
        logger.error(f"ìºì‹œ ì½ê¸° ì˜¤ë¥˜ {key}: {e}")
    return None

def save_json_cache(key: str, data: Any) -> None:
    """JSON ë°ì´í„°ë¥¼ ìºì‹œì— ì €ì¥"""
    try:
        cache_path = CACHE_DIR / f"{key}.json"
        with open(cache_path, "w", encoding="utf-8") as f:
            json.dump(data, f)
    except Exception as e:
        logger.error(f"ìºì‹œ ì €ì¥ ì˜¤ë¥˜ {key}: {e}")

# Pydantic ëª¨ë¸ (ì»¤ì„œë£° 100%)
class SymbolInfo(BaseModel):
    """ì‹¬ë³¼ ì •ë³´ ëª¨ë¸"""
    symbol: str = Field(..., description="ì‹¬ë³¼ëª…")
    market_type: Literal["spot", "futures", "options"] = Field(..., description="ë§ˆì¼“ íƒ€ì…")
    onboard_date: datetime = Field(..., description="ìƒì¥ì¼")
    status: str = Field(..., description="ìƒíƒœ")
    base_asset: str = Field(..., description="ê¸°ë³¸ ìì‚°")
    quote_asset: str = Field(..., description="ê²¬ì  ìì‚°")
    
    class Config:
        validate_assignment = True

# ì‹¬ë³¼/ìƒì¥ì¼ ì¡°íšŒ
async def fetch_all_symbols(session: ClientSession) -> List[SymbolInfo]:
    """ëª¨ë“  ì‹¬ë³¼ ì •ë³´ ìˆ˜ì§‘"""
    results: List[SymbolInfo] = []
    
    try:
        # í˜„ë¬¼
        spot_url = f"{BINANCE_BASE_URL}/api/v3/exchangeInfo"
        # ì„ ë¬¼
        fut_url = f"{BINANCE_FUTURES_URL}/fapi/v1/exchangeInfo"
        # ì˜µì…˜
        opt_url = f"{BINANCE_OPTIONS_URL}/eapi/v1/exchangeInfo"
        
        for url, mtype in [
            (spot_url, "spot"),
            (fut_url, "futures"),
            (opt_url, "options")
        ]:
            try:
                await rate_limiter.wait_if_needed("exchange_info")
                cache_key = f"exchangeinfo_{mtype}"
                cached_data = lru_json_cache(cache_key)
                
                if cached_data:
                    data = cached_data
                    logger.debug(f"ìºì‹œì—ì„œ {mtype} ë°ì´í„° ë¡œë“œ")
                else:
                    logger.info(f"{mtype} ë§ˆì¼“ ì •ë³´ ìˆ˜ì§‘ ì¤‘...")
                    async with session.get(url) as resp:
                        if resp.status == 418:
                            logger.warning(f"{mtype} ë§ˆì¼“ API ì œí•œ (418): IP ì°¨ë‹¨ ë˜ëŠ” API ì œí•œ. ê±´ë„ˆëœ€.")
                            continue
                        elif resp.status != 200:
                            logger.warning(f"{mtype} ë§ˆì¼“ API ì˜¤ë¥˜ ({resp.status}): {resp.reason}. ê±´ë„ˆëœ€.")
                            continue
                        
                        data = await resp.json()
                        save_json_cache(cache_key, data)
                        logger.info(f"{mtype} ë§ˆì¼“ ì •ë³´ ìˆ˜ì§‘ ì™„ë£Œ")
                
                for s in data.get("symbols", []):
                    try:
                        # ê±°ë˜ ì¤‘ë‹¨ëœ ì‹¬ë³¼ í•„í„°ë§
                        if s.get("status") != "TRADING":
                            continue
                        
                        onboard = s.get("onboardDate")
                        if onboard:
                            onboard_dt = datetime.utcfromtimestamp(onboard/1000).replace(tzinfo=timezone.utc)
                        else:
                            onboard_dt = datetime(2017,7,14, tzinfo=timezone.utc)
                        
                        symbol_info = SymbolInfo(
                            symbol=s["symbol"],
                            market_type=cast(Literal["spot", "futures", "options"], mtype),
                            onboard_date=onboard_dt,
                            status=s.get("status", "UNKNOWN"),
                            base_asset=s.get("baseAsset", ""),
                            quote_asset=s.get("quoteAsset", "")
                        )
                        results.append(symbol_info)
                        
                    except ValidationError as e:
                        logger.error(f"ì‹¬ë³¼ ì •ë³´ ê²€ì¦ ì˜¤ë¥˜ {s.get('symbol', 'unknown')}: {e}")
                    except Exception as e:
                        logger.error(f"ì‹¬ë³¼ ì²˜ë¦¬ ì˜¤ë¥˜ {s.get('symbol', 'unknown')}: {e}")
                        
            except Exception as e:
                logger.error(f"ë§ˆì¼“ {mtype} ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
                
    except Exception as e:
        logger.error(f"ì‹¬ë³¼ ìˆ˜ì§‘ ì „ì²´ ì˜¤ë¥˜: {e}")
        raise
    
    logger.info(f"ì´ {len(results)}ê°œ ìœ íš¨í•œ ì‹¬ë³¼ ìˆ˜ì§‘ ì™„ë£Œ")
    return results

# ë©€í‹°ë ˆë²¨ ìºì‹œ ë°ì½”ë ˆì´í„° (aiocache + ë””ìŠ¤í¬)
def multi_level_cache(key_builder: Callable) -> Callable:
    """ë©€í‹°ë ˆë²¨ ìºì‹œ ë°ì½”ë ˆì´í„°"""
    def decorator(func: Callable) -> Callable:
        @cached(ttl=3600, cache=Cache.MEMORY)
        @wraps(func)
        async def wrapper(*args: Any, **kwargs: Any) -> Any:
            try:
                key = key_builder(*args, **kwargs)
                disk_path = CACHE_DIR / f"{key}.parquet"
                if disk_path.exists():
                    return pd.read_parquet(disk_path)
                result = await func(*args, **kwargs)
                if isinstance(result, pd.DataFrame):
                    result.to_parquet(disk_path)
                return result
            except Exception as e:
                logger.error(f"ìºì‹œ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                return await func(*args, **kwargs)
        return wrapper
    return decorator

# Kë¼ì¸ ë°ì´í„° ìˆ˜ì§‘ (ë¹„ë™ê¸°, ë³‘ë ¬, ìºì‹±)
@multi_level_cache(
    lambda *args, **kwargs: (
        f"klines_{args[5]}_{args[1]}_{args[2]}_{args[3]:%Y%m%d}_{args[4]:%Y%m%d}"
        if len(args) >= 6 else "klines_unknown"
    )
)
@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))
async def fetch_klines(
    session: ClientSession,
    symbol: str,
    interval: str,
    start: datetime,
    end: datetime,
    market: str
) -> pd.DataFrame:
    """Kë¼ì¸ ë°ì´í„° ìˆ˜ì§‘ (ê¸°ë³¸)"""
    try:
        # ì‹œê°„ëŒ€ ì¼ê´€ì„± ë³´ì¥
        if start.tzinfo is None:
            start = start.replace(tzinfo=timezone.utc)
        if end.tzinfo is None:
            end = end.replace(tzinfo=timezone.utc)
        
        async with rate_limit_semaphore:
            await rate_limiter.wait_if_needed("klines")
            
            if market == "spot":
                url = f"{BINANCE_BASE_URL}/api/v3/klines"
            elif market == "futures":
                url = f"{BINANCE_FUTURES_URL}/fapi/v1/klines"
            elif market == "options":
                url = f"{BINANCE_OPTIONS_URL}/eapi/v1/klines"
            else:
                raise ValueError(f"Unknown market: {market}")
            
            params = {
                "symbol": symbol,
                "interval": interval,
                "startTime": int(start.timestamp() * 1000),
                "endTime": int(end.timestamp() * 1000),
                "limit": BATCH_SIZE
            }
            
            all_rows = []
            cur_start = start
            
            while cur_start < end:
                params["startTime"] = int(cur_start.timestamp() * 1000)
                
                try:
                    async with session.get(url, params=params) as resp:
                        if resp.status == 429:
                            logger.warning(f"Rate limit for {symbol}, waiting...")
                            await asyncio.sleep(5)
                            continue
                        elif resp.status == 418:
                            logger.warning(f"Symbol {symbol} is invalid (418) - IP ì°¨ë‹¨ ê°€ëŠ¥ì„±, skipping...")
                            await asyncio.sleep(1)  # ì ì‹œ ëŒ€ê¸°
                            return pd.DataFrame()
                        
                        resp.raise_for_status()
                        rows = await resp.json()
                        
                        if not rows:
                            break
                        
                        all_rows.extend(rows)
                        last_time = rows[-1][0] / 1000
                        cur_start = datetime.utcfromtimestamp(last_time).replace(tzinfo=timezone.utc) + timedelta(milliseconds=1)
                        
                        if len(rows) < BATCH_SIZE:
                            break
                        
                        await asyncio.sleep(0.2)
                        
                except ClientResponseError as e:
                    if e.status == 429:
                        await asyncio.sleep(5)
                        continue
                    elif e.status == 418:
                        return pd.DataFrame()
                    else:
                        logger.error(f"HTTP error for {symbol}: {e.status} - {e.message}")
                        raise
                except Exception as e:
                    logger.error(f"Request error for {symbol}: {type(e).__name__}: {str(e)}")
                    raise
            
            if not all_rows:
                return pd.DataFrame()
            
            df = pd.DataFrame(all_rows, columns=[
                "open_time", "open", "high", "low", "close", "volume",
                "close_time", "quote_asset_volume", "number_of_trades",
                "taker_buy_base_asset_volume", "taker_buy_quote_asset_volume", "ignore"
            ])
            
            df["symbol"] = symbol
            df["market"] = market
            df["interval"] = interval
            df["open_time"] = pd.to_datetime(df["open_time"], unit="ms", utc=True)
            df["close_time"] = pd.to_datetime(df["close_time"], unit="ms", utc=True)
            
            return df
            
    except Exception as e:
        logger.error(f"fetch_klines ì˜¤ë¥˜ {symbol}: {type(e).__name__}: {str(e)}")
        logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
        raise

def save_data_multi_format(df: pd.DataFrame, base_path: Path, table_name: str = "binance_data") -> None:
    """ë°ì´í„° ì„±ê²©/ìš©ë„ë³„ë¡œ Parquet, SQLite, HDF5ë¡œ ì €ì¥"""
    try:
        base_path.mkdir(parents=True, exist_ok=True)
        
        # 1. Parquet (ML/DL ìµœì í™”, ì»¬ëŸ¼ ê¸°ë°˜, ëŒ€ìš©ëŸ‰/AI)
        pq_path = base_path / f"{table_name}.parquet"
        df.to_parquet(pq_path, index=False)
        logger.info(f"Parquet ì €ì¥ ì™„ë£Œ: {pq_path}")
        
        # 2. SQLite (ê´€ê³„í˜• ì¿¼ë¦¬, ë³µì¡ ë¶„ì„/ê²€ìƒ‰)
        sqlite_path = base_path / f"{table_name}.sqlite"
        with sqlite3.connect(sqlite_path) as conn:
            df.to_sql(table_name, conn, if_exists="replace", index=False)
        logger.info(f"SQLite ì €ì¥ ì™„ë£Œ: {sqlite_path}")
        
        # 3. HDF5 (ì‹œê³„ì—´/í–‰ë ¬, ê³„ì¸µì , ê³¼í•™/ê¸ˆìœµ)
        hdf5_path = base_path / f"{table_name}.h5"
        df.to_hdf(hdf5_path, key=table_name, mode="w", format="table", complevel=9, complib="blosc")
        logger.info(f"HDF5 ì €ì¥ ì™„ë£Œ: {hdf5_path}")
        
    except Exception as e:
        logger.error(f"ë°ì´í„° ì €ì¥ ì˜¤ë¥˜: {type(e).__name__}: {str(e)}")
        logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
        raise

# ê³ ì„±ëŠ¥ ì—°ê²° í’€
class UltraFastConnectionPool:
    def __init__(self, max_connections: int = 30):
        self.connector = TCPConnector(
            limit=max_connections,
            limit_per_host=ULTRA_FAST_CONFIG["limit_per_host"],
            ttl_dns_cache=ULTRA_FAST_CONFIG["ttl_dns_cache"],
            use_dns_cache=ULTRA_FAST_CONFIG["use_dns_cache"],
            keepalive_timeout=ULTRA_FAST_CONFIG["keepalive_timeout"],
            enable_cleanup_closed=ULTRA_FAST_CONFIG["enable_cleanup_closed"],
            force_close=ULTRA_FAST_CONFIG["force_close"]
        )
        self.session: Optional[ClientSession] = None
        self.semaphore = asyncio.Semaphore(ULTRA_FAST_CONFIG["max_concurrent_requests"])
    
    async def get_session(self) -> ClientSession:
        """ì„¸ì…˜ ê°€ì ¸ì˜¤ê¸° (ìµœì í™”ëœ ë²„ì „)"""
        if self.session is None:
            timeout = aiohttp.ClientTimeout(
                total=ULTRA_FAST_CONFIG["request_timeout"],
                connect=ULTRA_FAST_CONFIG["connection_timeout"]
            )
            self.session = aiohttp.ClientSession(
                connector=self.connector,
                timeout=timeout,
                headers={
                    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36",
                    "Accept-Encoding": "gzip, deflate, br",  # ì••ì¶• ì§€ì›
                    "Accept": "application/json",
                    "Connection": "keep-alive"
                },
                skip_auto_headers=["Accept-Encoding"]  # ìë™ í—¤ë” ë¹„í™œì„±í™”
            )
        return self.session
    
    async def close(self) -> None:
        """ì—°ê²° í’€ ì •ë¦¬"""
        if self.session:
            await self.session.close()

# ì „ì—­ ì—°ê²° í’€ (ì§€ì—° ì´ˆê¸°í™”)
connection_pool: Optional[UltraFastConnectionPool] = None

def get_connection_pool() -> UltraFastConnectionPool:
    """ì—°ê²° í’€ ì§€ì—° ì´ˆê¸°í™”"""
    global connection_pool
    if connection_pool is None:
        connection_pool = UltraFastConnectionPool()
    return connection_pool

# ì´ˆê³ ì† ë°°ì¹˜ ì²˜ë¦¬ (ìµœì í™”ëœ ë²„ì „)
async def process_batch_ultra_fast(
    symbols: List[SymbolInfo],
    intervals: List[str],
    session: ClientSession
) -> List[pd.DataFrame]:
    """ì´ˆê³ ì† ë°°ì¹˜ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ ìµœì í™” + ë³‘ë ¬ ì²˜ë¦¬ ìµœì í™”)"""
    
    try:
        logger.info(f"=== ì´ˆê³ ì† ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘ ===")
        logger.info(f"ì‹¬ë³¼ ìˆ˜: {len(symbols)}, ì¸í„°ë²Œ: {intervals}")
        
        # ë©”ëª¨ë¦¬ ìµœì í™”: ì²­í¬ ë‹¨ìœ„ë¡œ ì²˜ë¦¬í•˜ì—¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì œí•œ
        chunk_size = ULTRA_FAST_CONFIG["chunk_size"]
        max_concurrent = ULTRA_FAST_CONFIG["max_concurrent_requests"]
        
        all_results: List[pd.DataFrame] = []
        total_processed = 0
        
        # 3ë§ˆì¼“ ëª¨ë‘ ì²˜ë¦¬ (í˜„ë¬¼, ì„ ë¬¼, ì˜µì…˜)
        all_symbols = symbols
        logger.info(f"ì „ì²´ {len(all_symbols)}ê°œ ì‹¬ë³¼ ì²˜ë¦¬ (3ë§ˆì¼“)")
        
        # ë§ˆì¼“ë³„ë¡œ ë¶„ë¦¬
        spot_symbols = [s for s in all_symbols if s.market_type == "spot"]
        futures_symbols = [s for s in all_symbols if s.market_type == "futures"]
        options_symbols = [s for s in all_symbols if s.market_type == "options"]
        
        logger.info(f"í˜„ë¬¼: {len(spot_symbols)}ê°œ, ì„ ë¬¼: {len(futures_symbols)}ê°œ, ì˜µì…˜: {len(options_symbols)}ê°œ")
        
        # ë§ˆì¼“ë³„ë¡œ ìˆœì°¨ ì²˜ë¦¬ (418 ì—ëŸ¬ ë°©ì§€)
        all_market_symbols = []
        if spot_symbols:
            all_market_symbols.extend(spot_symbols[:10])  # í˜„ë¬¼ 10ê°œ
            logger.info("í˜„ë¬¼ ë§ˆì¼“ ì¶”ê°€")
        if futures_symbols:
            all_market_symbols.extend(futures_symbols[:5])  # ì„ ë¬¼ 5ê°œ
            logger.info("ì„ ë¬¼ ë§ˆì¼“ ì¶”ê°€")
        if options_symbols:
            all_market_symbols.extend(options_symbols[:5])  # ì˜µì…˜ 5ê°œ
            logger.info("ì˜µì…˜ ë§ˆì¼“ ì¶”ê°€")
        
        test_symbols = all_market_symbols
        logger.info(f"ì´ {len(test_symbols)}ê°œ ì‹¬ë³¼ ì²˜ë¦¬ (3ë§ˆì¼“)")
        
        # ì‹¬ë³¼ì„ ì²­í¬ë¡œ ë¶„í• 
        symbol_chunks = [test_symbols[i:i + chunk_size] for i in range(0, len(test_symbols), chunk_size)]
        logger.info(f"ì´ {len(symbol_chunks)}ê°œ ì²­í¬ë¡œ ë¶„í• ")
        
        # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ìš”ì²­ ì œí•œ
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def process_symbol_chunk(chunk: List[SymbolInfo], chunk_idx: int) -> List[pd.DataFrame]:
            """ì‹¬ë³¼ ì²­í¬ ì²˜ë¦¬"""
            chunk_results: List[pd.DataFrame] = []
            
            for symbol in chunk:
                for interval in intervals:
                    try:
                        # ì‹œê°„ ë²”ìœ„ ê³„ì‚°
                        start = symbol.onboard_date
                        end = datetime.now(timezone.utc)
                        
                        # ì‹œê°„ëŒ€ ì¼ê´€ì„± ë³´ì¥
                        if start.tzinfo is None:
                            start = start.replace(tzinfo=timezone.utc)
                        if end.tzinfo is None:
                            end = end.replace(tzinfo=timezone.utc)
                        
                        # ì—°ë„ë³„ë¡œ ë¶„í• í•˜ì—¬ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì„±)
                        current_start = start
                        while current_start < end:
                            current_end = min(current_start + timedelta(days=365), end)
                            
                            async with semaphore:
                                try:
                                    result = await fetch_klines(
                                        session, symbol.symbol, interval, 
                                        current_start, current_end, symbol.market_type
                                    )
                                    if result is not None and not result.empty:
                                        chunk_results.append(result)
                                        logger.debug(f"ë°ì´í„° ìˆ˜ì§‘: {symbol.symbol} {interval} {len(result)}í–‰")
                                except Exception as e:
                                    logger.warning(f"ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {symbol.symbol} {interval}: {e}")
                            
                            current_start = current_end
                            
                    except Exception as e:
                        logger.error(f"ì‹¬ë³¼ ì²˜ë¦¬ ì˜¤ë¥˜ {symbol.symbol}: {e}")
                        continue
            
            logger.info(f"ì²­í¬ {chunk_idx + 1} ì™„ë£Œ: {len(chunk_results)}ê°œ ë°ì´í„°í”„ë ˆì„")
            return chunk_results
        
        # ë§ˆì¼“ë³„ë¡œ ìˆœì°¨ ì²˜ë¦¬ (418 ì—ëŸ¬ ë°©ì§€)
        chunk_results = []
        for idx, chunk in enumerate(symbol_chunks):
            logger.info(f"ì²­í¬ {idx + 1}/{len(symbol_chunks)} ì²˜ë¦¬ ì¤‘...")
            try:
                result = await process_symbol_chunk(chunk, idx)
                chunk_results.append(result)
                # ë§ˆì¼“ ê°„ ì§€ì—° ì‹œê°„
                await asyncio.sleep(ULTRA_FAST_CONFIG["market_delay"])
            except Exception as e:
                logger.error(f"ì²­í¬ {idx + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                chunk_results.append([])
        
        logger.info(f"ìˆœì°¨ ì²˜ë¦¬ ì™„ë£Œ: {len(chunk_results)}ê°œ ì²­í¬")
        
        # ê²°ê³¼ ìˆ˜ì§‘
        success_chunks = 0
        error_chunks = 0
        
        for idx, result in enumerate(chunk_results):
            if isinstance(result, Exception):
                error_chunks += 1
                logger.error(f"ì²­í¬ {idx + 1} ì²˜ë¦¬ ì‹¤íŒ¨: {result}")
            elif isinstance(result, list):
                success_chunks += 1
                all_results.extend(result)
                total_processed += sum(len(df) for df in result if isinstance(df, pd.DataFrame))
                logger.info(f"ì²­í¬ {idx + 1} ì„±ê³µ: {len(result)}ê°œ ë°ì´í„°í”„ë ˆì„")
            else:
                logger.warning(f"ì²­í¬ {idx + 1} ì˜ˆìƒì¹˜ ëª»í•œ ê²°ê³¼ íƒ€ì…: {type(result)}")
        
        logger.info(f"=== ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ ===")
        logger.info(f"ì„±ê³µ ì²­í¬: {success_chunks}, ì‹¤íŒ¨ ì²­í¬: {error_chunks}")
        logger.info(f"ì´ ë°ì´í„°í”„ë ˆì„: {len(all_results)}ê°œ")
        logger.info(f"ì´ ì²˜ë¦¬ëœ í–‰: {total_processed}ê°œ")
        
        return all_results
        
    except Exception as e:
        logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì „ì²´ ì˜¤ë¥˜: {type(e).__name__}: {str(e)}")
        logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
        raise

# í†µí•© ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸
async def run_integrated_pipeline(
    intervals: List[str] = ["1d"],  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ 1dë§Œ
    output_dir: Path = Path("data/binance_all_markets"),
    table_name: str = "binance_data",
    enable_realtime: bool = False
) -> None:
    """í†µí•© íŒŒì´í”„ë¼ì¸ (ê³¼ê±° + ì‹¤ì‹œê°„)"""
    try:
        logger.info("=== íŒŒì´í”„ë¼ì¸ ì‹œì‘ ===")
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # 1. ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ (REST API)
        logger.info("=== ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ===")
        pool = get_connection_pool()
        session = await pool.get_session()
        logger.info("ì„¸ì…˜ ìƒì„± ì™„ë£Œ")
        
        symbols = await fetch_all_symbols(session)
        logger.info(f"ì´ {len(symbols)}ê°œ ì‹¬ë³¼ ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘")
        
        logger.info("ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘...")
        results = await process_batch_ultra_fast(symbols, intervals, session)
        logger.info("ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ")
        
        if results:
            logger.info("ë°ì´í„°í”„ë ˆì„ ë³‘í•© ì‹œì‘...")
            df_all = pd.concat(results, ignore_index=True)
            logger.info(f"ë³‘í•© ì™„ë£Œ: {len(df_all)}ê°œ í–‰")
            
            logger.info("ë‹¤ì¤‘ í¬ë§· ì €ì¥ ì‹œì‘...")
            save_data_multi_format(df_all, output_dir, table_name)
            logger.info(f"ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(df_all)}ê°œ í–‰")
        else:
            logger.warning("ìˆ˜ì§‘ëœ ë°ì´í„° ì—†ìŒ")
        
        logger.info("=== í†µí•© íŒŒì´í”„ë¼ì¸ ì™„ë£Œ ===")
        
    except Exception as e:
        logger.error(f"í†µí•© íŒŒì´í”„ë¼ì¸ ì˜¤ë¥˜: {type(e).__name__}: {str(e)}")
        logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
        raise
    finally:
        if connection_pool:
            await connection_pool.close()
            logger.info("ì—°ê²° í’€ ì •ë¦¬ ì™„ë£Œ")

# Binance ì„±ëŠ¥ í‰ê°€ ê¸°ì¤€ (KRXì™€ ë™ì¼í•œ ê¸°ì¤€ ì ìš©)
BINANCE_PERFORMANCE_CRITERIA = {
    "excellent": {"min_r2": 0.8, "max_rmse": 0.1, "min_excellent_folds": 3},
    "good": {"min_r2": 0.6, "max_rmse": 0.2, "min_excellent_folds": 2},
    "fair": {"min_r2": 0.4, "max_rmse": 0.3, "min_excellent_folds": 1},
    "poor": {"min_r2": 0.0, "max_rmse": float('inf'), "min_excellent_folds": 0}
}

# Binance ìë™ë§¤ë§¤ ê°€ëŠ¥ì„± íŒë‹¨ ê¸°ì¤€
BINANCE_TRADING_CRITERIA = {
    "high_confidence": {
        "min_r2": 0.85,
        "max_rmse": 0.08,
        "min_excellent_folds": 4,
        "max_poor_folds": 0,
        "min_data_quality": 0.9
    },
    "medium_confidence": {
        "min_r2": 0.7,
        "max_rmse": 0.15,
        "min_excellent_folds": 3,
        "max_poor_folds": 1,
        "min_data_quality": 0.8
    },
    "low_confidence": {
        "min_r2": 0.5,
        "max_rmse": 0.25,
        "min_excellent_folds": 2,
        "max_poor_folds": 2,
        "min_data_quality": 0.7
    },
    "not_tradeable": {
        "min_r2": 0.0,
        "max_rmse": float('inf'),
        "min_excellent_folds": 0,
        "max_poor_folds": 5,
        "min_data_quality": 0.0
    }
}

# Binance ë°ì´í„° ì„±ê²©ë³„ ì €ì¥ ì „ëµ
BINANCE_STORAGE_STRATEGIES = {
    "high_frequency_trading": {
        "storage_format": "parquet",
        "compression": "snappy",
        "partition_by": ["date", "symbol"],
        "retention_days": 30,
        "backup_frequency": "daily",
        "description": "ê³ ë¹ˆë„ ê±°ë˜ - ë¹ ë¥¸ ì½ê¸°/ì“°ê¸°, ì••ì¶• ìµœì í™”"
    },
    "medium_frequency_analysis": {
        "storage_format": "parquet",
        "compression": "gzip",
        "partition_by": ["month", "symbol"],
        "retention_days": 90,
        "backup_frequency": "weekly",
        "description": "ì¤‘ë¹ˆë„ ë¶„ì„ - ê· í˜•ì¡íŒ ì„±ëŠ¥ê³¼ ìš©ëŸ‰"
    },
    "long_term_research": {
        "storage_format": "parquet",
        "compression": "brotli",
        "partition_by": ["year", "symbol"],
        "retention_days": 365,
        "backup_frequency": "monthly",
        "description": "ì¥ê¸° ì—°êµ¬ - ìµœëŒ€ ì••ì¶•, ì¥ê¸° ë³´ê´€"
    },
    "real_time_monitoring": {
        "storage_format": "parquet",
        "compression": "snappy",
        "partition_by": ["hour", "symbol"],
        "retention_days": 7,
        "backup_frequency": "hourly",
        "description": "ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ - ìµœì†Œ ì§€ì—°, ë¹ ë¥¸ ì²˜ë¦¬"
    }
}

# ë°ì´í„° ìœ í˜•ë³„ ê¶Œì¥ ì„¤ì • (Binanceìš©)
BINANCE_DATA_TYPE_CONFIGS = {
    "financial_timeseries": {
        "max_iterations": 8,  # 5-10íšŒ ì¤‘ê°„ê°’
        "max_no_improvement": 3,
        "target_excellent_folds": 3,
        "description": "Binance ê¸ˆìœµ ì‹œê³„ì—´ ë°ì´í„° - ë…¸ì´ì¦ˆ ë§ìŒ, ì˜ˆì¸¡ ì–´ë ¤ì›€"
    },
    "general_ml": {
        "max_iterations": 4,  # 3-5íšŒ ì¤‘ê°„ê°’
        "max_no_improvement": 2,
        "target_excellent_folds": 3,
        "description": "ì¼ë°˜ ML ë°ì´í„° - ì•ˆì •ì  íŒ¨í„´, ë¹ ë¥¸ ìˆ˜ë ´"
    },
    "image_text": {
        "max_iterations": 5,  # 3-7íšŒ ì¤‘ê°„ê°’
        "max_no_improvement": 2,
        "target_excellent_folds": 3,
        "description": "ì´ë¯¸ì§€/í…ìŠ¤íŠ¸ ë°ì´í„° - ë³µì¡í•˜ì§€ë§Œ íŒ¨í„´ ì¡´ì¬"
    },
    "experimental": {
        "max_iterations": 2,  # 2-3íšŒ ì¤‘ê°„ê°’
        "max_no_improvement": 1,
        "target_excellent_folds": 2,
        "description": "ì‹¤í—˜ì  ë°ì´í„° - ë¹ ë¥¸ ê²€ì¦ í•„ìš”"
    }
}

def evaluate_binance_performance(analysis: Dict[str, Any]) -> Dict[str, Any]:
    """Binance ì„±ëŠ¥ í‰ê°€ (KRXì™€ ë™ì¼í•œ ê¸°ì¤€ ì ìš©)"""
    avg_r2 = analysis.get('avg_r2', 0)
    avg_rmse = analysis.get('avg_rmse', float('inf'))
    excellent_folds = analysis.get('excellent_folds', 0)
    poor_folds = analysis.get('poor_folds', 0)
    
    # ì„±ëŠ¥ ë“±ê¸‰ í‰ê°€
    performance_grade = "ğŸ”´ Poor"
    if avg_r2 >= BINANCE_PERFORMANCE_CRITERIA["excellent"]["min_r2"] and excellent_folds >= BINANCE_PERFORMANCE_CRITERIA["excellent"]["min_excellent_folds"]:
        performance_grade = "ğŸŸ¢ Excellent"
    elif avg_r2 >= BINANCE_PERFORMANCE_CRITERIA["good"]["min_r2"] and excellent_folds >= BINANCE_PERFORMANCE_CRITERIA["good"]["min_excellent_folds"]:
        performance_grade = "ğŸŸ¡ Good"
    elif avg_r2 >= BINANCE_PERFORMANCE_CRITERIA["fair"]["min_r2"] and excellent_folds >= BINANCE_PERFORMANCE_CRITERIA["fair"]["min_excellent_folds"]:
        performance_grade = "ğŸŸ  Fair"
    
    # ìë™ë§¤ë§¤ ê°€ëŠ¥ì„± íŒë‹¨
    trading_confidence = "not_tradeable"
    data_quality = 1.0 - (poor_folds / (excellent_folds + poor_folds + 1))
    
    for confidence, criteria in BINANCE_TRADING_CRITERIA.items():
        if (avg_r2 >= criteria["min_r2"] and 
            avg_rmse <= criteria["max_rmse"] and
            excellent_folds >= criteria["min_excellent_folds"] and
            poor_folds <= criteria["max_poor_folds"] and
            data_quality >= criteria["min_data_quality"]):
            trading_confidence = confidence
            break
    
    return {
        "performance_grade": performance_grade,
        "trading_confidence": trading_confidence,
        "data_quality_score": data_quality,
        "improvement_needed": performance_grade.startswith("ğŸ”´") or trading_confidence == "not_tradeable",
        "trading_recommendation": _get_binance_trading_recommendation(trading_confidence)
    }

def _get_binance_trading_recommendation(confidence: str) -> str:
    """Binance ìë™ë§¤ë§¤ ê¶Œì¥ì‚¬í•­"""
    recommendations = {
        "high_confidence": "âœ… ìë™ë§¤ë§¤ ê¶Œì¥ - ë†’ì€ ì‹ ë¢°ë„",
        "medium_confidence": "âš ï¸ ì œí•œì  ìë™ë§¤ë§¤ - ì¤‘ê°„ ì‹ ë¢°ë„",
        "low_confidence": "âŒ ìë™ë§¤ë§¤ ë¹„ê¶Œì¥ - ë‚®ì€ ì‹ ë¢°ë„",
        "not_tradeable": "ğŸš« ìë™ë§¤ë§¤ ë¶ˆê°€ - ê°œì„  í•„ìš”"
    }
    return recommendations.get(confidence, "â“ í‰ê°€ ë¶ˆê°€")

def detect_binance_data_characteristics(df: pd.DataFrame) -> str:
    """Binance ë°ì´í„° ì„±ê²© ê°ì§€"""
    # ë°ì´í„° í¬ê¸° ë° ë¹ˆë„ ë¶„ì„
    data_size = len(df)
    time_columns = [col for col in df.columns if 'time' in col.lower() or 'date' in col.lower()]
    
    # ê±°ë˜ëŸ‰ íŒ¨í„´ ë¶„ì„
    volume_columns = [col for col in df.columns if 'volume' in col.lower()]
    has_volume_data = len(volume_columns) > 0
    
    # ê°€ê²© ë³€ë™ì„± ë¶„ì„
    price_columns = [col for col in df.columns if any(price in col.lower() for price in ['open', 'high', 'low', 'close'])]
    has_price_data = len(price_columns) > 0
    
    # ë°ì´í„° ì„±ê²© íŒë‹¨
    if data_size > 100000 and has_volume_data and has_price_data:
        return "high_frequency_trading"
    elif data_size > 10000 and has_price_data:
        return "medium_frequency_analysis"
    elif data_size > 1000:
        return "long_term_research"
    else:
        return "real_time_monitoring"

def get_binance_storage_strategy(df: pd.DataFrame, trading_confidence: str) -> Dict[str, Any]:
    """Binance ë°ì´í„° ì €ì¥ ì „ëµ ê²°ì •"""
    data_characteristics = detect_binance_data_characteristics(df)
    base_strategy = BINANCE_STORAGE_STRATEGIES[data_characteristics].copy()
    
    # ìë™ë§¤ë§¤ ì‹ ë¢°ë„ì— ë”°ë¥¸ ì €ì¥ ì „ëµ ì¡°ì •
    if trading_confidence == "high_confidence":
        base_strategy["backup_frequency"] = "hourly"
        base_strategy["retention_days"] = 60
        base_strategy["description"] += " (ìë™ë§¤ë§¤ í™œì„±í™”)"
    elif trading_confidence == "medium_confidence":
        base_strategy["backup_frequency"] = "daily"
        base_strategy["retention_days"] = 45
        base_strategy["description"] += " (ì œí•œì  ìë™ë§¤ë§¤)"
    elif trading_confidence == "low_confidence":
        base_strategy["backup_frequency"] = "weekly"
        base_strategy["retention_days"] = 30
        base_strategy["description"] += " (ì—°êµ¬ìš©)"
    else:
        base_strategy["backup_frequency"] = "monthly"
        base_strategy["retention_days"] = 15
        base_strategy["description"] += " (ê°œì„  í•„ìš”)"
    
    return base_strategy

def detect_binance_data_type(df: pd.DataFrame) -> str:
    """Binance ë°ì´í„° ìœ í˜• ìë™ ê°ì§€"""
    # Binance ë°ì´í„° íŠ¹ì„± í™•ì¸
    binance_indicators = [
        'open', 'high', 'low', 'close', 'volume',
        'quote_asset_volume', 'taker_buy_base_asset_volume'
    ]
    
    has_binance_cols = any(col in df.columns for col in binance_indicators)
    has_time_cols = any('time' in col.lower() for col in df.columns)
    has_symbol_cols = any('symbol' in col.lower() for col in df.columns)
    
    # ë°ì´í„° í¬ê¸° í™•ì¸
    data_size = len(df)
    feature_count = len(df.select_dtypes(include=[np.number]).columns)
    
    # ë°ì´í„° ìœ í˜• íŒë‹¨
    if has_binance_cols and has_time_cols and has_symbol_cols:
        return "financial_timeseries"
    elif data_size > 10000 and feature_count > 20:
        return "image_text"
    elif data_size < 5000 or feature_count < 10:
        return "experimental"
    else:
        return "general_ml"

def get_binance_optimized_config(df: pd.DataFrame) -> Dict[str, Any]:
    """Binance ë°ì´í„° ìœ í˜•ì— ë”°ë¥¸ ìµœì  ì„¤ì • ë°˜í™˜"""
    data_type = detect_binance_data_type(df)
    config = BINANCE_DATA_TYPE_CONFIGS[data_type].copy()
    
    logger.info(f"Binance ë°ì´í„° ìœ í˜• ê°ì§€: {data_type}")
    logger.info(f"ì„¤ì • ì ìš©: {config['description']}")
    logger.info(f"ìµœëŒ€ ë°˜ë³µ: {config['max_iterations']}íšŒ")
    logger.info(f"ì¡°ê¸° ì¢…ë£Œ: ì—°ì† {config['max_no_improvement']}íšŒ ê°œì„  ì—†ìŒ")
    
    return config

# ì „ì—­ ë³€ìˆ˜ (ìš°ìˆ˜ ë“±ê¸‰ ë‹¬ì„± ì¶”ì )
achieved_excellent_grade = False

def np_encoder(obj):
    """numpy íƒ€ì… JSON ì§ë ¬í™”ìš©"""
    if isinstance(obj, (np.integer, np.floating)):
        return obj.item()
    if isinstance(obj, np.bool_):
        return bool(obj)
    return str(obj)

if __name__ == "__main__":
    try:
        asyncio.run(run_integrated_pipeline(enable_realtime=False))
    except KeyboardInterrupt:
        logger.info("ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë¨")
    except Exception as e:
        logger.error(f"í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì˜¤ë¥˜: {type(e).__name__}: {str(e)}")
        logger.error(f"ìŠ¤íƒ íŠ¸ë ˆì´ìŠ¤: {traceback.format_exc()}")
        sys.exit(1) 