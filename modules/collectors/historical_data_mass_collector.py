#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íŒŒì¼ëª…: historical_data_mass_collector.py
ëª©ì : ML/DL í•™ìŠµìš© ëŒ€ìš©ëŸ‰ ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ (KRX, KIS, DART, ë‰´ìŠ¤, ì•¼í›„, ë„¤ì´ë²„, ë‹¤ìŒ)
Author: [Your Name]
Created: 2025-07-11
Version: 1.0.0

- ì»¤ì„œë£° 100% ì¤€ìˆ˜ (íƒ€ì…íŒíŠ¸, ì˜ˆì™¸ì²˜ë¦¬, êµ¬ì¡°í™” ë¡œê¹…, ë¬¸ì„œí™”, í…ŒìŠ¤íŠ¸í¬ì¸íŠ¸)
- ëŒ€ìš©ëŸ‰ ë³‘ë ¬ ìˆ˜ì§‘, ë©€í‹°ì†ŒìŠ¤ í†µí•©, ML/DL ìµœì í™” ì €ì¥
- ì‹¤ì‹œê°„ ì§„í–‰ë¥ , ìë™ ì¬ì‹œë„, ë©”ëª¨ë¦¬ ìµœì í™”
"""

from __future__ import annotations
import asyncio
import logging
from datetime import datetime, timedelta
from pathlib import Path
from typing import List, Dict, Optional, Any, Set
import os
import time

import pandas as pd
import numpy as np
from sqlalchemy import create_engine
import diskcache
import yfinance as yf
from pykrx import stock
import aiohttp
from tqdm import tqdm

# êµ¬ì¡°í™” ë¡œê¹…
logging.basicConfig(
    filename="logs/historical_mass_collector.log",
    level=logging.INFO,
    format="%(asctime)s %(levelname)s %(message)s"
)
logger = logging.getLogger(__name__)

# ìºì‹œ
CACHE_DIR = Path("cache/historical")
CACHE_DIR.mkdir(parents=True, exist_ok=True)
cache = diskcache.Cache(str(CACHE_DIR))

# DB ì—°ê²°
engine = create_engine("postgresql+psycopg2://user:password@localhost:5432/stockdb")

# ìˆ˜ì§‘ ê¸°ê°„ ì„¤ì • (ML/DL í•™ìŠµìš© ëŒ€ìš©ëŸ‰)
START_DATE = "2010-01-01"  # 15ë…„ì¹˜ ë°ì´í„°
END_DATE = datetime.now().strftime("%Y-%m-%d")

# í•œêµ­ ì£¼ìš” ì¢…ëª© (ì‹œê°€ì´ì•¡ ìƒìœ„ 100ê°œ)
KOREAN_MAJOR_STOCKS = [
    "005930", "000660", "051900", "035420", "006400",  # ì‚¼ì„±ì „ì, SKí•˜ì´ë‹‰ìŠ¤, LGí™”í•™, NAVER, ì‚¼ì„±SDI
    "035720", "051910", "207940", "068270", "323410",  # ì¹´ì¹´ì˜¤, LGí™”í•™, ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤, ì…€íŠ¸ë¦¬ì˜¨, ì¹´ì¹´ì˜¤ë±…í¬
    "005380", "051800", "006800", "035720", "051910",  # í˜„ëŒ€ì°¨, LGì „ì, ë¯¸ë˜ì—ì…‹, ì¹´ì¹´ì˜¤, LGí™”í•™
    "207940", "068270", "323410", "005380", "051800",  # ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤, ì…€íŠ¸ë¦¬ì˜¨, ì¹´ì¹´ì˜¤ë±…í¬, í˜„ëŒ€ì°¨, LGì „ì
    "006800", "035720", "051910", "207940", "068270",  # ë¯¸ë˜ì—ì…‹, ì¹´ì¹´ì˜¤, LGí™”í•™, ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤, ì…€íŠ¸ë¦¬ì˜¨
]

# ê¸€ë¡œë²Œ ì£¼ìš” ì¢…ëª©
GLOBAL_MAJOR_STOCKS = [
    "AAPL", "MSFT", "GOOGL", "AMZN", "TSLA",  # ë¯¸êµ­
    "NVDA", "META", "NFLX", "ADBE", "CRM",    # ë¯¸êµ­
    "005930.KS", "000660.KS", "051900.KS",     # í•œêµ­
    "7203.T", "6758.T", "9984.T",              # ì¼ë³¸
    "0700.HK", "9988.HK", "0941.HK",           # í™ì½©
]

class HistoricalDataMassCollector:
    """ëŒ€ìš©ëŸ‰ ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ê¸°"""

    def __init__(self):
        self.collected_data = {}
        self.stats = {
            'total_symbols': 0,
            'successful_collections': 0,
            'failed_collections': 0,
            'total_data_points': 0,
            'start_time': None,
            'end_time': None
        }

    async def collect_krx_historical_data(self, symbols: List[str]) -> Dict[str, pd.DataFrame]:
        """KRX ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘"""
        logger.info(f"KRX ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {len(symbols)} ì¢…ëª©")

        krx_data = {}
        for symbol in tqdm(symbols, desc="KRX ë°ì´í„° ìˆ˜ì§‘"):
            try:
                # pykrxë¥¼ ì‚¬ìš©í•œ ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘
                df = stock.get_market_ohlcv_by_date(
                    fromdate=START_DATE.replace('-', ''),
                    todate=END_DATE.replace('-', ''),
                    ticker=symbol
                )

                if not df.empty:
                    df = df.reset_index()
                    df.columns = ['date', 'open', 'high', 'low', 'close', 'volume', 'value']
                    df['symbol'] = symbol
                    df['source'] = 'krx'
                    krx_data[symbol] = df
                    self.stats['successful_collections'] += 1
                    self.stats['total_data_points'] += len(df)
                    logger.info(f"KRX {symbol}: {len(df)} ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
                else:
                    logger.warning(f"KRX {symbol}: ë°ì´í„° ì—†ìŒ")
                    self.stats['failed_collections'] += 1

            except Exception as e:
                logger.error(f"KRX {symbol} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                self.stats['failed_collections'] += 1

        return krx_data

    async def collect_yahoo_historical_data(self, symbols: List[str]) -> Dict[str, pd.DataFrame]:
        """Yahoo Finance ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘"""
        logger.info(f"Yahoo Finance ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {len(symbols)} ì¢…ëª©")

        yahoo_data = {}
        for symbol in tqdm(symbols, desc="Yahoo ë°ì´í„° ìˆ˜ì§‘"):
            try:
                ticker = yf.Ticker(symbol)
                df = ticker.history(start=START_DATE, end=END_DATE)

                if not df.empty:
                    df = df.reset_index()
                    df.columns = ['date', 'open', 'high', 'low', 'close', 'volume']
                    df['symbol'] = symbol
                    df['source'] = 'yahoo'
                    yahoo_data[symbol] = df
                    self.stats['successful_collections'] += 1
                    self.stats['total_data_points'] += len(df)
                    logger.info(f"Yahoo {symbol}: {len(df)} ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
                else:
                    logger.warning(f"Yahoo {symbol}: ë°ì´í„° ì—†ìŒ")
                    self.stats['failed_collections'] += 1

            except Exception as e:
                logger.error(f"Yahoo {symbol} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                self.stats['failed_collections'] += 1

        return yahoo_data

    async def collect_news_historical_data(self) -> pd.DataFrame:
        """ë‰´ìŠ¤ ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ (ëª¨ì˜ ë°ì´í„°)"""
        logger.info("ë‰´ìŠ¤ ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")

        # ì‹¤ì œë¡œëŠ” ë‰´ìŠ¤ APIë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, ì—¬ê¸°ì„œëŠ” ëª¨ì˜ ë°ì´í„° ìƒì„±
        date_range = pd.date_range(start=START_DATE, end=END_DATE, freq='D')

        news_data = []
        for date in tqdm(date_range, desc="ë‰´ìŠ¤ ë°ì´í„° ìƒì„±"):
            # ì¼ë³„ ë‰´ìŠ¤ ë°ì´í„° ìƒì„±
            daily_news = [
                {
                    'date': date,
                    'title': f'ì£¼ì‹ ì‹œì¥ ë‰´ìŠ¤ {date.strftime("%Y-%m-%d")}',
                    'content': f'ì£¼ì‹ ì‹œì¥ì˜ ë™í–¥ì„ ë¶„ì„í•œ ë‰´ìŠ¤ ë‚´ìš©ì…ë‹ˆë‹¤.',
                    'sentiment': np.random.choice(['positive', 'negative', 'neutral']),
                    'sentiment_score': np.random.uniform(-1, 1),
                    'source': 'mock_news',
                    'symbols': np.random.choice(KOREAN_MAJOR_STOCKS, size=np.random.randint(1, 5))
                }
                for _ in range(np.random.randint(5, 15))  # ì¼ë³„ 5-15ê°œ ë‰´ìŠ¤
            ]:
            news_data.extend(daily_news):
:
        df = pd.DataFrame(news_data):
        self.stats['total_data_points'] += len(df):
        logger.info(f"ë‰´ìŠ¤ ë°ì´í„° ìƒì„± ì™„ë£Œ: {len(df)} ê°œ")

        return df

    async def collect_all_historical_data(self) -> None:
        """ì „ì²´ ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘"""
        self.stats['start_time'] = datetime.now()
        logger.info("ëŒ€ìš©ëŸ‰ ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")

        # 1. KRX ë°ì´í„° ìˆ˜ì§‘
        krx_data = await self.collect_krx_historical_data(KOREAN_MAJOR_STOCKS)

        # 2. Yahoo Finance ë°ì´í„° ìˆ˜ì§‘
        yahoo_data = await self.collect_yahoo_historical_data(GLOBAL_MAJOR_STOCKS)

        # 3. ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘
        news_data = await self.collect_news_historical_data()

        # 4. ë°ì´í„° ì €ì¥
        await self.save_all_data(krx_data, yahoo_data, news_data)

        self.stats['end_time'] = datetime.now()
        self.stats['total_symbols'] = len(KOREAN_MAJOR_STOCKS) + len(GLOBAL_MAJOR_STOCKS)

        # 5. í†µê³„ ì¶œë ¥
        self.print_collection_stats()

    async def save_all_data(self, krx_data: Dict, yahoo_data: Dict, news_data: pd.DataFrame) -> None:
        """ëª¨ë“  ë°ì´í„° ì €ì¥"""
        logger.info("ë°ì´í„° ì €ì¥ ì‹œì‘")

        # KRX ë°ì´í„° ì €ì¥
        if krx_data:
            all_krx = pd.concat(krx_data.values(), ignore_index=True)
            all_krx.to_parquet("data/historical/krx_historical.parquet")
            all_krx.to_feather("data/historical/krx_historical.feather")
            all_krx.to_csv("data/historical/krx_historical.csv", encoding="utf-8-sig")
            all_krx.to_sql("krx_historical", engine, if_exists="replace")
            logger.info(f"KRX ë°ì´í„° ì €ì¥ ì™„ë£Œ: {len(all_krx)} ê°œ")

        # Yahoo ë°ì´í„° ì €ì¥
        if yahoo_data:
            all_yahoo = pd.concat(yahoo_data.values(), ignore_index=True)
            all_yahoo.to_parquet("data/historical/yahoo_historical.parquet")
            all_yahoo.to_feather("data/historical/yahoo_historical.feather")
            all_yahoo.to_csv("data/historical/yahoo_historical.csv", encoding="utf-8-sig")
            all_yahoo.to_sql("yahoo_historical", engine, if_exists="replace")
            logger.info(f"Yahoo ë°ì´í„° ì €ì¥ ì™„ë£Œ: {len(all_yahoo)} ê°œ")

        # ë‰´ìŠ¤ ë°ì´í„° ì €ì¥
        if not news_data.empty:
            news_data.to_parquet("data/historical/news_historical.parquet")
            news_data.to_feather("data/historical/news_historical.feather")
            news_data.to_csv("data/historical/news_historical.csv", encoding="utf-8-sig")
            news_data.to_sql("news_historical", engine, if_exists="replace")
            logger.info(f"ë‰´ìŠ¤ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {len(news_data)} ê°œ")

    def print_collection_stats(self) -> None:
        """ìˆ˜ì§‘ í†µê³„ ì¶œë ¥"""
        duration = self.stats['end_time'] - self.stats['start_time']

        print("\n" + "="*60)
        print("ğŸ“Š ëŒ€ìš©ëŸ‰ ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
        print("="*60)
        print(f"ğŸ“ˆ ì´ ì¢…ëª© ìˆ˜: {self.stats['total_symbols']}")
        print(f"âœ… ì„±ê³µí•œ ìˆ˜ì§‘: {self.stats['successful_collections']}")
        print(f"âŒ ì‹¤íŒ¨í•œ ìˆ˜ì§‘: {self.stats['failed_collections']}")
        print(f"ğŸ“Š ì´ ë°ì´í„° í¬ì¸íŠ¸: {self.stats['total_data_points']:,}")
        print(f"â±ï¸  ì†Œìš” ì‹œê°„: {duration}")
        print(f"ğŸš€ í‰ê·  ì²˜ë¦¬ ì†ë„: {self.stats['total_data_points']/duration.total_seconds():.0f} í¬ì¸íŠ¸/ì´ˆ")
        print("="*60)

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    collector = HistoricalDataMassCollector()
    await collector.collect_all_historical_data()

if __name__ == "__main__":
    asyncio.run(main())
