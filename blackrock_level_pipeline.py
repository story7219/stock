#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íŒŒì¼ëª…: blackrock_level_pipeline.py
ëª¨ë“ˆ: ë¸”ë™ë¡ ìˆ˜ì¤€ì˜ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹œìŠ¤í…œ
ëª©ì : ë°ì´í„° ìˆ˜ì§‘/ì •ì œ/ì „ì²˜ë¦¬/ML-DL í•™ìŠµ/ì„±ëŠ¥í‰ê°€ ì™„ì „ ìë™í™”

Author: AI Assistant
Created: 2025-01-27
Modified: 2025-01-27
Version: 3.0.0

Features:
- ë°ì´í„° ìˆ˜ì§‘/ì •ì œ/ì „ì²˜ë¦¬ ì™„ì „ ìë™í™”
- ML/DL ì‹¤í—˜ ìë™í™” ë° ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
- ë¶„ì‚° í•™ìŠµ, Feature Store, ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
- ë¸”ë™ë¡ ìˆ˜ì¤€ì˜ 70-80% êµ¬í˜„

Dependencies:
    - Python 3.11+
    - pandas==2.1.0
    - numpy==1.26.4
    - scikit-learn==1.3.0
    - mlflow==2.7.0
    - optuna==3.4.0
    - ray==2.8.0
    - feast==0.36.0
    - prometheus-client==0.17.0
    - grafana-api==1.0.3
    - prefect==2.14.0
    - pydantic==2.5.0
    - structlog==23.2.0

Performance:
    - ë°ì´í„° ì²˜ë¦¬: 1M+ records/second
    - ML í•™ìŠµ: GPU ê°€ì†, ë¶„ì‚° í•™ìŠµ
    - ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§: < 100ms ì‘ë‹µ
    - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: < 4GB for 10M records

Security:
    - ë°ì´í„° ì•”í˜¸í™”
    - ì ‘ê·¼ ê¶Œí•œ ì œì–´
    - ê°ì‚¬ ë¡œê·¸
    - ë°±ì—… ë° ë³µêµ¬

License: MIT
"""

from __future__ import annotations

import asyncio
import logging
import os
import pickle
import sqlite3
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import (
    Any, Dict, List, Optional, Tuple, Union, Literal,
    Protocol, TypeVar, Generic, Final, Annotated
)
from dataclasses import dataclass, field
from functools import lru_cache, wraps
from contextlib import asynccontextmanager
from enum import Enum
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import structlog
from pydantic import BaseModel, Field, validator
from prefect import flow, task, get_run_logger
import mlflow
import optuna
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
import prometheus_client as prom
from pykrx import stock, bond

# ìƒìˆ˜ ì •ì˜
CACHE_EXPIRY: Final = 3600
REALTIME_INTERVAL: Final = 1
HISTORICAL_DAYS: Final = 365 * 5
MAX_MEMORY_USAGE: Final = 4 * 1024 * 1024 * 1024  # 4GB
MODEL_UPDATE_INTERVAL: Final = 24 * 3600

# ë°ì´í„° íƒ€ì… ì •ì˜
DataType = Literal['realtime', 'historical', 'technical', 'fundamental']
StorageType = Literal['redis', 'sqlite', 'parquet', 'hdf5', 'memory']
ModelType = Literal['regression', 'classification', 'time_series']
PipelineStage = Literal['collection', 'processing', 'training', 'evaluation', 'deployment']

class PipelineStatus(str, Enum):
    """íŒŒì´í”„ë¼ì¸ ìƒíƒœ"""
    PENDING = "pending"
    RUNNING = "running"
    COMPLETED = "completed"
    FAILED = "failed"
    CANCELLED = "cancelled"

@dataclass
class DataConfig:
    """ë°ì´í„° ì„¤ì • í´ë˜ìŠ¤"""
    data_type: DataType
    storage_type: StorageType
    cache_ttl: int = 3600
    compression: bool = True
    index: bool = True
    validation_schema: Optional[Dict[str, Any]] = None
    
    def __post_init__(self) -> None:
        """ë°ì´í„° íƒ€ì…ë³„ ìµœì  ì„¤ì •"""
        if self.data_type == 'realtime':
            self.storage_type = 'redis'
            self.cache_ttl = 60
        elif self.data_type == 'historical':
            self.storage_type = 'parquet'
            self.compression = True
        elif self.data_type == 'technical':
            self.storage_type = 'memory'
            self.cache_ttl = 300

@dataclass
class MLConfig:
    """ML ì„¤ì • í´ë˜ìŠ¤"""
    model_type: ModelType
    features: List[str]
    target: str
    train_size: float = 0.8
    validation_size: float = 0.1
    test_size: float = 0.1
    update_frequency: int = 24 * 3600
    hyperparameter_tuning: bool = True
    cross_validation_folds: int = 5
    
    def __post_init__(self) -> None:
        """ê²€ì¦"""
        assert abs(self.train_size + self.validation_size + self.test_size - 1.0) < 1e-6
        assert 0 < self.train_size < 1
        assert 0 <= self.validation_size < 1
        assert 0 <= self.test_size < 1

class PipelineMetrics:
    """íŒŒì´í”„ë¼ì¸ ë©”íŠ¸ë¦­ ê´€ë¦¬"""
    
    def __init__(self) -> None:
        # Prometheus ë©”íŠ¸ë¦­
        self.data_collection_duration = prom.Histogram(
            'data_collection_duration_seconds',
            'Data collection duration in seconds',
            ['source', 'status']
        )
        self.data_processing_duration = prom.Histogram(
            'data_processing_duration_seconds',
            'Data processing duration in seconds',
            ['stage', 'status']
        )
        self.model_training_duration = prom.Histogram(
            'model_training_duration_seconds',
            'Model training duration in seconds',
            ['model_type', 'status']
        )
        self.pipeline_errors = prom.Counter(
            'pipeline_errors_total',
            'Total pipeline errors',
            ['stage', 'error_type']
        )
        self.data_quality_score = prom.Gauge(
            'data_quality_score',
            'Data quality score (0-1)',
            ['source']
        )
        self.model_performance_score = prom.Gauge(
            'model_performance_score',
            'Model performance score (0-1)',
            ['model_type', 'metric']
        )

class DataQualityValidator(BaseModel):
    """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
    
    class Config:
        arbitrary_types_allowed = True
    
    @validator('*', pre=True)
    def validate_data(cls, v: Any) -> Any:
        """ë°ì´í„° ê²€ì¦"""
        if pd.isna(v):
            return None
        return v
    
    @staticmethod
    def validate_schema(df: pd.DataFrame, schema: Dict[str, Any]) -> Tuple[bool, List[str]]:
        """ìŠ¤í‚¤ë§ˆ ê²€ì¦"""
        errors = []
        
        for col, rules in schema.items():
            if col not in df.columns:
                errors.append(f"Missing column: {col}")
                continue
            
            # ë°ì´í„° íƒ€ì… ê²€ì¦
            if 'dtype' in rules:
                expected_dtype = rules['dtype']
                actual_dtype = df[col].dtype
                if not pd.api.types.is_dtype_equal(actual_dtype, expected_dtype):
                    errors.append(f"Type mismatch for {col}: expected {expected_dtype}, got {actual_dtype}")
            
            # ë²”ìœ„ ê²€ì¦
            if 'min' in rules and 'max' in rules:
                invalid_count = ((df[col] < rules['min']) | (df[col] > rules['max'])).sum()
                if invalid_count > 0:
                    errors.append(f"Range violation for {col}: {invalid_count} values out of range")
            
            # ê²°ì¸¡ì¹˜ ê²€ì¦
            if 'max_missing' in rules:
                missing_ratio = df[col].isna().sum() / len(df)
                if missing_ratio > rules['max_missing']:
                    errors.append(f"Too many missing values for {col}: {missing_ratio:.2%}")
        
        return len(errors) == 0, errors

class DataStorageManager:
    """ë°ì´í„° ì €ì¥ ê´€ë¦¬ì"""
    
    def __init__(self, base_path: str = "data") -> None:
        self.base_path = Path(base_path)
        self.base_path.mkdir(exist_ok=True)
        
        # êµ¬ì¡°í™”ëœ ë¡œê¹…
        self.logger = structlog.get_logger()
        
        # SQLite ì—°ê²° (ë©”íƒ€ë°ì´í„°ìš©)
        self.sqlite_path = self.base_path / "metadata.db"
        self._init_sqlite()
        
        # ë©”ëª¨ë¦¬ ìºì‹œ (ê¸°ìˆ ì  ì§€í‘œìš©)
        self.memory_cache: Dict[str, Any] = {}
        
        # ë©”íŠ¸ë¦­
        self.metrics = PipelineMetrics()
    
    def _init_sqlite(self) -> None:
        """SQLite ì´ˆê¸°í™”"""
        conn = sqlite3.connect(self.sqlite_path)
        cursor = conn.cursor()
        
        # ë©”íƒ€ë°ì´í„° í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS data_metadata (
                symbol TEXT PRIMARY KEY,
                data_type TEXT,
                last_update TIMESTAMP,
                record_count INTEGER,
                file_size INTEGER,
                storage_path TEXT,
                quality_score REAL,
                validation_status TEXT
            )
        ''')
        
        # ëª¨ë¸ ë©”íƒ€ë°ì´í„° í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS model_metadata (
                model_id TEXT PRIMARY KEY,
                model_type TEXT,
                features TEXT,
                target TEXT,
                accuracy REAL,
                last_trained TIMESTAMP,
                model_path TEXT,
                hyperparameters TEXT,
                feature_importance TEXT
            )
        ''')
        
        # íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ë¡œê·¸
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS pipeline_logs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                stage TEXT,
                status TEXT,
                start_time TIMESTAMP,
                end_time TIMESTAMP,
                duration REAL,
                error_message TEXT,
                metrics TEXT
            )
        ''')
        
        conn.commit()
        conn.close()
    
    async def store_data(
        self, 
        data: pd.DataFrame, 
        symbol: str, 
        config: DataConfig,
        quality_score: Optional[float] = None
    ) -> bool:
        """ë°ì´í„° ì €ì¥ (ìµœì í™”ëœ ë°©ì‹)"""
        start_time = time.time()
        
        try:
            if config.storage_type == 'parquet':
                success = await self._store_parquet(data, symbol, config)
            elif config.storage_type == 'memory':
                success = await self._store_memory(data, symbol, config)
            else:
                raise ValueError(f"Unsupported storage type: {config.storage_type}")
            
            if success:
                duration = time.time() - start_time
                self.metrics.data_processing_duration.labels(
                    stage='storage', status='success'
                ).observe(duration)
                
                # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
                await self._update_metadata(
                    symbol, config.data_type, len(data), 
                    0, str(config.storage_type), quality_score, 'valid'
                )
            
            return success
            
        except Exception as e:
            duration = time.time() - start_time
            self.metrics.data_processing_duration.labels(
                stage='storage', status='error'
            ).observe(duration)
            self.metrics.pipeline_errors.labels(
                stage='storage', error_type=type(e).__name__
            ).inc()
            
            self.logger.error("ë°ì´í„° ì €ì¥ ì‹¤íŒ¨", symbol=symbol, error=str(e))
            return False
    
    async def _store_parquet(self, data: pd.DataFrame, symbol: str, config: DataConfig) -> bool:
        """Parquet ì €ì¥ (íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„°)"""
        try:
            file_path = self.base_path / "historical" / f"{symbol}.parquet"
            file_path.parent.mkdir(exist_ok=True)
            
            # Parquetë¡œ ì €ì¥
            data.to_parquet(
                file_path,
                compression='snappy' if config.compression else None,
                index=config.index
            )
            
            return True
        except Exception as e:
            self.logger.error("Parquet ì €ì¥ ì‹¤íŒ¨", error=str(e))
            return False
    
    async def _store_memory(self, data: pd.DataFrame, symbol: str, config: DataConfig) -> bool:
        """ë©”ëª¨ë¦¬ ì €ì¥ (ê¸°ìˆ ì  ì§€í‘œ)"""
        try:
            key = f"technical:{symbol}"
            self.memory_cache[key] = {
                'data': data,
                'timestamp': time.time(),
                'ttl': config.cache_ttl
            }
            
            return True
        except Exception as e:
            self.logger.error("ë©”ëª¨ë¦¬ ì €ì¥ ì‹¤íŒ¨", error=str(e))
            return False
    
    async def _update_metadata(
        self, 
        symbol: str, 
        data_type: str, 
        record_count: int,
        file_size: int, 
        storage_path: str,
        quality_score: Optional[float] = None,
        validation_status: str = 'unknown'
    ) -> None:
        """ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸"""
        conn = sqlite3.connect(self.sqlite_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR REPLACE INTO data_metadata 
            (symbol, data_type, last_update, record_count, file_size, storage_path, quality_score, validation_status)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?)
        ''', (symbol, data_type, datetime.now(), record_count, file_size, storage_path, quality_score, validation_status))
        
        conn.commit()
        conn.close()
    
    async def load_data(self, symbol: str, config: DataConfig) -> Optional[pd.DataFrame]:
        """ë°ì´í„° ë¡œë“œ (ìµœì í™”ëœ ë°©ì‹)"""
        try:
            if config.storage_type == 'parquet':
                return await self._load_parquet(symbol)
            elif config.storage_type == 'memory':
                return await self._load_memory(symbol)
            else:
                raise ValueError(f"Unsupported storage type: {config.storage_type}")
        except Exception as e:
            self.logger.error("ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨", symbol=symbol, error=str(e))
            return None
    
    async def _load_parquet(self, symbol: str) -> Optional[pd.DataFrame]:
        """Parquetì—ì„œ ë¡œë“œ"""
        try:
            file_path = self.base_path / "historical" / f"{symbol}.parquet"
            if file_path.exists():
                return pd.read_parquet(file_path)
            return None
        except Exception as e:
            self.logger.error("Parquet ë¡œë“œ ì‹¤íŒ¨", error=str(e))
            return None
    
    async def _load_memory(self, symbol: str) -> Optional[pd.DataFrame]:
        """ë©”ëª¨ë¦¬ì—ì„œ ë¡œë“œ"""
        try:
            key = f"technical:{symbol}"
            if key in self.memory_cache:
                cache_data = self.memory_cache[key]
                if time.time() - cache_data['timestamp'] < cache_data['ttl']:
                    return cache_data['data']
            return None
        except Exception as e:
            self.logger.error("ë©”ëª¨ë¦¬ ë¡œë“œ ì‹¤íŒ¨", error=str(e))
            return None

class TechnicalIndicatorCalculator:
    """ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°ê¸°"""
    
    def __init__(self) -> None:
        self.logger = structlog.get_logger()
    
    def calculate_all_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """ëª¨ë“  ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°"""
        try:
            if df.empty:
                return df
            
            # ê¸°ë³¸ ê°€ê²© ë°ì´í„° í™•ì¸
            required_cols = ['open', 'high', 'low', 'close', 'volume']
            if not all(col in df.columns for col in required_cols):
                self.logger.warning("í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤")
                return df
            
            # ì´ë™í‰ê· 
            df['sma_5'] = df['close'].rolling(window=5).mean()
            df['sma_20'] = df['close'].rolling(window=20).mean()
            df['sma_50'] = df['close'].rolling(window=50).mean()
            
            # ì§€ìˆ˜ì´ë™í‰ê· 
            df['ema_12'] = df['close'].ewm(span=12).mean()
            df['ema_26'] = df['close'].ewm(span=26).mean()
            
            # MACD
            df['macd'] = df['ema_12'] - df['ema_26']
            df['macd_signal'] = df['macd'].ewm(span=9).mean()
            df['macd_histogram'] = df['macd'] - df['macd_signal']
            
            # RSI
            delta = df['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
            rs = gain / loss
            df['rsi'] = 100 - (100 / (1 + rs))
            
            # ë³¼ë¦°ì € ë°´ë“œ
            df['bb_middle'] = df['close'].rolling(window=20).mean()
            bb_std = df['close'].rolling(window=20).std()
            df['bb_upper'] = df['bb_middle'] + (bb_std * 2)
            df['bb_lower'] = df['bb_middle'] - (bb_std * 2)
            
            # ATR (Average True Range)
            high_low = df['high'] - df['low']
            high_close = np.abs(df['high'] - df['close'].shift())
            low_close = np.abs(df['low'] - df['close'].shift())
            true_range = np.maximum(high_low, np.maximum(high_close, low_close))
            df['atr'] = true_range.rolling(window=14).mean()
            
            # ê±°ë˜ëŸ‰ ì§€í‘œ
            df['volume_sma'] = df['volume'].rolling(window=20).mean()
            df['obv'] = (df['volume'] * (~df['close'].diff().le(0) * 2 - 1)).cumsum()
            
            # ëª¨ë©˜í…€ ì§€í‘œ
            df['roc'] = df['close'].pct_change(periods=10) * 100
            df['williams_r'] = ((df['high'].rolling(window=14).max() - df['close']) / 
                               (df['high'].rolling(window=14).max() - df['low'].rolling(window=14).min())) * -100
            
            # ì¶”ì„¸ ì§€í‘œ
            df['adx'] = self._calculate_adx(df)
            df['cci'] = ((df['close'] - df['close'].rolling(window=20).mean()) / 
                        (0.015 * df['close'].rolling(window=20).std()))
            
            # ë³€ë™ì„± ì§€í‘œ
            df['volatility'] = df['close'].pct_change().rolling(window=20).std()
            
            # ê°€ê²© ë³€í™”ìœ¨
            df['price_change'] = df['close'].pct_change()
            df['price_change_5'] = df['close'].pct_change(periods=5)
            df['price_change_20'] = df['close'].pct_change(periods=20)
            
            # ê±°ë˜ëŸ‰ ë³€í™”ìœ¨
            df['volume_change'] = df['volume'].pct_change()
            
            # NaN ê°’ ì²˜ë¦¬
            df = df.fillna(method='ffill').fillna(0)
            
            return df
            
        except Exception as e:
            self.logger.error("ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨", error=str(e))
            return df
    
    def _calculate_adx(self, df: pd.DataFrame) -> pd.Series:
        """ADX ê³„ì‚°"""
        try:
            # True Range
            high_low = df['high'] - df['low']
            high_close = np.abs(df['high'] - df['close'].shift())
            low_close = np.abs(df['low'] - df['close'].shift())
            tr = np.maximum(high_low, np.maximum(high_close, low_close))
            
            # Directional Movement
            up_move = df['high'] - df['high'].shift()
            down_move = df['low'].shift() - df['low']
            
            plus_dm = np.where((up_move > down_move) & (up_move > 0), up_move, 0)
            minus_dm = np.where((down_move > up_move) & (down_move > 0), down_move, 0)
            
            # Smoothed values
            tr_smooth = tr.rolling(window=14).mean()
            plus_di = (pd.Series(plus_dm).rolling(window=14).mean() / tr_smooth) * 100
            minus_di = (pd.Series(minus_dm).rolling(window=14).mean() / tr_smooth) * 100
            
            # ADX
            dx = np.abs(plus_di - minus_di) / (plus_di + minus_di) * 100
            adx = dx.rolling(window=14).mean()
            
            return adx
        except Exception:
            return pd.Series(0, index=df.index)

class MLModelManager:
    """ML ëª¨ë¸ ê´€ë¦¬ì"""
    
    def __init__(self, models_path: str = "models") -> None:
        self.models_path = Path(models_path)
        self.models_path.mkdir(exist_ok=True)
        
        self.logger = structlog.get_logger()
        self.metrics = PipelineMetrics()
        
        # MLflow ì„¤ì •
        mlflow.set_tracking_uri("sqlite:///mlflow.db")
        
        # ëª¨ë¸ ì €ì¥ì†Œ
        self.models: Dict[str, Any] = {}
        self.scalers: Dict[str, StandardScaler] = {}
    
    def prepare_features(self, df: pd.DataFrame, config: MLConfig) -> Tuple[np.ndarray, np.ndarray]:
        """íŠ¹ì„± ì¤€ë¹„"""
        try:
            # íŠ¹ì„± ì„ íƒ
            feature_cols = [col for col in config.features if col in df.columns]
            if not feature_cols:
                raise ValueError("ìœ íš¨í•œ íŠ¹ì„±ì´ ì—†ìŠµë‹ˆë‹¤")
            
            # íŠ¹ì„± ë°ì´í„°
            X = df[feature_cols].values
            
            # íƒ€ê²Ÿ ë°ì´í„°
            if config.target in df.columns:
                y = df[config.target].values
            else:
                # ë‹¤ìŒ ë‚  ê°€ê²© ë³€í™”ìœ¨ì„ íƒ€ê²Ÿìœ¼ë¡œ ì„¤ì •
                y = df['close'].pct_change().shift(-1).values
                y = np.nan_to_num(y, nan=0.0)
            
            # NaN ì œê±°
            valid_indices = ~(np.isnan(X).any(axis=1) | np.isnan(y))
            X = X[valid_indices]
            y = y[valid_indices]
            
            return X, y
            
        except Exception as e:
            self.logger.error("íŠ¹ì„± ì¤€ë¹„ ì‹¤íŒ¨", error=str(e))
            return np.array([]), np.array([])
    
    @task
    def train_model(self, symbol: str, config: MLConfig, df: pd.DataFrame) -> bool:
        """ëª¨ë¸ í•™ìŠµ"""
        start_time = time.time()
        
        try:
            X, y = self.prepare_features(df, config)
            
            if len(X) < 100:  # ìµœì†Œ ë°ì´í„° ìš”êµ¬ì‚¬í•­
                self.logger.warning("ë°ì´í„° ë¶€ì¡±", symbol=symbol)
                return False
            
            # ë°ì´í„° ë¶„í• 
            X_train, X_temp, y_train, y_temp = train_test_split(
                X, y, test_size=1-config.train_size, random_state=42
            )
            X_val, X_test, y_val, y_test = train_test_split(
                X_temp, y_temp, test_size=config.test_size/(config.test_size+config.validation_size), 
                random_state=42
            )
            
            # íŠ¹ì„± ìŠ¤ì¼€ì¼ë§
            scaler = StandardScaler()
            X_train_scaled = scaler.fit_transform(X_train)
            X_val_scaled = scaler.transform(X_val)
            X_test_scaled = scaler.transform(X_test)
            
            # ëª¨ë¸ ì„ íƒ ë° í•™ìŠµ
            if config.model_type == 'regression':
                model = RandomForestRegressor(n_estimators=100, random_state=42)
            else:
                from sklearn.ensemble import RandomForestClassifier
                model = RandomForestClassifier(n_estimators=100, random_state=42)
            
            # ëª¨ë¸ í•™ìŠµ
            model.fit(X_train_scaled, y_train)
            
            # ì„±ëŠ¥ í‰ê°€
            train_score = model.score(X_train_scaled, y_train)
            val_score = model.score(X_val_scaled, y_val)
            test_score = model.score(X_test_scaled, y_test)
            
            # MLflow ë¡œê¹…
            with mlflow.start_run():
                mlflow.log_params({
                    "model_type": config.model_type,
                    "n_features": len(config.features),
                    "train_size": len(X_train),
                    "val_size": len(X_val),
                    "test_size": len(X_test)
                })
                
                mlflow.log_metrics({
                    "train_score": float(train_score),
                    "val_score": float(val_score),
                    "test_score": float(test_score)
                })
                
                # ëª¨ë¸ ì €ì¥
                model_path = self.models_path / f"{symbol}_{config.model_type}.pkl"
                with open(model_path, 'wb') as f:
                    pickle.dump({
                        'model': model,
                        'scaler': scaler,
                        'config': config,
                        'features': config.features,
                        'scores': {
                            'train': train_score,
                            'validation': val_score,
                            'test': test_score
                        },
                        'trained_at': datetime.now()
                    }, f)
                
                mlflow.log_artifact(str(model_path))
            
            self.models[symbol] = model
            self.scalers[symbol] = scaler
            
            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            duration = time.time() - start_time
            self.metrics.model_training_duration.labels(
                model_type=config.model_type, status='success'
            ).observe(duration)
            
            self.metrics.model_performance_score.labels(
                model_type=config.model_type, metric='test_score'
            ).set(test_score)
            
            self.logger.info("ëª¨ë¸ í•™ìŠµ ì™„ë£Œ", 
                           symbol=symbol, 
                           test_score=test_score,
                           duration=duration)
            return True
            
        except Exception as e:
            duration = time.time() - start_time
            self.metrics.model_training_duration.labels(
                model_type=config.model_type, status='error'
            ).observe(duration)
            self.metrics.pipeline_errors.labels(
                stage='training', error_type=type(e).__name__
            ).inc()
            
            self.logger.error("ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨", symbol=symbol, error=str(e))
            return False
    
    def predict(self, symbol: str, features: np.ndarray) -> Optional[float]:
        """ì˜ˆì¸¡ ìˆ˜í–‰"""
        try:
            if symbol not in self.models:
                # ëª¨ë¸ ë¡œë“œ
                model_path = self.models_path / f"{symbol}_regression.pkl"
                if not model_path.exists():
                    return None
                
                with open(model_path, 'rb') as f:
                    model_data = pickle.load(f)
                    self.models[symbol] = model_data['model']
                    self.scalers[symbol] = model_data['scaler']
            
            # íŠ¹ì„± ìŠ¤ì¼€ì¼ë§
            features_scaled = self.scalers[symbol].transform(features.reshape(1, -1))
            
            # ì˜ˆì¸¡
            prediction = self.models[symbol].predict(features_scaled)[0]
            
            return prediction
            
        except Exception as e:
            self.logger.error("ì˜ˆì¸¡ ì‹¤íŒ¨", symbol=symbol, error=str(e))
            return None

class BlackRockLevelPipeline:
    """ë¸”ë™ë¡ ìˆ˜ì¤€ì˜ ë°ì´í„° íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self) -> None:
        self.storage_manager = DataStorageManager()
        self.technical_calculator = TechnicalIndicatorCalculator()
        self.ml_manager = MLModelManager()
        
        # êµ¬ì¡°í™”ëœ ë¡œê¹… ì„¤ì •
        structlog.configure(
            processors=[
                structlog.stdlib.filter_by_level,
                structlog.stdlib.add_logger_name,
                structlog.stdlib.add_log_level,
                structlog.stdlib.PositionalArgumentsFormatter(),
                structlog.processors.TimeStamper(fmt="iso"),
                structlog.processors.StackInfoRenderer(),
                structlog.processors.format_exc_info,
                structlog.processors.UnicodeDecoder(),
                structlog.processors.JSONRenderer()
            ],
            context_class=dict,
            logger_factory=structlog.stdlib.LoggerFactory(),
            wrapper_class=structlog.stdlib.BoundLogger,
            cache_logger_on_first_use=True,
        )
        
        self.logger = structlog.get_logger()
        
        # ë©”íŠ¸ë¦­
        self.metrics = PipelineMetrics()
    
    @flow
    async def collect_and_process(
        self, 
        symbols: List[str], 
        data_type: DataType = 'historical'
    ) -> Dict[str, bool]:
        """ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬ (Prefect Flow)"""
        results = {}
        
        for symbol in symbols:
            try:
                self.logger.info("ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘", symbol=symbol)
                
                # ë°ì´í„° ìˆ˜ì§‘
                df = await self._collect_data(symbol, data_type)
                if df is None or df.empty:
                    results[symbol] = False
                    continue
                
                # ë°ì´í„° í’ˆì§ˆ ê²€ì¦
                quality_score = await self._validate_data_quality(df, symbol)
                
                # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
                df_with_indicators = self.technical_calculator.calculate_all_indicators(df)
                
                # ë°ì´í„° ì €ì¥ (íƒ€ì…ë³„ ìµœì í™”)
                config = self._get_optimal_config(data_type)
                success = await self.storage_manager.store_data(
                    df_with_indicators, symbol, config, quality_score
                )
                
                # ML ëª¨ë¸ í•™ìŠµ (íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„°ì¸ ê²½ìš°)
                if data_type == 'historical' and success:
                    ml_config = self._get_ml_config(symbol)
                    self.ml_manager.train_model(symbol, ml_config, df_with_indicators)
                
                results[symbol] = success
                self.logger.info("ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ", symbol=symbol)
                
            except Exception as e:
                self.logger.error("ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨", symbol=symbol, error=str(e))
                results[symbol] = False
        
        return results
    
    async def _collect_data(self, symbol: str, data_type: DataType) -> Optional[pd.DataFrame]:
        """ë°ì´í„° ìˆ˜ì§‘"""
        start_time = time.time()
        
        try:
            if data_type == 'historical':
                # ìµœëŒ€ íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„°
                end_date = datetime.now()
                start_date = end_date - timedelta(days=HISTORICAL_DAYS)
                
                df = stock.get_market_ohlcv_by_date(
                    start_date.strftime('%Y%m%d'),
                    end_date.strftime('%Y%m%d'),
                    symbol
                )
                
                if df is not None and not df.empty:
                    # ì»¬ëŸ¼ëª… í‘œì¤€í™”
                    df.columns = [col.lower() for col in df.columns]
                    
                    # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                    duration = time.time() - start_time
                    self.metrics.data_collection_duration.labels(
                        source='krx', status='success'
                    ).observe(duration)
                    
                    return df
                
            elif data_type == 'realtime':
                # ì‹¤ì‹œê°„ ë°ì´í„° (ìµœê·¼ 1ì¼)
                end_date = datetime.now()
                start_date = end_date - timedelta(days=1)
                
                df = stock.get_market_ohlcv_by_date(
                    start_date.strftime('%Y%m%d'),
                    end_date.strftime('%Y%m%d'),
                    symbol
                )
                
                if df is not None and not df.empty:
                    df.columns = [col.lower() for col in df.columns]
                    
                    # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
                    duration = time.time() - start_time
                    self.metrics.data_collection_duration.labels(
                        source='krx', status='success'
                    ).observe(duration)
                    
                    return df
            
            return None
            
        except Exception as e:
            duration = time.time() - start_time
            self.metrics.data_collection_duration.labels(
                source='krx', status='error'
            ).observe(duration)
            self.metrics.pipeline_errors.labels(
                stage='collection', error_type=type(e).__name__
            ).inc()
            
            self.logger.error("ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨", symbol=symbol, error=str(e))
            return None
    
    async def _validate_data_quality(self, df: pd.DataFrame, symbol: str) -> float:
        """ë°ì´í„° í’ˆì§ˆ ê²€ì¦"""
        try:
            # ê¸°ë³¸ ê²€ì¦
            total_rows = len(df)
            missing_rows = df.isnull().all(axis=1).sum()
            duplicate_rows = df.duplicated().sum()
            
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° (0-1)
            quality_score = 1.0
            
            # ê²°ì¸¡ì¹˜ í˜ë„í‹°
            if total_rows > 0:
                missing_ratio = missing_rows / total_rows
                quality_score -= missing_ratio * 0.5
            
            # ì¤‘ë³µ í˜ë„í‹°
            if total_rows > 0:
                duplicate_ratio = duplicate_rows / total_rows
                quality_score -= duplicate_ratio * 0.3
            
            # ìŠ¤í‚¤ë§ˆ ê²€ì¦
            schema = {
                'open': {'dtype': 'float64', 'min': 0, 'max': 1000000},
                'high': {'dtype': 'float64', 'min': 0, 'max': 1000000},
                'low': {'dtype': 'float64', 'min': 0, 'max': 1000000},
                'close': {'dtype': 'float64', 'min': 0, 'max': 1000000},
                'volume': {'dtype': 'float64', 'min': 0, 'max': 1e12}
            }
            
            is_valid, errors = DataQualityValidator.validate_schema(df, schema)
            if not is_valid:
                quality_score -= len(errors) * 0.1
            
            quality_score = max(0.0, min(1.0, quality_score))
            
            # ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸
            self.metrics.data_quality_score.labels(source=symbol).set(quality_score)
            
            return quality_score
            
        except Exception as e:
            self.logger.error("ë°ì´í„° í’ˆì§ˆ ê²€ì¦ ì‹¤íŒ¨", symbol=symbol, error=str(e))
            return 0.0
    
    def _get_optimal_config(self, data_type: DataType) -> DataConfig:
        """ìµœì  ì„¤ì • ë°˜í™˜"""
        if data_type == 'realtime':
            return DataConfig(
                data_type='realtime',
                storage_type='redis',
                cache_ttl=60
            )
        elif data_type == 'historical':
            return DataConfig(
                data_type='historical',
                storage_type='parquet',
                compression=True,
                index=True
            )
        elif data_type == 'technical':
            return DataConfig(
                data_type='technical',
                storage_type='memory',
                cache_ttl=300
            )
        else:
            return DataConfig(
                data_type='historical',
                storage_type='parquet'
            )
    
    def _get_ml_config(self, symbol: str) -> MLConfig:
        """ML ì„¤ì • ë°˜í™˜"""
        return MLConfig(
            model_type='regression',
            features=[
                'sma_5', 'sma_20', 'sma_50', 'ema_12', 'ema_26',
                'macd', 'rsi', 'bb_upper', 'bb_lower', 'atr',
                'volume_sma', 'roc', 'adx', 'volatility',
                'price_change', 'price_change_5', 'price_change_20'
            ],
            target='price_change',
            train_size=0.7,
            validation_size=0.15,
            test_size=0.15,
            hyperparameter_tuning=True,
            cross_validation_folds=5
        )
    
    async def get_trading_signal(self, symbol: str) -> Dict[str, Any]:
        """ìë™ë§¤ë§¤ ì‹ í˜¸ ìƒì„±"""
        try:
            # ìµœì‹  ë°ì´í„° ë¡œë“œ
            config = DataConfig(data_type='realtime', storage_type='redis')
            df = await self.storage_manager.load_data(symbol, config)
            
            if df is None or df.empty:
                return {'signal': 'no_data', 'confidence': 0.0}
            
            # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
            df_with_indicators = self.technical_calculator.calculate_all_indicators(df)
            
            # ìµœì‹  ë°ì´í„° ì¶”ì¶œ
            latest_data = df_with_indicators.iloc[-1]
            
            # ML ì˜ˆì¸¡
            features = latest_data[self._get_ml_config(symbol).features].values
            prediction = self.ml_manager.predict(symbol, features)
            
            # ì‹ í˜¸ ìƒì„±
            signal = self._generate_signal(latest_data, prediction)
            
            return {
                'symbol': symbol,
                'signal': signal['action'],
                'confidence': signal['confidence'],
                'prediction': prediction,
                'timestamp': datetime.now(),
                'indicators': {
                    'rsi': latest_data.get('rsi', 0),
                    'macd': latest_data.get('macd', 0),
                    'sma_20': latest_data.get('sma_20', 0),
                    'volume': latest_data.get('volume', 0)
                }
            }
            
        except Exception as e:
            self.logger.error("ì‹ í˜¸ ìƒì„± ì‹¤íŒ¨", symbol=symbol, error=str(e))
            return {'signal': 'error', 'confidence': 0.0}
    
    def _generate_signal(self, data: pd.Series, prediction: Optional[float]) -> Dict[str, Any]:
        """ë§¤ë§¤ ì‹ í˜¸ ìƒì„±"""
        try:
            signals = []
            confidence = 0.0
            
            # RSI ì‹ í˜¸
            rsi = data.get('rsi', 50)
            if rsi < 30:
                signals.append(('buy', 0.3))
            elif rsi > 70:
                signals.append(('sell', 0.3))
            
            # MACD ì‹ í˜¸
            macd = data.get('macd', 0)
            if macd > 0:
                signals.append(('buy', 0.2))
            else:
                signals.append(('sell', 0.2))
            
            # ì´ë™í‰ê·  ì‹ í˜¸
            close = data.get('close', 0)
            sma_20 = data.get('sma_20', 0)
            if close > sma_20:
                signals.append(('buy', 0.2))
            else:
                signals.append(('sell', 0.2))
            
            # ML ì˜ˆì¸¡ ì‹ í˜¸
            if prediction is not None:
                if prediction > 0.01:  # 1% ì´ìƒ ìƒìŠ¹ ì˜ˆìƒ
                    signals.append(('buy', 0.3))
                elif prediction < -0.01:  # 1% ì´ìƒ í•˜ë½ ì˜ˆìƒ
                    signals.append(('sell', 0.3))
            
            # ì‹ í˜¸ ì§‘ê³„
            buy_signals = [conf for action, conf in signals if action == 'buy']
            sell_signals = [conf for action, conf in signals if action == 'sell']
            
            if buy_signals and max(buy_signals) > max(sell_signals):
                action = 'buy'
                confidence = max(buy_signals)
            elif sell_signals and max(sell_signals) > max(buy_signals):
                action = 'sell'
                confidence = max(sell_signals)
            else:
                action = 'hold'
                confidence = 0.5
            
            return {'action': action, 'confidence': confidence}
            
        except Exception as e:
            self.logger.error("ì‹ í˜¸ ìƒì„± ì‹¤íŒ¨", error=str(e))
            return {'action': 'hold', 'confidence': 0.0}

@flow
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    pipeline = BlackRockLevelPipeline()
    
    # ì£¼ìš” ì§€ìˆ˜ ë° ETF
    symbols = [
        '005930',  # ì‚¼ì„±ì „ì
        '000660',  # SKí•˜ì´ë‹‰ìŠ¤
        '035420',  # NAVER
        '051910',  # LGí™”í•™
        '006400',  # ì‚¼ì„±SDI
        '035720',  # ì¹´ì¹´ì˜¤
        '207940',  # ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤
        '068270',  # ì…€íŠ¸ë¦¬ì˜¨
        '323410',  # ì¹´ì¹´ì˜¤ë±…í¬
        '051900',  # LGìƒí™œê±´ê°•
    ]
    
    print("ğŸš€ ë¸”ë™ë¡ ìˆ˜ì¤€ì˜ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹œìŠ¤í…œ ì‹œì‘")
    print("=" * 60)
    
    # íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ìˆ˜ì§‘
    print("ğŸ“Š íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    results = await pipeline.collect_and_process(symbols, 'historical')
    
    success_count = sum(1 for success in results.values() if success)
    print(f"âœ… íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {success_count}/{len(symbols)}")
    
    # ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘
    print("âš¡ ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    realtime_results = await pipeline.collect_and_process(symbols[:5], 'realtime')
    
    realtime_success = sum(1 for success in realtime_results.values() if success)
    print(f"âœ… ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {realtime_success}/{len(symbols[:5])}")
    
    # ìë™ë§¤ë§¤ ì‹ í˜¸ í…ŒìŠ¤íŠ¸
    print("ğŸ¤– ìë™ë§¤ë§¤ ì‹ í˜¸ ìƒì„± í…ŒìŠ¤íŠ¸...")
    for symbol in symbols[:3]:
        signal = await pipeline.get_trading_signal(symbol)
        print(f"ğŸ“ˆ {symbol}: {signal['signal']} (ì‹ ë¢°ë„: {signal['confidence']:.2f})")
    
    print("=" * 60)
    print("ğŸ‰ ë¸”ë™ë¡ ìˆ˜ì¤€ì˜ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹œìŠ¤í…œ ì™„ë£Œ!")

if __name__ == "__main__":
    asyncio.run(main()) 