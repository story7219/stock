#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íŒŒì¼ëª…: github_backup_manager.py
ëª©ì : ì¤‘ë³µ íŒŒì¼ì„ GitHubì— ë°±ì—…í•˜ê³  êµ¬ì¡° ì •ë¦¬ ì‹œ ë³µì›
Author: GitHub Backup Manager
Created: 2025-07-13
Version: 1.0.0

Features:
    - ì¤‘ë³µ íŒŒì¼ ìë™ ê°ì§€
    - GitHub ë°±ì—… ë° ë³µì›
    - íŒŒì¼ ë²„ì „ ê´€ë¦¬
    - ì•ˆì „í•œ êµ¬ì¡° ì •ë¦¬
"""

import os
import json
import hashlib
import shutil
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Set, Optional, Any
import subprocess
import logging

class GitHubBackupManager:
    """GitHub ë°±ì—… ê´€ë¦¬ì"""
    
    def __init__(self, repo_name: str = "auto-trading-backup"):
        self.repo_name = repo_name
        self.backup_dir = Path("backup/github")
        self.backup_dir.mkdir(parents=True, exist_ok=True)
        
        # ë¡œê¹… ì„¤ì •
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('logs/github_backup.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
        
        # ë°±ì—… ë©”íƒ€ë°ì´í„°
        self.backup_metadata_file = self.backup_dir / "backup_metadata.json"
        self.backup_metadata = self._load_metadata()
    
    def _load_metadata(self) -> Dict[str, Any]:
        """ë°±ì—… ë©”íƒ€ë°ì´í„° ë¡œë“œ"""
        if self.backup_metadata_file.exists():
            with open(self.backup_metadata_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {
            "backups": {},
            "duplicates": {},
            "last_backup": None,
            "total_files": 0
        }
    
    def _save_metadata(self) -> None:
        """ë°±ì—… ë©”íƒ€ë°ì´í„° ì €ì¥"""
        with open(self.backup_metadata_file, 'w', encoding='utf-8') as f:
            json.dump(self.backup_metadata, f, indent=2, ensure_ascii=False)
    
    def _calculate_file_hash(self, file_path: Path) -> str:
        """íŒŒì¼ í•´ì‹œ ê³„ì‚°"""
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def _find_duplicate_files(self) -> Dict[str, List[str]]:
        """ì¤‘ë³µ íŒŒì¼ ì°¾ê¸°"""
        file_hashes = {}
        duplicates = {}
        
        # ëª¨ë“  íŒŒì¼ ìŠ¤ìº”
        for file_path in Path(".").rglob("*"):
            if file_path.is_file() and not file_path.name.startswith('.'):
                try:
                    file_hash = self._calculate_file_hash(file_path)
                    
                    if file_hash in file_hashes:
                        if file_hash not in duplicates:
                            duplicates[file_hash] = [file_hashes[file_hash]]
                        duplicates[file_hash].append(str(file_path))
                    else:
                        file_hashes[file_hash] = str(file_path)
                        
                except Exception as e:
                    self.logger.warning(f"íŒŒì¼ í•´ì‹œ ê³„ì‚° ì‹¤íŒ¨: {file_path} - {e}")
        
        return duplicates
    
    def backup_duplicates_to_github(self) -> Dict[str, Any]:
        """ì¤‘ë³µ íŒŒì¼ì„ GitHubì— ë°±ì—…"""
        try:
            self.logger.info("ì¤‘ë³µ íŒŒì¼ ë°±ì—… ì‹œì‘")
            
            # ì¤‘ë³µ íŒŒì¼ ì°¾ê¸°
            duplicates = self._find_duplicate_files()
            
            if not duplicates:
                self.logger.info("ì¤‘ë³µ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                return {"success": True, "backed_up": 0}
            
            # ë°±ì—… ë””ë ‰í† ë¦¬ ìƒì„±
            backup_timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_path = self.backup_dir / f"backup_{backup_timestamp}"
            backup_path.mkdir(parents=True, exist_ok=True)
            
            backed_up_files = []
            
            # ì¤‘ë³µ íŒŒì¼ ë°±ì—…
            for file_hash, file_paths in duplicates.items():
                if len(file_paths) > 1:
                    # ì²« ë²ˆì§¸ íŒŒì¼ì„ ì›ë³¸ìœ¼ë¡œ ì‚¬ìš©
                    original_path = Path(file_paths[0])
                    backup_file_path = backup_path / f"{file_hash}_{original_path.name}"
                    
                    try:
                        shutil.copy2(original_path, backup_file_path)
                        backed_up_files.append({
                            "hash": file_hash,
                            "original": str(original_path),
                            "backup": str(backup_file_path),
                            "duplicates": file_paths[1:]
                        })
                        
                        self.logger.info(f"ë°±ì—… ì™„ë£Œ: {original_path.name}")
                        
                    except Exception as e:
                        self.logger.error(f"ë°±ì—… ì‹¤íŒ¨: {original_path} - {e}")
            
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            self.backup_metadata["backups"][backup_timestamp] = {
                "timestamp": datetime.now().isoformat(),
                "files": backed_up_files,
                "total_files": len(backed_up_files)
            }
            self.backup_metadata["duplicates"] = duplicates
            self.backup_metadata["last_backup"] = datetime.now().isoformat()
            self.backup_metadata["total_files"] += len(backed_up_files)
            
            self._save_metadata()
            
            # GitHubì— í‘¸ì‹œ
            self._push_to_github(backup_timestamp)
            
            self.logger.info(f"ë°±ì—… ì™„ë£Œ: {len(backed_up_files)}ê°œ íŒŒì¼")
            
            return {
                "success": True,
                "backed_up": len(backed_up_files),
                "backup_path": str(backup_path),
                "timestamp": backup_timestamp
            }
            
        except Exception as e:
            self.logger.error(f"ë°±ì—… ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def _push_to_github(self, backup_timestamp: str) -> bool:
        """GitHubì— í‘¸ì‹œ"""
        try:
            # Git ì´ˆê¸°í™” (ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ)
            if not (self.backup_dir / ".git").exists():
                subprocess.run(["git", "init"], cwd=self.backup_dir, check=True)
            
            # ì›ê²© ì €ì¥ì†Œ ì¶”ê°€ (ì´ë¯¸ ìˆìœ¼ë©´ ìŠ¤í‚µ)
            try:
                subprocess.run(["git", "remote", "add", "origin", f"https://github.com/your-username/{self.repo_name}.git"], 
                             cwd=self.backup_dir, check=True)
            except subprocess.CalledProcessError:
                # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” ê²½ìš° ë¬´ì‹œ
                pass
            
            # íŒŒì¼ ì¶”ê°€
            subprocess.run(["git", "add", "."], cwd=self.backup_dir, check=True)
            
            # ì»¤ë°‹
            commit_message = f"Backup duplicate files - {backup_timestamp}"
            subprocess.run(["git", "commit", "-m", commit_message], cwd=self.backup_dir, check=True)
            
            # í‘¸ì‹œ
            subprocess.run(["git", "push", "origin", "main"], cwd=self.backup_dir, check=True)
            
            self.logger.info("GitHub í‘¸ì‹œ ì™„ë£Œ")
            return True
            
        except Exception as e:
            self.logger.error(f"GitHub í‘¸ì‹œ ì‹¤íŒ¨: {e}")
            return False
    
    def restore_from_github(self, backup_timestamp: Optional[str] = None) -> Dict[str, Any]:
        """GitHubì—ì„œ ë³µì›"""
        try:
            self.logger.info("GitHubì—ì„œ ë³µì› ì‹œì‘")
            
            # ìµœì‹  ë°±ì—… ê°€ì ¸ì˜¤ê¸°
            subprocess.run(["git", "pull", "origin", "main"], cwd=self.backup_dir, check=True)
            
            # ë©”íƒ€ë°ì´í„° ë‹¤ì‹œ ë¡œë“œ
            self.backup_metadata = self._load_metadata()
            
            if not backup_timestamp:
                # ê°€ì¥ ìµœê·¼ ë°±ì—… ì‚¬ìš©
                backup_timestamp = self.backup_metadata.get("last_backup", "").split("T")[0].replace("-", "")
            
            if backup_timestamp not in self.backup_metadata["backups"]:
                return {"success": False, "error": f"ë°±ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {backup_timestamp}"}
            
            backup_info = self.backup_metadata["backups"][backup_timestamp]
            restored_files = []
            
            # íŒŒì¼ ë³µì›
            for file_info in backup_info["files"]:
                backup_file_path = Path(file_info["backup"])
                if backup_file_path.exists():
                    # ì›ë³¸ ìœ„ì¹˜ì— ë³µì›
                    original_path = Path(file_info["original"])
                    original_path.parent.mkdir(parents=True, exist_ok=True)
                    
                    shutil.copy2(backup_file_path, original_path)
                    restored_files.append(str(original_path))
                    
                    self.logger.info(f"ë³µì› ì™„ë£Œ: {original_path.name}")
            
            self.logger.info(f"ë³µì› ì™„ë£Œ: {len(restored_files)}ê°œ íŒŒì¼")
            
            return {
                "success": True,
                "restored": len(restored_files),
                "files": restored_files,
                "timestamp": backup_timestamp
            }
            
        except Exception as e:
            self.logger.error(f"ë³µì› ì‹¤íŒ¨: {e}")
            return {"success": False, "error": str(e)}
    
    def list_backups(self) -> List[Dict[str, Any]]:
        """ë°±ì—… ëª©ë¡ ì¡°íšŒ"""
        backups = []
        for timestamp, info in self.backup_metadata["backups"].items():
            backups.append({
                "timestamp": timestamp,
                "date": info["timestamp"],
                "files_count": info["total_files"],
                "files": [f["original"] for f in info["files"]]
            })
        return sorted(backups, key=lambda x: x["timestamp"], reverse=True)
    
    def get_duplicate_report(self) -> Dict[str, Any]:
        """ì¤‘ë³µ íŒŒì¼ ë¦¬í¬íŠ¸"""
        duplicates = self._find_duplicate_files()
        
        report = {
            "total_duplicates": len(duplicates),
            "total_files": sum(len(paths) for paths in duplicates.values()),
            "duplicates": {}
        }
        
        for file_hash, file_paths in duplicates.items():
            if len(file_paths) > 1:
                report["duplicates"][file_hash] = {
                    "count": len(file_paths),
                    "files": file_paths
                }
        
        return report

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    backup_manager = GitHubBackupManager()
    
    print("ğŸ” ì¤‘ë³µ íŒŒì¼ ë¶„ì„ ì¤‘...")
    duplicate_report = backup_manager.get_duplicate_report()
    
    print(f"ğŸ“Š ì¤‘ë³µ íŒŒì¼ ë¦¬í¬íŠ¸:")
    print(f"  - ì¤‘ë³µ ê·¸ë£¹: {duplicate_report['total_duplicates']}ê°œ")
    print(f"  - ì´ íŒŒì¼: {duplicate_report['total_files']}ê°œ")
    
    if duplicate_report['total_duplicates'] > 0:
        print("\nğŸ“¦ GitHub ë°±ì—… ì‹œì‘...")
        result = backup_manager.backup_duplicates_to_github()
        
        if result["success"]:
            print(f"âœ… ë°±ì—… ì™„ë£Œ: {result['backed_up']}ê°œ íŒŒì¼")
            print(f"ğŸ“ ë°±ì—… ìœ„ì¹˜: {result['backup_path']}")
        else:
            print(f"âŒ ë°±ì—… ì‹¤íŒ¨: {result['error']}")
    
    print("\nğŸ“‹ ë°±ì—… ëª©ë¡:")
    backups = backup_manager.list_backups()
    for backup in backups[:5]:  # ìµœê·¼ 5ê°œë§Œ í‘œì‹œ
        print(f"  ğŸ“… {backup['date']}: {backup['files_count']}ê°œ íŒŒì¼")

if __name__ == "__main__":
    main() 