#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íŒŒì¼ëª…: gpu_optimized_training.py
ëª¨ë“ˆ: GPU ìµœì í™” ë”¥ëŸ¬ë‹ í›ˆë ¨ ì‹œìŠ¤í…œ
ëª©ì : GPU ìì›ì„ ì ê·¹ì ìœ¼ë¡œ í™œìš©í•œ ëŒ€ìš©ëŸ‰ ë°ì´í„° ë”¥ëŸ¬ë‹ í›ˆë ¨

Author: AI Assistant
Created: 2025-01-27
Modified: 2025-01-27
Version: 1.0.0

Dependencies:
    - Python 3.11+
    - torch, torchvision, torchaudio
    - cudf, cupy (GPU ê°€ì†)
    - dask-cuda (GPU ë¶„ì‚° ì²˜ë¦¬)
    - ray[tune] (í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹)
    - optuna (ìµœì í™”)
"""

from __future__ import annotations

import json
import logging
import os
import time
import warnings
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Callable, Dict, List, Optional, Tuple, Union

try:
    import matplotlib.pyplot as plt
    import numpy as np
    import pandas as pd
    import seaborn as sns
    MATPLOTLIB_AVAILABLE = True
    NP_AVAILABLE = True
    PD_AVAILABLE = True
    SEABORN_AVAILABLE = True
except ImportError:
    MATPLOTLIB_AVAILABLE = False
    NP_AVAILABLE = False
    PD_AVAILABLE = False
    SEABORN_AVAILABLE = False

try:
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim
    from torch.cuda.amp import autocast, GradScaler
    from torch.utils.data import DataLoader, TensorDataset
    TORCH_AVAILABLE = True
except ImportError:
    TORCH_AVAILABLE = False

try:
    from torch.optim import Lion
    LION_AVAILABLE = True
except ImportError:
    LION_AVAILABLE = False

try:
    import cudf
    import cupy as cp
    CUDF_AVAILABLE = True
    CUPY_AVAILABLE = True
except ImportError:
    CUDF_AVAILABLE = False
    CUPY_AVAILABLE = False

try:
    import dask
    from dask.distributed import Client
    from dask_cuda import LocalCUDACluster
    DASK_AVAILABLE = True
except ImportError:
    DASK_AVAILABLE = False

try:
    import ray
    from ray import tune
    from ray.tune.schedulers import ASHAScheduler
    from ray.tune.search.optuna import OptunaSearch
    RAY_AVAILABLE = True
except ImportError:
    RAY_AVAILABLE = False

try:
    import optuna
    OPTUNA_AVAILABLE = True
except ImportError:
    OPTUNA_AVAILABLE = False

warnings.filterwarnings('ignore')

# GPU ë¼ì´ë¸ŒëŸ¬ë¦¬
if TORCH_AVAILABLE and CUDF_AVAILABLE and CUPY_AVAILABLE:
    GPU_AVAILABLE = True
    print("âœ… GPU ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì„±ê³µ")
else:
    GPU_AVAILABLE = False
    print("âš ï¸ GPU ë¼ì´ë¸ŒëŸ¬ë¦¬ ë¡œë“œ ì‹¤íŒ¨")

# í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
if RAY_AVAILABLE and OPTUNA_AVAILABLE:
    TUNING_AVAILABLE = True
else:
    TUNING_AVAILABLE = False

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('gpu_training.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)


@dataclass
class GPUTrainingConfig:
    """GPU í›ˆë ¨ ì„¤ì •"""
    # GPU ì„¤ì •
    use_gpu: bool = GPU_AVAILABLE
    gpu_memory_fraction: float = 0.9
    mixed_precision: bool = True
    gradient_accumulation_steps: int = 4

    # ëª¨ë¸ ì„¤ì •
    model_type: str = "lstm"  # lstm, transformer, cnn
    hidden_size: int = 256
    num_layers: int = 4
    dropout: float = 0.2

    # í›ˆë ¨ ì„¤ì •
    batch_size: int = 1024
    learning_rate: float = 1e-3
    num_epochs: int = 100
    early_stopping_patience: int = 10

    # ë°ì´í„° ì„¤ì •
    sequence_length: int = 60
    prediction_horizon: int = 5
    train_split: float = 0.8
    validation_split: float = 0.1

    # ìµœì í™” ì„¤ì •
    optimizer: str = "adamw"  # adam, adamw, lion
    scheduler: str = "cosine"  # cosine, step, plateau
    weight_decay: float = 1e-4

    # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
    enable_tuning: bool = TUNING_AVAILABLE
    num_trials: int = 50
    max_concurrent_trials: int = 4


class AdvancedLSTM(nn.Module):
    """ê³ ê¸‰ LSTM ëª¨ë¸"""

    def __init__(self, input_size: int, hidden_size: int, num_layers: int,
                 output_size: int, dropout: float = 0.2) -> None:
        super(AdvancedLSTM, self).__init__()

        self.hidden_size = hidden_size
        self.num_layers = num_layers

        # LSTM ë ˆì´ì–´
        self.lstm = nn.LSTM(
            input_size=input_size,
            hidden_size=hidden_size,
            num_layers=num_layers,
            dropout=dropout if num_layers > 1 else 0,
            batch_first=True,
            bidirectional=True
        )

        # ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜
        self.attention = nn.MultiheadAttention(
            embed_dim=hidden_size * 2,  # bidirectional
            num_heads=8,
            dropout=dropout,
            batch_first=True
        )

        # ì¶œë ¥ ë ˆì´ì–´
        self.fc_layers = nn.Sequential(
            nn.Linear(hidden_size * 2, hidden_size),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, output_size)
        )

        # ë°°ì¹˜ ì •ê·œí™”
        self.batch_norm = nn.BatchNorm1d(hidden_size * 2)

        # ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”
        self._init_weights()

    def _init_weights(self) -> None:
        """ê°€ì¤‘ì¹˜ ì´ˆê¸°í™”"""
        for name, param in self.named_parameters():
            if 'weight' in name:
                if 'lstm' in name:
                    nn.init.orthogonal_(param)
                else:
                    nn.init.xavier_uniform_(param)
            elif 'bias' in name:
                nn.init.constant_(param, 0)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # LSTM ìˆœì „íŒŒ
        lstm_out, (hidden, cell) = self.lstm(x)

        # ì–´í…ì…˜ ì ìš©
        attn_out, _ = self.attention(lstm_out, lstm_out, lstm_out)

        # ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ ì¶œë ¥
        last_output = attn_out[:, -1, :]

        # ë°°ì¹˜ ì •ê·œí™”
        last_output = self.batch_norm(last_output)

        # ì¶œë ¥ ë ˆì´ì–´
        output = self.fc_layers(last_output)

        return output


class TransformerModel(nn.Module):
    """Transformer ëª¨ë¸"""

    def __init__(self, input_size: int, hidden_size: int, num_layers: int,
                 output_size: int, num_heads: int = 8, dropout: float = 0.2) -> None:
        super(TransformerModel, self).__init__()

        self.input_projection = nn.Linear(input_size, hidden_size)
        self.positional_encoding = self._create_positional_encoding(hidden_size, 1000)

        # Transformer ì¸ì½”ë”
        encoder_layer = nn.TransformerEncoderLayer(
            d_model=hidden_size,
            nhead=num_heads,
            dim_feedforward=hidden_size * 4,
            dropout=dropout,
            batch_first=True
        )

        self.transformer = nn.TransformerEncoder(
            encoder_layer,
            num_layers=num_layers
        )

        # ì¶œë ¥ ë ˆì´ì–´
        self.output_projection = nn.Sequential(
            nn.Linear(hidden_size, hidden_size // 2),
            nn.ReLU(),
            nn.Dropout(dropout),
            nn.Linear(hidden_size // 2, output_size)
        )

    def _create_positional_encoding(self, d_model: int, max_len: int) -> torch.Tensor:
        """ìœ„ì¹˜ ì¸ì½”ë”© ìƒì„±"""
        if not NP_AVAILABLE:
            raise ImportError("numpyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2).float() *
                           (-np.log(10000.0) / d_model))

        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)

        return pe.unsqueeze(0)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # ì…ë ¥ íˆ¬ì˜
        x = self.input_projection(x)

        # ìœ„ì¹˜ ì¸ì½”ë”© ì¶”ê°€
        seq_len = x.size(1)
        x = x + self.positional_encoding[:, :seq_len, :].to(x.device)

        # Transformer ì¸ì½”ë”
        transformer_out = self.transformer(x)

        # ë§ˆì§€ë§‰ ì‹œí€€ìŠ¤ ì¶œë ¥
        last_output = transformer_out[:, -1, :]

        # ì¶œë ¥ íˆ¬ì˜
        output = self.output_projection(last_output)

        return output


class GPUOptimizedTrainer:
    """GPU ìµœì í™” í›ˆë ¨ê¸°"""

    def __init__(self, config: GPUTrainingConfig) -> None:
        if not TORCH_AVAILABLE:
            raise ImportError("PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        self.config = config
        self.device = self._setup_device()

        # ëª¨ë¸ ë° ì˜µí‹°ë§ˆì´ì €
        self.model: Optional[nn.Module] = None
        self.optimizer: Optional[optim.Optimizer] = None
        self.scheduler: Optional[Any] = None
        self.scaler: Optional[GradScaler] = GradScaler() if config.mixed_precision else None

        # í›ˆë ¨ ìƒíƒœ
        self.current_epoch = 0
        self.best_val_loss = float('inf')
        self.patience_counter = 0
        self.training_history = {
            'train_loss': [],
            'val_loss': [],
            'train_acc': [],
            'val_acc': []
        }

        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.performance_stats = {
            'training_time': [],
            'gpu_memory_usage': [],
            'throughput': []
        }

        logger.info(f"GPU ìµœì í™” í›ˆë ¨ê¸° ì´ˆê¸°í™” ì™„ë£Œ: {self.device}")

    def _setup_device(self) -> torch.device:
        """ë””ë°”ì´ìŠ¤ ì„¤ì •"""
        if self.config.use_gpu and torch.cuda.is_available():
            device = torch.device('cuda')

            # GPU ë©”ëª¨ë¦¬ ì„¤ì •
            torch.cuda.set_per_process_memory_fraction(self.config.gpu_memory_fraction)

            # GPU ì •ë³´ ì¶œë ¥
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
            logger.info(f"GPU ì‚¬ìš©: {gpu_name} ({gpu_memory:.1f}GB)")

            return device
        else:
            logger.info("CPU ëª¨ë“œë¡œ ì‹¤í–‰")
            return torch.device('cpu')

    def create_model(self, input_size: int, output_size: int) -> nn.Module:
        """ëª¨ë¸ ìƒì„±"""
        if self.config.model_type == "lstm":
            model = AdvancedLSTM(
                input_size=input_size,
                hidden_size=self.config.hidden_size,
                num_layers=self.config.num_layers,
                output_size=output_size,
                dropout=self.config.dropout
            )
        elif self.config.model_type == "transformer":
            model = TransformerModel(
                input_size=input_size,
                hidden_size=self.config.hidden_size,
                num_layers=self.config.num_layers,
                output_size=output_size,
                dropout=self.config.dropout
            )
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {self.config.model_type}")

        model = model.to(self.device)

        # ëª¨ë¸ ì •ë³´ ì¶œë ¥
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

        logger.info(f"ëª¨ë¸ ìƒì„± ì™„ë£Œ:")
        logger.info(f"  íƒ€ì…: {self.config.model_type}")
        logger.info(f"  ì´ íŒŒë¼ë¯¸í„°: {total_params:,}")
        logger.info(f"  í›ˆë ¨ ê°€ëŠ¥ íŒŒë¼ë¯¸í„°: {trainable_params:,}")

        return model

    def setup_optimizer(self, model: nn.Module) -> None:
        """ì˜µí‹°ë§ˆì´ì € ì„¤ì •"""
        # ì˜µí‹°ë§ˆì´ì € ì„ íƒ
        if self.config.optimizer == "adam":
            self.optimizer = optim.Adam(
                model.parameters(),
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay
            )
        elif self.config.optimizer == "adamw":
            self.optimizer = optim.AdamW(
                model.parameters(),
                lr=self.config.learning_rate,
                weight_decay=self.config.weight_decay
            )
        elif self.config.optimizer == "lion":
            # Lion ì˜µí‹°ë§ˆì´ì € (PyTorch 2.0+)
            if LION_AVAILABLE:
                self.optimizer = Lion(
                    model.parameters(),
                    lr=self.config.learning_rate,
                    weight_decay=self.config.weight_decay
                )
            else:
                logger.warning("Lion ì˜µí‹°ë§ˆì´ì €ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. AdamWë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
                self.optimizer = optim.AdamW(
                    model.parameters(),
                    lr=self.config.learning_rate,
                    weight_decay=self.config.weight_decay
                )

        # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
        if self.config.scheduler == "cosine":
            self.scheduler = optim.lr_scheduler.CosineAnnealingLR(
                self.optimizer,
                T_max=self.config.num_epochs
            )
        elif self.config.scheduler == "step":
            self.scheduler = optim.lr_scheduler.StepLR(
                self.optimizer,
                step_size=30,
                gamma=0.1
            )
        elif self.config.scheduler == "plateau":
            self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(
                self.optimizer,
                mode='min',
                factor=0.5,
                patience=5
            )

        logger.info(f"ì˜µí‹°ë§ˆì´ì € ì„¤ì • ì™„ë£Œ: {self.config.optimizer}")

    def prepare_data_gpu(self, data_path: str) -> Tuple[DataLoader, DataLoader, DataLoader]:
        """GPU ìµœì í™” ë°ì´í„° ì¤€ë¹„"""
        if not PD_AVAILABLE:
            raise ImportError("pandasê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        logger.info("GPU ìµœì í™” ë°ì´í„° ì¤€ë¹„ ì‹œì‘")

        # ë°ì´í„° ë¡œë“œ
        if data_path.endswith('.parquet'):
            df = pd.read_parquet(data_path)
        elif data_path.endswith('.csv'):
            df = pd.read_csv(data_path)
        else:
            raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° í˜•ì‹")

        # GPUì—ì„œ ë°ì´í„° ì „ì²˜ë¦¬
        if self.config.use_gpu and GPU_AVAILABLE:
            df = self._preprocess_gpu(df)
        else:
            df = self._preprocess_cpu(df)

        # ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
        X, y = self._create_sequences(df)

        # ë°ì´í„° ë¶„í• 
        train_size = int(len(X) * self.config.train_split)
        val_size = int(len(X) * self.config.validation_split)

        X_train = X[:train_size]
        y_train = y[:train_size]
        X_val = X[train_size:train_size + val_size]
        y_val = y[train_size:train_size + val_size]
        X_test = X[train_size + val_size:]
        y_test = y[train_size + val_size:]

        # GPUë¡œ ë°ì´í„° ì´ë™
        X_train = torch.FloatTensor(X_train).to(self.device)
        y_train = torch.FloatTensor(y_train).to(self.device)
        X_val = torch.FloatTensor(X_val).to(self.device)
        y_val = torch.FloatTensor(y_val).to(self.device)
        X_test = torch.FloatTensor(X_test).to(self.device)
        y_test = torch.FloatTensor(y_test).to(self.device)

        # DataLoader ìƒì„±
        train_dataset = TensorDataset(X_train, y_train)
        val_dataset = TensorDataset(X_val, y_val)
        test_dataset = TensorDataset(X_test, y_test)

        train_loader = DataLoader(
            train_dataset,
            batch_size=self.config.batch_size,
            shuffle=True,
            pin_memory=True,
            num_workers=4
        )

        val_loader = DataLoader(
            val_dataset,
            batch_size=self.config.batch_size,
            shuffle=False,
            pin_memory=True,
            num_workers=4
        )

        test_loader = DataLoader(
            test_dataset,
            batch_size=self.config.batch_size,
            shuffle=False,
            pin_memory=True,
            num_workers=4
        )

        logger.info(f"ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ:")
        logger.info(f"  í›ˆë ¨: {len(X_train):,} ìƒ˜í”Œ")
        logger.info(f"  ê²€ì¦: {len(X_val):,} ìƒ˜í”Œ")
        logger.info(f"  í…ŒìŠ¤íŠ¸: {len(X_test):,} ìƒ˜í”Œ")

        return train_loader, val_loader, test_loader

    def _preprocess_gpu(self, df: pd.DataFrame) -> pd.DataFrame:
        """GPU ê¸°ë°˜ ë°ì´í„° ì „ì²˜ë¦¬"""
        if not CUDF_AVAILABLE:
            raise ImportError("cudfê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # cuDFë¡œ ë³€í™˜
        gdf = cudf.DataFrame.from_pandas(df)

        # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° (GPU)
        gdf = self._calculate_technical_indicators_gpu(gdf)

        # ì •ê·œí™” (GPU)
        numeric_columns = gdf.select_dtypes(include=['float64', 'int64']).columns
        for col in numeric_columns:
            if col != 'target':
                mean_val = gdf[col].mean()
                std_val = gdf[col].std()
                gdf[col] = (gdf[col] - mean_val) / (std_val + 1e-8)

        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        gdf = gdf.fillna(0)

        # pandasë¡œ ë³€í™˜
        return gdf.to_pandas()

    def _preprocess_cpu(self, df: pd.DataFrame) -> pd.DataFrame:
        """CPU ê¸°ë°˜ ë°ì´í„° ì „ì²˜ë¦¬"""
        # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
        df = self._calculate_technical_indicators_cpu(df)

        # ì •ê·œí™”
        numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns
        for col in numeric_columns:
            if col != 'target':
                mean_val = df[col].mean()
                std_val = df[col].std()
                df[col] = (df[col] - mean_val) / (std_val + 1e-8)

        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        df = df.fillna(0)

        return df

    def _calculate_technical_indicators_gpu(self, gdf: 'cudf.DataFrame') -> 'cudf.DataFrame':
        """GPU ê¸°ë°˜ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°"""
        if 'close' in gdf.columns:
            # ì´ë™í‰ê· 
            gdf['ma_5'] = gdf['close'].rolling(window=5).mean()
            gdf['ma_20'] = gdf['close'].rolling(window=20).mean()
            gdf['ma_50'] = gdf['close'].rolling(window=50).mean()

            # RSI
            delta = gdf['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
            rs = gain / loss
            gdf['rsi'] = 100 - (100 / (1 + rs))

            # MACD
            ema_12 = gdf['close'].ewm(span=12).mean()
            ema_26 = gdf['close'].ewm(span=26).mean()
            gdf['macd'] = ema_12 - ema_26
            gdf['macd_signal'] = gdf['macd'].ewm(span=9).mean()

            # ë³¼ë¦°ì € ë°´ë“œ
            gdf['bb_middle'] = gdf['close'].rolling(window=20).mean()
            bb_std = gdf['close'].rolling(window=20).std()
            gdf['bb_upper'] = gdf['bb_middle'] + (bb_std * 2)
            gdf['bb_lower'] = gdf['bb_middle'] - (bb_std * 2)

        return gdf

    def _calculate_technical_indicators_cpu(self, df: pd.DataFrame) -> pd.DataFrame:
        """CPU ê¸°ë°˜ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°"""
        if 'close' in df.columns:
            # ì´ë™í‰ê· 
            df['ma_5'] = df['close'].rolling(window=5).mean()
            df['ma_20'] = df['close'].rolling(window=20).mean()
            df['ma_50'] = df['close'].rolling(window=50).mean()

            # RSI
            delta = df['close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
            rs = gain / loss
            df['rsi'] = 100 - (100 / (1 + rs))

            # MACD
            ema_12 = df['close'].ewm(span=12).mean()
            ema_26 = df['close'].ewm(span=26).mean()
            df['macd'] = ema_12 - ema_26
            df['macd_signal'] = df['macd'].ewm(span=9).mean()

            # ë³¼ë¦°ì € ë°´ë“œ
            df['bb_middle'] = df['close'].rolling(window=20).mean()
            bb_std = df['close'].rolling(window=20).std()
            df['bb_upper'] = df['bb_middle'] + (bb_std * 2)
            df['bb_lower'] = df['bb_middle'] - (bb_std * 2)

        return df

    def _create_sequences(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±"""
        if not NP_AVAILABLE:
            raise ImportError("numpyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        # íŠ¹ì„± ì»¬ëŸ¼ ì„ íƒ
        feature_columns = [col for col in df.columns if col not in ['symbol', 'date', 'timestamp']]

        # ì‹œí€€ìŠ¤ ìƒì„±
        X, y = [], []

        for i in range(self.config.sequence_length, len(df) - self.config.prediction_horizon + 1):
            # ì…ë ¥ ì‹œí€€ìŠ¤
            sequence = df[feature_columns].iloc[i-self.config.sequence_length:i].values
            X.append(sequence)

            # íƒ€ê²Ÿ (ë¯¸ë˜ ê°€ê²©)
            if 'close' in df.columns:
                target = df['close'].iloc[i:i+self.config.prediction_horizon].values
            else:
                target = df[feature_columns[0]].iloc[i:i+self.config.prediction_horizon].values
            y.append(target)

        return np.array(X), np.array(y)

    def train_epoch(self, model: nn.Module, train_loader: DataLoader) -> Tuple[float, float]:
        """í•œ ì—í¬í¬ í›ˆë ¨"""
        if not self.optimizer:
            raise ValueError("ì˜µí‹°ë§ˆì´ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        model.train()
        total_loss = 0.0
        total_samples = 0

        start_time = time.time()

        for batch_idx, (data, target) in enumerate(train_loader):
            # ê·¸ë˜ë””ì–¸íŠ¸ ì´ˆê¸°í™”
            self.optimizer.zero_grad()

            # Mixed Precision í›ˆë ¨
            if self.config.mixed_precision and self.scaler:
                with autocast():
                    output = model(data)
                    loss = F.mse_loss(output, target)

                # ìŠ¤ì¼€ì¼ëœ ì—­ì „íŒŒ
                self.scaler.scale(loss).backward()

                # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì 
                if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
            else:
                output = model(data)
                loss = F.mse_loss(output, target)
                loss.backward()

                # ê·¸ë˜ë””ì–¸íŠ¸ ëˆ„ì 
                if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:
                    self.optimizer.step()

            # í†µê³„ ì—…ë°ì´íŠ¸
            total_loss += loss.item() * data.size(0)
            total_samples += data.size(0)

            # ì§„í–‰ë¥  ì¶œë ¥
            if batch_idx % 100 == 0:
                logger.info(f"  ë°°ì¹˜ {batch_idx}/{len(train_loader)}: Loss={loss.item():.6f}")

        # ë‚¨ì€ ê·¸ë˜ë””ì–¸íŠ¸ ì—…ë°ì´íŠ¸
        if self.config.mixed_precision and self.scaler:
            self.scaler.step(self.optimizer)
            self.scaler.update()
        else:
            self.optimizer.step()

        epoch_time = time.time() - start_time
        avg_loss = total_loss / total_samples

        # ì„±ëŠ¥ í†µê³„ ì—…ë°ì´íŠ¸
        self.performance_stats['training_time'].append(epoch_time)
        if self.config.use_gpu:
            gpu_memory = torch.cuda.memory_allocated() / 1e9
            self.performance_stats['gpu_memory_usage'].append(gpu_memory)

        throughput = total_samples / epoch_time
        self.performance_stats['throughput'].append(throughput)

        return avg_loss, throughput

    def validate(self, model: nn.Module, val_loader: DataLoader) -> Tuple[float, float]:
        """ê²€ì¦"""
        model.eval()
        total_loss = 0.0
        total_samples = 0

        with torch.no_grad():
            for data, target in val_loader:
                output = model(data)
                loss = F.mse_loss(output, target)

                total_loss += loss.item() * data.size(0)
                total_samples += data.size(0)

        avg_loss = total_loss / total_samples

        return avg_loss, 0.0

    def train(self, model: nn.Module, train_loader: DataLoader, val_loader: DataLoader) -> nn.Module:
        """ì „ì²´ í›ˆë ¨ ê³¼ì •"""
        logger.info("ğŸš€ GPU ìµœì í™” í›ˆë ¨ ì‹œì‘")

        self.model = model
        self.setup_optimizer(model)

        best_model_state = None

        for epoch in range(self.config.num_epochs):
            self.current_epoch = epoch

            # í›ˆë ¨
            train_loss, train_throughput = self.train_epoch(model, train_loader)

            # ê²€ì¦
            val_loss, _ = self.validate(model, val_loader)

            # ìŠ¤ì¼€ì¤„ëŸ¬ ì—…ë°ì´íŠ¸
            if self.scheduler:
                if isinstance(self.scheduler, optim.lr_scheduler.ReduceLROnPlateau):
                    self.scheduler.step(val_loss)
                else:
                    self.scheduler.step()

            # íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            self.training_history['train_loss'].append(train_loss)
            self.training_history['val_loss'].append(val_loss)

            # ë¡œê·¸ ì¶œë ¥
            if self.optimizer:
                current_lr = self.optimizer.param_groups[0]['lr']
                logger.info(f"ì—í¬í¬ {epoch+1}/{self.config.num_epochs}:")
                logger.info(f"  í›ˆë ¨ ì†ì‹¤: {train_loss:.6f}")
                logger.info(f"  ê²€ì¦ ì†ì‹¤: {val_loss:.6f}")
                logger.info(f"  í•™ìŠµë¥ : {current_lr:.2e}")
                logger.info(f"  ì²˜ë¦¬ëŸ‰: {train_throughput:.0f} ìƒ˜í”Œ/ì´ˆ")

            if self.config.use_gpu:
                gpu_memory = torch.cuda.memory_allocated() / 1e9
                logger.info(f"  GPU ë©”ëª¨ë¦¬: {gpu_memory:.2f}GB")

            # Early Stopping
            if val_loss < self.best_val_loss:
                self.best_val_loss = val_loss
                self.patience_counter = 0
                best_model_state = model.state_dict().copy()
            else:
                self.patience_counter += 1

                if self.patience_counter >= self.config.early_stopping_patience:
                    logger.info(f"Early stopping triggered after {epoch+1} epochs")
                    break

        # ìµœê³  ëª¨ë¸ ë³µì›
        if best_model_state:
            model.load_state_dict(best_model_state)

        # ì„±ëŠ¥ í†µê³„ ì¶œë ¥
        self._print_performance_stats()

        return model

    def _print_performance_stats(self) -> None:
        """ì„±ëŠ¥ í†µê³„ ì¶œë ¥"""
        if not NP_AVAILABLE:
            logger.warning("numpyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ ì„±ëŠ¥ í†µê³„ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return

        avg_training_time = np.mean(self.performance_stats['training_time'])
        avg_throughput = np.mean(self.performance_stats['throughput'])

        logger.info("ğŸ¯ GPU í›ˆë ¨ ì„±ëŠ¥ í†µê³„:")
        logger.info(f"  í‰ê·  ì—í¬í¬ ì‹œê°„: {avg_training_time:.2f}ì´ˆ")
        logger.info(f"  í‰ê·  ì²˜ë¦¬ëŸ‰: {avg_throughput:.0f} ìƒ˜í”Œ/ì´ˆ")

        if self.config.use_gpu and self.performance_stats['gpu_memory_usage']:
            avg_gpu_memory = np.mean(self.performance_stats['gpu_memory_usage'])
            max_gpu_memory = np.max(self.performance_stats['gpu_memory_usage'])
            logger.info(f"  í‰ê·  GPU ë©”ëª¨ë¦¬: {avg_gpu_memory:.2f}GB")
            logger.info(f"  ìµœëŒ€ GPU ë©”ëª¨ë¦¬: {max_gpu_memory:.2f}GB")

        if self.config.mixed_precision:
            logger.info("  Mixed Precision: í™œì„±í™”")
        else:
            logger.info("  Mixed Precision: ë¹„í™œì„±í™”")

    def save_model(self, model: nn.Module, filepath: str) -> None:
        """ëª¨ë¸ ì €ì¥"""
        if not self.optimizer:
            raise ValueError("ì˜µí‹°ë§ˆì´ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        torch.save({
            'model_state_dict': model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict() if self.scheduler else None,
            'config': self.config,
            'training_history': self.training_history,
            'performance_stats': self.performance_stats,
            'best_val_loss': self.best_val_loss
        }, filepath)

        logger.info(f"ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {filepath}")

    def load_model(self, model: nn.Module, filepath: str) -> None:
        """ëª¨ë¸ ë¡œë“œ"""
        if not self.optimizer:
            raise ValueError("ì˜µí‹°ë§ˆì´ì €ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

        checkpoint = torch.load(filepath, map_location=self.device)

        model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

        if checkpoint['scheduler_state_dict'] and self.scheduler:
            self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

        self.training_history = checkpoint['training_history']
        self.performance_stats = checkpoint['performance_stats']
        self.best_val_loss = checkpoint['best_val_loss']

        logger.info(f"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {filepath}")


def run_hyperparameter_tuning(config: GPUTrainingConfig, data_path: str) -> None:
    """í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í–‰"""
    if not config.enable_tuning:
        logger.warning("í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì´ ë¹„í™œì„±í™”ë˜ì–´ ìˆìŠµë‹ˆë‹¤.")
        return

    if not RAY_AVAILABLE or not OPTUNA_AVAILABLE:
        logger.warning("Ray ë˜ëŠ” Optunaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return

    logger.info("ğŸ” í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹œì‘")

    # Ray ì´ˆê¸°í™”
    ray.init(num_cpus=config.max_concurrent_trials)

    # ê²€ìƒ‰ ê³µê°„ ì •ì˜
    search_space = {
        "learning_rate": tune.loguniform(1e-5, 1e-2),
        "hidden_size": tune.choice([128, 256, 512]),
        "num_layers": tune.choice([2, 3, 4]),
        "dropout": tune.uniform(0.1, 0.5),
        "batch_size": tune.choice([512, 1024, 2048])
    }

    # ìŠ¤ì¼€ì¤„ëŸ¬ ì„¤ì •
    scheduler = ASHAScheduler(
        time_attr="training_iteration",
        metric="val_loss",
        mode="min",
        max_t=config.num_epochs,
        grace_period=10,
        reduction_factor=2
    )

    # ê²€ìƒ‰ ì•Œê³ ë¦¬ì¦˜
    search_alg = OptunaSearch(metric="val_loss", mode="min")

    # íŠœë‹ ì‹¤í–‰
    analysis = tune.run(
        lambda trial_config: train_with_config(trial_config, data_path),
        config=search_space,
        num_samples=config.num_trials,
        scheduler=scheduler,
        search_alg=search_alg,
        resources_per_trial={"cpu": 1, "gpu": 1 if config.use_gpu else 0}
    )

    # ìµœê³  ê²°ê³¼ ì¶œë ¥
    best_trial = analysis.get_best_trial("val_loss", "min")
    logger.info(f"ìµœê³  í•˜ì´í¼íŒŒë¼ë¯¸í„°: {best_trial.config}")
    logger.info(f"ìµœê³  ê²€ì¦ ì†ì‹¤: {best_trial.last_result['val_loss']}")

    ray.shutdown()


def train_with_config(trial_config: Dict[str, Any], data_path: str) -> None:
    """íŠœë‹ìš© í›ˆë ¨ í•¨ìˆ˜"""
    # ì„¤ì • ì—…ë°ì´íŠ¸
    config = GPUTrainingConfig()
    for key, value in trial_config.items():
        setattr(config, key, value)

    # í›ˆë ¨ ì‹¤í–‰
    trainer = GPUOptimizedTrainer(config)
    train_loader, val_loader, _ = trainer.prepare_data_gpu(data_path)

    # ëª¨ë¸ ìƒì„±
    input_size = train_loader.dataset[0][0].shape[-1]
    output_size = train_loader.dataset[0][1].shape[-1]
    model = trainer.create_model(input_size, output_size)

    # í›ˆë ¨
    for epoch in range(config.num_epochs):
        train_loss, _ = trainer.train_epoch(model, train_loader)
        val_loss, _ = trainer.validate(model, val_loader)

        # Ray Tuneì— ê²°ê³¼ ë³´ê³ 
        tune.report(
            train_loss=train_loss,
            val_loss=val_loss,
            training_iteration=epoch + 1
        )


async def main() -> None:
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ GPU ìµœì í™” ë”¥ëŸ¬ë‹ í›ˆë ¨ ì‹œìŠ¤í…œ ì‹œì‘")
    print("=" * 60)

    # ì„¤ì •
    config = GPUTrainingConfig()

    # ë°ì´í„° ê²½ë¡œ (ì‹¤ì œ ë°ì´í„° ê²½ë¡œë¡œ ë³€ê²½ í•„ìš”)
    data_path = "optimized_data_20250127_120000.parquet"

    if not os.path.exists(data_path):
        print(f"âš ï¸ ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_path}")
        print("ë¨¼ì € optimized_data_pipeline.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•˜ì„¸ìš”.")
        return

    try:
        # í›ˆë ¨ê¸° ìƒì„±
        trainer = GPUOptimizedTrainer(config)

        # ë°ì´í„° ì¤€ë¹„
        train_loader, val_loader, test_loader = trainer.prepare_data_gpu(data_path)

        # ëª¨ë¸ ìƒì„±
        input_size = train_loader.dataset[0][0].shape[-1]
        output_size = train_loader.dataset[0][1].shape[-1]
        model = trainer.create_model(input_size, output_size)

        # í›ˆë ¨ ì‹¤í–‰
        trained_model = trainer.train(model, train_loader, val_loader)

        # ëª¨ë¸ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        model_path = f"gpu_optimized_model_{timestamp}.pth"
        trainer.save_model(trained_model, model_path)

        # í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ (ì„ íƒì‚¬í•­)
        if config.enable_tuning:
            run_hyperparameter_tuning(config, data_path)

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        logger.error(f"í›ˆë ¨ ì‹¤íŒ¨: {e}")
    finally:
        print("âœ… GPU ìµœì í™” í›ˆë ¨ ì™„ë£Œ")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())

