#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸš€ Ultra ë°ì´í„° ë§¤ë‹ˆì € v5.0 - ê³ ì„±ëŠ¥ ë¹„ë™ê¸° ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬
- ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ & ë©€í‹°ë ˆë²¨ ìºì‹±
- ì»¤ë„¥ì…˜ í’€ë§ & ë©”ëª¨ë¦¬ ìµœì í™”
- ì‹¤ì‹œê°„ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ & ìë™ ìŠ¤ì¼€ì¼ë§
"""

import asyncio
import aiohttp
import pandas as pd
import numpy as np
from typing import Dict, List, Optional, Any, Tuple, Union, Callable
from datetime import datetime, timedelta
from dataclasses import dataclass, field
from enum import Enum
from concurrent.futures import ThreadPoolExecutor
import json
import structlog
import weakref
import time
import hashlib
from pathlib import Path
import gzip
import lz4.frame

from core.cache_manager import get_cache_manager, cached
from core.database_manager import get_database_manager
from core.performance_monitor import monitor_performance
from core.api_manager import get_api_manager
from config.settings import settings

logger = structlog.get_logger(__name__)


class DataSource(Enum):
    """ë°ì´í„° ì†ŒìŠ¤ ìœ í˜•"""
    YAHOO_FINANCE = "yahoo_finance"
    ALPHA_VANTAGE = "alpha_vantage"
    FINANCIAL_MODELING_PREP = "fmp"
    KRAX = "krax"
    INVESTING_COM = "investing"
    CACHE = "cache"


class MarketType(Enum):
    """ì‹œì¥ ìœ í˜•"""
    KOSPI = "KOSPI"
    KOSDAQ = "KOSDAQ"
    NASDAQ = "NASDAQ"
    NYSE = "NYSE"
    SP500 = "S&P500"


@dataclass
class DataRequest:
    """ë°ì´í„° ìš”ì²­ ì •ì˜"""
    request_id: str
    data_type: str  # stocks, technical, fundamental, news
    params: Dict[str, Any]
    priority: int = 1
    callback: Optional[Callable] = None
    timeout: int = 30
    retry_count: int = 3
    use_cache: bool = True


@dataclass
class DataResponse:
    """ë°ì´í„° ì‘ë‹µ"""
    request_id: str
    data: Any
    source: DataSource
    timestamp: datetime
    cache_hit: bool = False
    processing_time: float = 0.0
    error: Optional[str] = None


@dataclass
class DataStats:
    """ë°ì´í„° ì²˜ë¦¬ í†µê³„"""
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    avg_response_time: float = 0.0
    data_volume_mb: float = 0.0
    
    @property
    def success_rate(self) -> float:
        """ì„±ê³µë¥ """
        return self.successful_requests / self.total_requests if self.total_requests > 0 else 0.0
    
    @property
    def cache_hit_rate(self) -> float:
        """ìºì‹œ íˆíŠ¸ìœ¨"""
        total = self.cache_hits + self.cache_misses
        return self.cache_hits / total if total > 0 else 0.0


class UltraDataManager:
    """ğŸš€ Ultra ë°ì´í„° ë§¤ë‹ˆì € - ê³ ì„±ëŠ¥ ë¹„ë™ê¸° ë°ì´í„° ì²˜ë¦¬"""
    
    def __init__(self):
        # ë¹„ë™ê¸° ì²˜ë¦¬ í
        self._request_queue: asyncio.Queue = asyncio.Queue(maxsize=10000)
        self._batch_queue: asyncio.Queue = asyncio.Queue(maxsize=1000)
        self._priority_queue: asyncio.PriorityQueue = asyncio.PriorityQueue(maxsize=5000)
        
        # HTTP ì„¸ì…˜ í’€
        self._session_pool: List[aiohttp.ClientSession] = []
        self._session_semaphore = asyncio.Semaphore(20)
        
        # ìºì‹œ ë° ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì €
        self._cache_manager = get_cache_manager()
        self._db_manager = None
        self._api_manager = None
        
        # ì„±ëŠ¥ ìµœì í™”
        self._executor = ThreadPoolExecutor(max_workers=16)
        self._workers: List[asyncio.Task] = []
        
        # í†µê³„ ë° ëª¨ë‹ˆí„°ë§
        self._stats = DataStats()
        self._active_requests: weakref.WeakSet = weakref.WeakSet()
        
        # ìƒ˜í”Œ ë°ì´í„° (ê³ ì„±ëŠ¥ ìºì‹±)
        self._sample_data_cache: Dict[str, Any] = {}
        self._last_cache_update = 0
        
        logger.info("Ultra ë°ì´í„° ë§¤ë‹ˆì € ì´ˆê¸°í™”")
    
    async def initialize(self) -> None:
        """ë°ì´í„° ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
        try:
            # ë°ì´í„°ë² ì´ìŠ¤ ë§¤ë‹ˆì € ì´ˆê¸°í™”
            self._db_manager = get_database_manager()
            await self._db_manager.initialize()
            
            # API ë§¤ë‹ˆì € ì´ˆê¸°í™”
            self._api_manager = await get_api_manager()
            
            # HTTP ì„¸ì…˜ í’€ ìƒì„±
            await self._initialize_session_pool()
            
            # ë°±ê·¸ë¼ìš´ë“œ ì›Œì»¤ ì‹œì‘
            await self._start_workers()
            
            # ìƒ˜í”Œ ë°ì´í„° ìºì‹œ ì´ˆê¸°í™”
            await self._initialize_sample_data()
            
            logger.info("Ultra ë°ì´í„° ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë§¤ë‹ˆì € ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def _initialize_session_pool(self) -> None:
        """HTTP ì„¸ì…˜ í’€ ì´ˆê¸°í™”"""
        try:
            # ê³ ì„±ëŠ¥ ì»¤ë„¥í„° ì„¤ì •
            connector = aiohttp.TCPConnector(
                limit=settings.performance.http_pool_connections,
                limit_per_host=settings.performance.http_pool_maxsize,
                ttl_dns_cache=300,
                use_dns_cache=True,
                keepalive_timeout=30,
                enable_cleanup_closed=True
            )
            
            # íƒ€ì„ì•„ì›ƒ ì„¤ì •
            timeout = aiohttp.ClientTimeout(
                total=settings.performance.connection_timeout,
                connect=10,
                sock_read=settings.performance.read_timeout
            )
            
            # ì„¸ì…˜ í’€ ìƒì„±
            for i in range(10):
                session = aiohttp.ClientSession(
                    connector=connector,
                    timeout=timeout,
                    headers={
                        'User-Agent': 'Ultra-HTS-DataManager/5.0',
                        'Accept': 'application/json',
                        'Accept-Encoding': 'gzip, deflate, lz4'
                    }
                )
                self._session_pool.append(session)
            
            logger.info(f"HTTP ì„¸ì…˜ í’€ ì´ˆê¸°í™” ì™„ë£Œ: {len(self._session_pool)}ê°œ ì„¸ì…˜")
            
        except Exception as e:
            logger.error(f"ì„¸ì…˜ í’€ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def _start_workers(self) -> None:
        """ë°±ê·¸ë¼ìš´ë“œ ì›Œì»¤ ì‹œì‘"""
        try:
            # ë°ì´í„° ìš”ì²­ ì›Œì»¤
            for i in range(8):
                worker = asyncio.create_task(
                    self._request_worker(f"request_worker_{i}")
                )
                self._workers.append(worker)
            
            # ë°°ì¹˜ ì²˜ë¦¬ ì›Œì»¤
            for i in range(4):
                worker = asyncio.create_task(
                    self._batch_worker(f"batch_worker_{i}")
                )
                self._workers.append(worker)
            
            # ìš°ì„ ìˆœìœ„ í ì›Œì»¤
            for i in range(2):
                worker = asyncio.create_task(
                    self._priority_worker(f"priority_worker_{i}")
                )
                self._workers.append(worker)
            
            # í†µê³„ ìˆ˜ì§‘ ì›Œì»¤
            stats_worker = asyncio.create_task(self._stats_worker())
            self._workers.append(stats_worker)
            
            # ìºì‹œ ê´€ë¦¬ ì›Œì»¤
            cache_worker = asyncio.create_task(self._cache_worker())
            self._workers.append(cache_worker)
            
            logger.info(f"ë°±ê·¸ë¼ìš´ë“œ ì›Œì»¤ ì‹œì‘: {len(self._workers)}ê°œ")
            
        except Exception as e:
            logger.error(f"ì›Œì»¤ ì‹œì‘ ì‹¤íŒ¨: {e}")
            raise
    
    async def _initialize_sample_data(self) -> None:
        """ìƒ˜í”Œ ë°ì´í„° ì´ˆê¸°í™” ë° ìºì‹±"""
        try:
            # ê³ ì„±ëŠ¥ ìƒ˜í”Œ ë°ì´í„° ìƒì„±
            sample_data = {
                "KOSPI 200": await self._generate_kospi_data(),
                "NASDAQ-100": await self._generate_nasdaq_data(),
                "S&P 500": await self._generate_sp500_data()
            }
            
            # ë©€í‹°ë ˆë²¨ ìºì‹œì— ì €ì¥
            await self._cache_manager.set(
                "sample_stock_data",
                sample_data,
                ttl=3600  # 1ì‹œê°„
            )
            
            self._sample_data_cache = sample_data
            self._last_cache_update = time.time()
            
            logger.info("ìƒ˜í”Œ ë°ì´í„° ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ìƒ˜í”Œ ë°ì´í„° ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    
    async def _generate_kospi_data(self) -> List[Dict[str, Any]]:
        """KOSPI 200 ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
        stocks = [
            {"name": "ì‚¼ì„±ì „ì", "code": "005930", "sector": "ë°˜ë„ì²´", "base_price": 75000},
            {"name": "SKí•˜ì´ë‹‰ìŠ¤", "code": "000660", "sector": "ë°˜ë„ì²´", "base_price": 120000},
            {"name": "NAVER", "code": "035420", "sector": "ì¸í„°ë„·", "base_price": 180000},
            {"name": "ì¹´ì¹´ì˜¤", "code": "035720", "sector": "ì¸í„°ë„·", "base_price": 95000},
            {"name": "LGì—ë„ˆì§€ì†”ë£¨ì…˜", "code": "373220", "sector": "ë°°í„°ë¦¬", "base_price": 450000},
            {"name": "ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤", "code": "207940", "sector": "ë°”ì´ì˜¤", "base_price": 850000},
            {"name": "í˜„ëŒ€ì°¨", "code": "005380", "sector": "ìë™ì°¨", "base_price": 190000},
            {"name": "ê¸°ì•„", "code": "000270", "sector": "ìë™ì°¨", "base_price": 85000},
            {"name": "POSCOí™€ë”©ìŠ¤", "code": "005490", "sector": "ì² ê°•", "base_price": 380000},
            {"name": "LGí™”í•™", "code": "051910", "sector": "í™”í•™", "base_price": 420000},
        ]
        
        # ë³‘ë ¬ë¡œ ì‹¤ì‹œê°„ ë°ì´í„° ìƒì„±
        tasks = [self._generate_realtime_data(stock) for stock in stocks]
        return await asyncio.gather(*tasks)
    
    async def _generate_nasdaq_data(self) -> List[Dict[str, Any]]:
        """NASDAQ-100 ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
        stocks = [
            {"name": "Apple Inc.", "code": "AAPL", "sector": "Technology", "base_price": 175},
            {"name": "Microsoft Corp.", "code": "MSFT", "sector": "Technology", "base_price": 330},
            {"name": "Amazon.com Inc.", "code": "AMZN", "sector": "E-commerce", "base_price": 140},
            {"name": "Tesla Inc.", "code": "TSLA", "sector": "Electric Vehicles", "base_price": 250},
            {"name": "Meta Platforms", "code": "META", "sector": "Social Media", "base_price": 320},
            {"name": "Alphabet Inc.", "code": "GOOGL", "sector": "Technology", "base_price": 140},
            {"name": "Netflix Inc.", "code": "NFLX", "sector": "Streaming", "base_price": 450},
            {"name": "Adobe Inc.", "code": "ADBE", "sector": "Software", "base_price": 580},
            {"name": "Salesforce Inc.", "code": "CRM", "sector": "Cloud", "base_price": 220},
            {"name": "PayPal Holdings", "code": "PYPL", "sector": "Fintech", "base_price": 65},
        ]
        
        tasks = [self._generate_realtime_data(stock) for stock in stocks]
        return await asyncio.gather(*tasks)
    
    async def _generate_sp500_data(self) -> List[Dict[str, Any]]:
        """S&P 500 ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
        stocks = [
            {"name": "Berkshire Hathaway", "code": "BRK.B", "sector": "Conglomerate", "base_price": 350},
            {"name": "JPMorgan Chase", "code": "JPM", "sector": "Banking", "base_price": 145},
            {"name": "Johnson & Johnson", "code": "JNJ", "sector": "Healthcare", "base_price": 160},
            {"name": "Visa Inc.", "code": "V", "sector": "Payment", "base_price": 250},
            {"name": "Procter & Gamble", "code": "PG", "sector": "Consumer Goods", "base_price": 150},
            {"name": "Mastercard Inc.", "code": "MA", "sector": "Payment", "base_price": 380},
            {"name": "UnitedHealth Group", "code": "UNH", "sector": "Healthcare", "base_price": 520},
            {"name": "Home Depot", "code": "HD", "sector": "Retail", "base_price": 320},
            {"name": "Coca-Cola Co.", "code": "KO", "sector": "Beverages", "base_price": 58},
            {"name": "Walt Disney Co.", "code": "DIS", "sector": "Entertainment", "base_price": 95},
        ]
        
        tasks = [self._generate_realtime_data(stock) for stock in stocks]
        return await asyncio.gather(*tasks)
    
    async def _generate_realtime_data(self, base_stock: Dict[str, Any]) -> Dict[str, Any]:
        """ì‹¤ì‹œê°„ ì£¼ì‹ ë°ì´í„° ìƒì„± (ê³ ì„±ëŠ¥ ì‹œë®¬ë ˆì´ì…˜)"""
        try:
            # ì‹œì¥ ì‹œê°„ í™•ì¸
            now = datetime.now()
            is_market_open = self._is_market_open(now)
            
            # ë³€ë™ë¥  ìƒì„± (ì‹œì¥ ê°œì¥ ì‹œê°„ì— ë”°ë¼ ë‹¤ë¥´ê²Œ)
            if is_market_open:
                change_rate = np.random.normal(0, 2.5)  # í‰ê·  0%, í‘œì¤€í¸ì°¨ 2.5%
            else:
                change_rate = np.random.normal(0, 0.5)  # ì‹œì¥ ì™¸ ì‹œê°„ì—ëŠ” ë³€ë™ ì ìŒ
            
            base_price = base_stock["base_price"]
            current_price = base_price * (1 + change_rate / 100)
            
            # ê±°ë˜ëŸ‰ ìƒì„± (ë¡œê·¸ ì •ê·œë¶„í¬ ì‚¬ìš©)
            volume = int(np.random.lognormal(13, 1))  # ë” í˜„ì‹¤ì ì¸ ê±°ë˜ëŸ‰ ë¶„í¬
            
            # ì‹œê°€ì´ì•¡ ê³„ì‚°
            shares_outstanding = np.random.uniform(100000000, 1000000000)
            market_cap = current_price * shares_outstanding / 100000000  # ì–µì› ë‹¨ìœ„
            
            return {
                "name": base_stock["name"],
                "code": base_stock["code"],
                "sector": base_stock["sector"],
                "price": round(current_price, 0 if current_price > 1000 else 2),
                "change_rate": round(change_rate, 2),
                "volume": volume,
                "market_cap": round(market_cap, 0),
                "updated_at": now.isoformat(),
                "data_quality": "high",
                "source": "simulation"
            }
            
        except Exception as e:
            logger.error(f"ì‹¤ì‹œê°„ ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")
            return {
                "name": base_stock.get("name", "Unknown"),
                "code": base_stock.get("code", "000000"),
                "sector": base_stock.get("sector", "Unknown"),
                "price": base_stock.get("base_price", 50000),
                "change_rate": 0.0,
                "volume": 1000000,
                "market_cap": 10000,
                "updated_at": datetime.now().isoformat(),
                "data_quality": "low",
                "source": "fallback"
            }
    
    def _is_market_open(self, dt: datetime) -> bool:
        """ì‹œì¥ ê°œì¥ ì‹œê°„ í™•ì¸ (ìµœì í™”ëœ ë²„ì „)"""
        weekday = dt.weekday()
        if weekday >= 5:  # í† ìš”ì¼, ì¼ìš”ì¼
            return False
        
        hour, minute = dt.hour, dt.minute
        
        # í•œêµ­ ì‹œì¥ ì‹œê°„ (09:00-15:30)
        korean_open = (9 <= hour < 15) or (hour == 15 and minute <= 30)
        
        # ë¯¸êµ­ ì‹œì¥ ì‹œê°„ (23:30-06:00, ë‹¤ìŒë‚ )
        us_open = (hour >= 23 and minute >= 30) or (hour < 6)
        
        return korean_open or us_open
    
    @monitor_performance("get_stocks_by_index")
    @cached(ttl=60, key_prefix="ultra_stocks_by_index")
    async def get_stocks_by_index(self, index_name: str) -> List[Dict[str, Any]]:
        """ì§€ìˆ˜ë³„ ì£¼ì‹ ë°ì´í„° ì¡°íšŒ (Ultra ê³ ì„±ëŠ¥)"""
        try:
            # ìºì‹œì—ì„œ ë¨¼ì € í™•ì¸
            cached_data = await self._cache_manager.get(f"stocks_index_{index_name}")
            if cached_data and time.time() - self._last_cache_update < 300:  # 5ë¶„ ìºì‹œ
                self._stats.cache_hits += 1
                return cached_data
            
            # ìƒ˜í”Œ ë°ì´í„°ì—ì„œ ì¡°íšŒ
            if index_name in self._sample_data_cache:
                base_stocks = self._sample_data_cache[index_name]
            else:
                # ìºì‹œ ë¯¸ìŠ¤ ì‹œ ì¬ìƒì„±
                await self._initialize_sample_data()
                base_stocks = self._sample_data_cache.get(index_name, [])
            
            # ì‹¤ì‹œê°„ ë°ì´í„° ì—…ë°ì´íŠ¸ (ë°°ì¹˜ ì²˜ë¦¬)
            if base_stocks:
                # ë¹„ë™ê¸° ë°°ì¹˜ ì²˜ë¦¬ë¡œ ì„±ëŠ¥ ìµœì í™”
                semaphore = asyncio.Semaphore(10)
                
                async def update_stock_data(stock):
                    async with semaphore:
                        return await self._generate_realtime_data(stock)
                
                tasks = [update_stock_data(stock) for stock in base_stocks]
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                # ì—ëŸ¬ê°€ ì•„ë‹Œ ê²°ê³¼ë§Œ í•„í„°ë§
                valid_results = [
                    result for result in results 
                    if not isinstance(result, Exception)
                ]
                
                # ì„±ëŠ¥ ê¸°ì¤€ ì •ë ¬ (ë“±ë½ë¥  + ê±°ë˜ëŸ‰)
                valid_results.sort(
                    key=lambda x: (x['change_rate'] * 0.7 + 
                                 np.log(x['volume'] / 1000000) * 0.3), 
                    reverse=True
                )
                
                # ìºì‹œì— ì €ì¥
                await self._cache_manager.set(
                    f"stocks_index_{index_name}",
                    valid_results,
                    ttl=300
                )
                
                self._stats.successful_requests += 1
                return valid_results
            
            self._stats.failed_requests += 1
            return []
            
        except Exception as e:
            logger.error(f"ì§€ìˆ˜ë³„ ì£¼ì‹ ì¡°íšŒ ì‹¤íŒ¨ {index_name}: {e}")
            self._stats.failed_requests += 1
            return []
    
    @monitor_performance("get_stock_by_code")
    @cached(ttl=30, key_prefix="ultra_stock_by_code")
    async def get_stock_by_code(self, stock_code: str) -> Optional[Dict[str, Any]]:
        """ì¢…ëª© ì½”ë“œë¡œ ì£¼ì‹ ë°ì´í„° ì¡°íšŒ (Ultra ìµœì í™”)"""
        try:
            # ëª¨ë“  ì§€ìˆ˜ì—ì„œ ë³‘ë ¬ ê²€ìƒ‰
            search_tasks = []
            for index_name in self._sample_data_cache.keys():
                search_tasks.append(
                    self._search_stock_in_index(stock_code, index_name, by_code=True)
                )
            
            results = await asyncio.gather(*search_tasks, return_exceptions=True)
            
            # ì²« ë²ˆì§¸ ìœ íš¨í•œ ê²°ê³¼ ë°˜í™˜
            for result in results:
                if result and not isinstance(result, Exception):
                    self._stats.successful_requests += 1
                    return result
            
            self._stats.failed_requests += 1
            return None
            
        except Exception as e:
            logger.error(f"ì¢…ëª© ì½”ë“œ ì¡°íšŒ ì‹¤íŒ¨ {stock_code}: {e}")
            self._stats.failed_requests += 1
            return None
    
    async def _search_stock_in_index(self, 
                                   search_term: str, 
                                   index_name: str, 
                                   by_code: bool = False) -> Optional[Dict[str, Any]]:
        """ì§€ìˆ˜ ë‚´ì—ì„œ ì¢…ëª© ê²€ìƒ‰"""
        try:
            stocks = self._sample_data_cache.get(index_name, [])
            
            for stock in stocks:
                if by_code:
                    if stock.get("code") == search_term:
                        return await self._generate_realtime_data(stock)
                else:
                    if search_term.lower() in stock.get("name", "").lower():
                        return await self._generate_realtime_data(stock)
            
            return None
            
        except Exception as e:
            logger.error(f"ì§€ìˆ˜ ë‚´ ê²€ìƒ‰ ì‹¤íŒ¨ {search_term} in {index_name}: {e}")
            return None
    
    @monitor_performance("get_market_summary")
    @cached(ttl=300, key_prefix="ultra_market_summary")
    async def get_market_summary(self) -> Dict[str, Any]:
        """ì‹œì¥ ìš”ì•½ ì •ë³´ ì¡°íšŒ (Ultra ê³ ì„±ëŠ¥)"""
        try:
            # ë³‘ë ¬ë¡œ ê° ì§€ìˆ˜ ìš”ì•½ ì •ë³´ ìˆ˜ì§‘
            summary_tasks = [
                self._get_index_summary_ultra(index_name)
                for index_name in self._sample_data_cache.keys()
            ]
            
            results = await asyncio.gather(*summary_tasks, return_exceptions=True)
            
            summary = {}
            for i, index_name in enumerate(self._sample_data_cache.keys()):
                if not isinstance(results[i], Exception):
                    summary[index_name] = results[i]
                else:
                    summary[index_name] = {"error": str(results[i])}
            
            # ì „ì²´ ì‹œì¥ í†µê³„ ì¶”ê°€
            summary["market_overview"] = await self._calculate_market_overview(summary)
            
            self._stats.successful_requests += 1
            return summary
            
        except Exception as e:
            logger.error(f"ì‹œì¥ ìš”ì•½ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            self._stats.failed_requests += 1
            return {}
    
    async def _get_index_summary_ultra(self, index_name: str) -> Dict[str, Any]:
        """ì§€ìˆ˜ ìš”ì•½ ì •ë³´ ìƒì„± (Ultra ìµœì í™”)"""
        try:
            stocks = await self.get_stocks_by_index(index_name)
            
            if not stocks:
                return {"error": "ë°ì´í„° ì—†ìŒ"}
            
            # NumPyë¥¼ ì‚¬ìš©í•œ ê³ ì„±ëŠ¥ í†µê³„ ê³„ì‚°
            prices = np.array([stock["price"] for stock in stocks])
            change_rates = np.array([stock["change_rate"] for stock in stocks])
            volumes = np.array([stock["volume"] for stock in stocks])
            market_caps = np.array([stock.get("market_cap", 0) for stock in stocks])
            
            return {
                "total_stocks": len(stocks),
                "avg_price": round(float(np.mean(prices)), 2),
                "median_price": round(float(np.median(prices)), 2),
                "avg_change_rate": round(float(np.mean(change_rates)), 2),
                "total_volume": int(np.sum(volumes)),
                "avg_volume": int(np.mean(volumes)),
                "total_market_cap": round(float(np.sum(market_caps)), 0),
                "gainers": int(np.sum(change_rates > 0)),
                "losers": int(np.sum(change_rates < 0)),
                "unchanged": int(np.sum(change_rates == 0)),
                "top_gainer": round(float(np.max(change_rates)), 2),
                "top_loser": round(float(np.min(change_rates)), 2),
                "volatility": round(float(np.std(change_rates)), 2),
                "updated_at": datetime.now().isoformat(),
                "data_quality": "ultra_high"
            }
            
        except Exception as e:
            logger.error(f"ì§€ìˆ˜ ìš”ì•½ ìƒì„± ì‹¤íŒ¨ {index_name}: {e}")
            return {"error": str(e)}
    
    async def _calculate_market_overview(self, index_summaries: Dict[str, Any]) -> Dict[str, Any]:
        """ì „ì²´ ì‹œì¥ ê°œìš” ê³„ì‚°"""
        try:
            total_stocks = sum(
                summary.get("total_stocks", 0) 
                for summary in index_summaries.values()
                if isinstance(summary, dict) and "error" not in summary
            )
            
            avg_change_rates = [
                summary.get("avg_change_rate", 0)
                for summary in index_summaries.values()
                if isinstance(summary, dict) and "error" not in summary
            ]
            
            return {
                "total_stocks_tracked": total_stocks,
                "market_sentiment": np.mean(avg_change_rates) if avg_change_rates else 0.0,
                "market_volatility": np.std(avg_change_rates) if len(avg_change_rates) > 1 else 0.0,
                "active_indices": len([
                    s for s in index_summaries.values() 
                    if isinstance(s, dict) and "error" not in s
                ]),
                "last_updated": datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"ì‹œì¥ ê°œìš” ê³„ì‚° ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    # ë°±ê·¸ë¼ìš´ë“œ ì›Œì»¤ë“¤
    async def _request_worker(self, worker_name: str) -> None:
        """ìš”ì²­ ì²˜ë¦¬ ì›Œì»¤"""
        while True:
            try:
                request = await self._request_queue.get()
                await self._process_data_request(request)
                self._request_queue.task_done()
            except Exception as e:
                logger.error(f"{worker_name} ì˜¤ë¥˜: {e}")
                await asyncio.sleep(1)
    
    async def _batch_worker(self, worker_name: str) -> None:
        """ë°°ì¹˜ ì²˜ë¦¬ ì›Œì»¤"""
        while True:
            try:
                batch_requests = await self._batch_queue.get()
                await self._process_batch_requests(batch_requests)
                self._batch_queue.task_done()
            except Exception as e:
                logger.error(f"{worker_name} ì˜¤ë¥˜: {e}")
                await asyncio.sleep(1)
    
    async def _priority_worker(self, worker_name: str) -> None:
        """ìš°ì„ ìˆœìœ„ í ì›Œì»¤"""
        while True:
            try:
                priority, request = await self._priority_queue.get()
                await self._process_data_request(request)
                self._priority_queue.task_done()
            except Exception as e:
                logger.error(f"{worker_name} ì˜¤ë¥˜: {e}")
                await asyncio.sleep(1)
    
    async def _stats_worker(self) -> None:
        """í†µê³„ ìˆ˜ì§‘ ì›Œì»¤"""
        while True:
            try:
                await asyncio.sleep(60)  # 1ë¶„ë§ˆë‹¤
                await self._update_stats()
            except Exception as e:
                logger.error(f"í†µê³„ ì›Œì»¤ ì˜¤ë¥˜: {e}")
    
    async def _cache_worker(self) -> None:
        """ìºì‹œ ê´€ë¦¬ ì›Œì»¤"""
        while True:
            try:
                await asyncio.sleep(300)  # 5ë¶„ë§ˆë‹¤
                await self._refresh_cache()
            except Exception as e:
                logger.error(f"ìºì‹œ ì›Œì»¤ ì˜¤ë¥˜: {e}")
    
    async def _process_data_request(self, request: DataRequest) -> DataResponse:
        """ë°ì´í„° ìš”ì²­ ì²˜ë¦¬"""
        start_time = time.time()
        
        try:
            # ìºì‹œ í™•ì¸
            if request.use_cache:
                cache_key = self._generate_cache_key(request)
                cached_data = await self._cache_manager.get(cache_key)
                if cached_data:
                    self._stats.cache_hits += 1
                    return DataResponse(
                        request_id=request.request_id,
                        data=cached_data,
                        source=DataSource.CACHE,
                        timestamp=datetime.now(),
                        cache_hit=True,
                        processing_time=time.time() - start_time
                    )
            
            # ì‹¤ì œ ë°ì´í„° ì²˜ë¦¬
            data = await self._fetch_data(request)
            
            # ìºì‹œì— ì €ì¥
            if request.use_cache and data:
                cache_key = self._generate_cache_key(request)
                await self._cache_manager.set(cache_key, data, ttl=300)
            
            self._stats.cache_misses += 1
            self._stats.successful_requests += 1
            
            return DataResponse(
                request_id=request.request_id,
                data=data,
                source=DataSource.YAHOO_FINANCE,  # ê¸°ë³¸ê°’
                timestamp=datetime.now(),
                cache_hit=False,
                processing_time=time.time() - start_time
            )
            
        except Exception as e:
            self._stats.failed_requests += 1
            return DataResponse(
                request_id=request.request_id,
                data=None,
                source=DataSource.CACHE,
                timestamp=datetime.now(),
                error=str(e),
                processing_time=time.time() - start_time
            )
    
    async def _fetch_data(self, request: DataRequest) -> Any:
        """ì‹¤ì œ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°"""
        # í˜„ì¬ëŠ” ìƒ˜í”Œ ë°ì´í„° ë°˜í™˜
        # ì‹¤ì œ í™˜ê²½ì—ì„œëŠ” API í˜¸ì¶œ
        data_type = request.data_type
        params = request.params
        
        if data_type == "stocks":
            index_name = params.get("index_name")
            return await self.get_stocks_by_index(index_name)
        elif data_type == "stock":
            stock_code = params.get("stock_code")
            return await self.get_stock_by_code(stock_code)
        elif data_type == "market_summary":
            return await self.get_market_summary()
        else:
            return None
    
    def _generate_cache_key(self, request: DataRequest) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        key_data = f"{request.data_type}:{request.params}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    async def _update_stats(self) -> None:
        """í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            # ì‘ë‹µ ì‹œê°„ ì—…ë°ì´íŠ¸
            if self._stats.successful_requests > 0:
                # ì‹¤ì œ ì‘ë‹µ ì‹œê°„ ê³„ì‚° ë¡œì§
                pass
            
            # ë¡œê·¸ ì¶œë ¥
            logger.info(
                "ë°ì´í„° ë§¤ë‹ˆì € í†µê³„",
                extra={
                    "total_requests": self._stats.total_requests,
                    "success_rate": self._stats.success_rate,
                    "cache_hit_rate": self._stats.cache_hit_rate,
                    "avg_response_time": self._stats.avg_response_time
                }
            )
            
        except Exception as e:
            logger.error(f"í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    async def _refresh_cache(self) -> None:
        """ìºì‹œ ê°±ì‹ """
        try:
            # ìƒ˜í”Œ ë°ì´í„° ê°±ì‹ 
            await self._initialize_sample_data()
            logger.debug("ìºì‹œ ê°±ì‹  ì™„ë£Œ")
        except Exception as e:
            logger.error(f"ìºì‹œ ê°±ì‹  ì‹¤íŒ¨: {e}")
    
    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ì •ë³´ ë°˜í™˜"""
        return {
            "total_requests": self._stats.total_requests,
            "successful_requests": self._stats.successful_requests,
            "failed_requests": self._stats.failed_requests,
            "success_rate": self._stats.success_rate,
            "cache_hits": self._stats.cache_hits,
            "cache_misses": self._stats.cache_misses,
            "cache_hit_rate": self._stats.cache_hit_rate,
            "avg_response_time": self._stats.avg_response_time,
            "data_volume_mb": self._stats.data_volume_mb,
            "active_workers": len(self._workers),
            "queue_sizes": {
                "request_queue": self._request_queue.qsize(),
                "batch_queue": self._batch_queue.qsize(),
                "priority_queue": self._priority_queue.qsize()
            }
        }
    
    async def cleanup(self) -> None:
        """ë°ì´í„° ë§¤ë‹ˆì € ì •ë¦¬"""
        try:
            # ì›Œì»¤ ì¢…ë£Œ
            for worker in self._workers:
                worker.cancel()
            
            # í ì •ë¦¬
            while not self._request_queue.empty():
                self._request_queue.get_nowait()
                self._request_queue.task_done()
            
            # ì„¸ì…˜ í’€ ì •ë¦¬
            for session in self._session_pool:
                await session.close()
            
            # ìŠ¤ë ˆë“œ í’€ ì¢…ë£Œ
            self._executor.shutdown(wait=False)
            
            logger.info("Ultra ë°ì´í„° ë§¤ë‹ˆì € ì •ë¦¬ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ë°ì´í„° ë§¤ë‹ˆì € ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")


# ì „ì—­ ë°ì´í„° ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
_data_manager: Optional[UltraDataManager] = None


def get_data_manager() -> UltraDataManager:
    """ì „ì—­ ë°ì´í„° ë§¤ë‹ˆì € ë°˜í™˜"""
    global _data_manager
    if _data_manager is None:
        _data_manager = UltraDataManager()
    return _data_manager


async def initialize_data_manager() -> None:
    """ë°ì´í„° ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
    data_manager = get_data_manager()
    await data_manager.initialize()


async def cleanup_data_manager() -> None:
    """ë°ì´í„° ë§¤ë‹ˆì € ì •ë¦¬"""
    global _data_manager
    if _data_manager:
        await _data_manager.cleanup()
        _data_manager = None


# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
DataManager = UltraDataManager 