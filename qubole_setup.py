#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íŒŒì¼ëª…: qubole_setup.py
ëª¨ë“ˆ: Qubole í´ë¼ìš°ë“œ í™˜ê²½ ì„¤ì •
ëª©ì : Qubole Data Lake ì´ˆê¸° ì„¤ì • ë° í…Œì´ë¸” ìƒì„±

Author: AI Assistant
Created: 2025-01-27
Modified: 2025-01-27
Version: 1.0.0

Dependencies:
    - Python 3.11+
    - qds-sdk (Qubole Data Service SDK)
    - boto3 (AWS S3)
    - asyncio, logging, os, json
"""

from __future__ import annotations

import asyncio
import json
import logging
import os
from typing import Any, Dict, List, Optional, Tuple

try:
    import boto3
    BOTO3_AVAILABLE = True
except ImportError:
    BOTO3_AVAILABLE = False

try:
    from qds_sdk.commands import HiveCommand
    from qds_sdk.qubole import Qubole
    QUBOLE_AVAILABLE = True
except ImportError:
    QUBOLE_AVAILABLE = False

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class QuboleSetup:
    """Qubole ì„¤ì • í´ë˜ìŠ¤"""

    def __init__(self) -> None:
        # Qubole API ì„¤ì •
        self.api_token: Optional[str] = os.getenv('QUBOLE_API_TOKEN')
        self.api_url: str = os.getenv('QUBOLE_API_URL', 'https://api.qubole.com')

        # AWS S3 ì„¤ì •
        self.s3_bucket: str = os.getenv('S3_BUCKET', 'trading-data-lake')
        self.s3_region: str = os.getenv('S3_REGION', 'us-east-1')
        self.aws_access_key: Optional[str] = os.getenv('AWS_ACCESS_KEY_ID')
        self.aws_secret_key: Optional[str] = os.getenv('AWS_SECRET_ACCESS_KEY')

        # ë°ì´í„° ë ˆì´í¬ ê²½ë¡œ
        self.data_lake_path: str = f"s3://{self.s3_bucket}"
        self.raw_data_path: str = f"s3://{self.s3_bucket}/raw"
        self.processed_data_path: str = f"s3://{self.s3_bucket}/processed"
        self.ml_data_path: str = f"s3://{self.s3_bucket}/ml"

    async def setup_qubole(self) -> None:
        """Qubole ì´ˆê¸° ì„¤ì •"""
        logger.info("Qubole ì´ˆê¸° ì„¤ì • ì‹œì‘")

        try:
            # 1. Qubole í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            await self._initialize_qubole()

            # 2. S3 ë²„í‚· ì„¤ì •
            await self._setup_s3_bucket()

            # 3. Hive í…Œì´ë¸” ìƒì„±
            await self._create_hive_tables()

            # 4. ë°ì´í„° ë ˆì´í¬ êµ¬ì¡° ìƒì„±
            await self._create_data_lake_structure()

            # 5. ì—°ê²° í…ŒìŠ¤íŠ¸
            await self._test_connections()

            logger.info("Qubole ì´ˆê¸° ì„¤ì • ì™„ë£Œ")

        except Exception as e:
            logger.error(f"Qubole ì´ˆê¸° ì„¤ì • ì‹¤íŒ¨: {e}")
            raise

    async def _initialize_qubole(self) -> None:
        """Qubole í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            if not QUBOLE_AVAILABLE:
                raise ImportError("Qubole SDKê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            if not self.api_token:
                raise ValueError("QUBOLE_API_TOKENì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            # Qubole SDK ì´ˆê¸°í™”
            Qubole.configure(api_token=self.api_token)

            logger.info("Qubole í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")

        except Exception as e:
            logger.error(f"Qubole í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    async def _setup_s3_bucket(self) -> None:
        """S3 ë²„í‚· ì„¤ì •"""
        try:
            if not BOTO3_AVAILABLE:
                raise ImportError("boto3ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            if not self.aws_access_key or not self.aws_secret_key:
                raise ValueError("AWS ì¸ì¦ ì •ë³´ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            # S3 í´ë¼ì´ì–¸íŠ¸ ìƒì„±
            s3_client = boto3.client(
                's3',
                aws_access_key_id=self.aws_access_key,
                aws_secret_access_key=self.aws_secret_key,
                region_name=self.s3_region
            )

            # ë²„í‚· ì¡´ì¬ í™•ì¸ ë° ìƒì„±
            try:
                s3_client.head_bucket(Bucket=self.s3_bucket)
                logger.info(f"S3 ë²„í‚· '{self.s3_bucket}' ì´ë¯¸ ì¡´ì¬")
            except:
                # ë²„í‚· ìƒì„±
                s3_client.create_bucket(
                    Bucket=self.s3_bucket,
                    CreateBucketConfiguration={'LocationConstraint': self.s3_region}
                )
                logger.info(f"S3 ë²„í‚· '{self.s3_bucket}' ìƒì„± ì™„ë£Œ")

            # ë²„í‚· ì •ì±… ì„¤ì • (ì„ íƒì‚¬í•­)
            bucket_policy = {
                "Version": "2012-10-17",
                "Statement": [
                    {
                        "Sid": "QuboleAccess",
                        "Effect": "Allow",
                        "Principal": {"AWS": "*"},
                        "Action": [
                            "s3:GetObject",
                            "s3:PutObject",
                            "s3:DeleteObject",
                            "s3:ListBucket"
                        ],
                        "Resource": [
                            f"arn:aws:s3:::{self.s3_bucket}",
                            f"arn:aws:s3:::{self.s3_bucket}/*"
                        ]
                    }
                ]
            }

            s3_client.put_bucket_policy(
                Bucket=self.s3_bucket,
                Policy=json.dumps(bucket_policy)
            )

            logger.info("S3 ë²„í‚· ì„¤ì • ì™„ë£Œ")

        except Exception as e:
            logger.error(f"S3 ë²„í‚· ì„¤ì • ì‹¤íŒ¨: {e}")
            raise

    async def _create_hive_tables(self) -> None:
        """Hive í…Œì´ë¸” ìƒì„±"""
        try:
            if not QUBOLE_AVAILABLE:
                raise ImportError("Qubole SDKê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            # ì›ì‹œ ë°ì´í„° í…Œì´ë¸”
            raw_table_query = f"""
            CREATE EXTERNAL TABLE IF NOT EXISTS raw_stock_data (
                symbol STRING,
                timestamp TIMESTAMP,
                current_price DOUBLE,
                ohlcv MAP<STRING, STRING>,
                orderbook MAP<STRING, STRING>,
                category STRING,
                data_type STRING,
                collection_id STRING
            )
            PARTITIONED BY (date STRING)
            STORED AS PARQUET
            LOCATION '{self.raw_data_path}'
            TBLPROPERTIES ('parquet.compression'='SNAPPY')
            """

            # ì²˜ë¦¬ëœ OHLCV í…Œì´ë¸”
            ohlcv_table_query = f"""
            CREATE EXTERNAL TABLE IF NOT EXISTS processed_ohlcv (
                symbol STRING,
                date STRING,
                low DOUBLE,
                high DOUBLE,
                open DOUBLE,
                close DOUBLE,
                volume BIGINT,
                data_points INT
            )
            PARTITIONED BY (date STRING)
            STORED AS PARQUET
            LOCATION '{self.processed_data_path}/ohlcv'
            TBLPROPERTIES ('parquet.compression'='SNAPPY')
            """

            # ê¸°ìˆ ì  ì§€í‘œ í…Œì´ë¸”
            technical_table_query = f"""
            CREATE EXTERNAL TABLE IF NOT EXISTS technical_indicators (
                symbol STRING,
                date STRING,
                ma_20 DOUBLE,
                ma_50 DOUBLE,
                rsi DOUBLE,
                macd DOUBLE,
                bollinger_upper DOUBLE,
                bollinger_lower DOUBLE,
                volume_ma DOUBLE
            )
            PARTITIONED BY (date STRING)
            STORED AS PARQUET
            LOCATION '{self.processed_data_path}/technical_indicators'
            TBLPROPERTIES ('parquet.compression'='SNAPPY')
            """

            # ML íŠ¹ì„± í…Œì´ë¸”
            features_table_query = f"""
            CREATE EXTERNAL TABLE IF NOT EXISTS ml_features (
                symbol STRING,
                date STRING,
                features ARRAY<DOUBLE>,
                scaled_features ARRAY<DOUBLE>,
                price_change DOUBLE,
                price_change_pct DOUBLE,
                volume_ma_ratio DOUBLE,
                volatility DOUBLE
            )
            PARTITIONED BY (date STRING)
            STORED AS PARQUET
            LOCATION '{self.ml_data_path}/features'
            TBLPROPERTIES ('parquet.compression'='SNAPPY')
            """

            # ì˜ˆì¸¡ ê²°ê³¼ í…Œì´ë¸”
            predictions_table_query = f"""
            CREATE EXTERNAL TABLE IF NOT EXISTS predictions (
                symbol STRING,
                date STRING,
                predicted_price DOUBLE,
                predicted_direction INT,
                confidence DOUBLE,
                model_version STRING
            )
            PARTITIONED BY (date STRING)
            STORED AS PARQUET
            LOCATION '{self.ml_data_path}/predictions'
            TBLPROPERTIES ('parquet.compression'='SNAPPY')
            """

            # í…Œì´ë¸” ìƒì„± ì‹¤í–‰
            tables = [
                ("raw_stock_data", raw_table_query),
                ("processed_ohlcv", ohlcv_table_query),
                ("technical_indicators", technical_table_query),
                ("ml_features", features_table_query),
                ("predictions", predictions_table_query)
            ]

            for table_name, query in tables:
                try:
                    hive_cmd = HiveCommand.run(query=query)
                    logger.info(f"Hive í…Œì´ë¸” '{table_name}' ìƒì„± ì™„ë£Œ: {hive_cmd.id}")
                except Exception as e:
                    logger.error(f"Hive í…Œì´ë¸” '{table_name}' ìƒì„± ì‹¤íŒ¨: {e}")

            logger.info("Hive í…Œì´ë¸” ìƒì„± ì™„ë£Œ")

        except Exception as e:
            logger.error(f"Hive í…Œì´ë¸” ìƒì„± ì‹¤íŒ¨: {e}")
            raise

    async def _create_data_lake_structure(self) -> None:
        """ë°ì´í„° ë ˆì´í¬ êµ¬ì¡° ìƒì„±"""
        try:
            if not BOTO3_AVAILABLE:
                raise ImportError("boto3ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            # S3 ê²½ë¡œ êµ¬ì¡° ìƒì„±
            paths = [
                f"{self.raw_data_path}/",
                f"{self.processed_data_path}/",
                f"{self.processed_data_path}/ohlcv/",
                f"{self.processed_data_path}/technical_indicators/",
                f"{self.ml_data_path}/",
                f"{self.ml_data_path}/features/",
                f"{self.ml_data_path}/models/",
                f"{self.ml_data_path}/predictions/",
                f"{self.ml_data_path}/predictions/price/",
                f"{self.ml_data_path}/predictions/direction/",
                f"s3://{self.s3_bucket}/checkpoints/",
                f"s3://{self.s3_bucket}/logs/",
                f"s3://{self.s3_bucket}/backup/",
            ]

            s3_client = boto3.client(
                's3',
                aws_access_key_id=self.aws_access_key,
                aws_secret_access_key=self.aws_secret_key,
                region_name=self.s3_region
            )

            for path in paths:
                # S3 ê²½ë¡œëŠ” ì‹¤ì œë¡œëŠ” íŒŒì¼ì„ ì—…ë¡œë“œí•  ë•Œ ìë™ìœ¼ë¡œ ìƒì„±ë¨
                # ì—¬ê¸°ì„œëŠ” ë”ë¯¸ íŒŒì¼ì„ ì—…ë¡œë“œí•˜ì—¬ ê²½ë¡œ ìƒì„±
                bucket = path.replace(f"s3://{self.s3_bucket}/", "").rstrip("/")
                key = f"{bucket}/.keep"

                try:
                    s3_client.put_object(
                        Bucket=self.s3_bucket,
                        Key=key,
                        Body=""
                    )
                    logger.info(f"ë°ì´í„° ë ˆì´í¬ ê²½ë¡œ ìƒì„±: {path}")
                except Exception as e:
                    logger.warning(f"ê²½ë¡œ ìƒì„± ì‹¤íŒ¨ (ë¬´ì‹œë¨): {path} - {e}")

            logger.info("ë°ì´í„° ë ˆì´í¬ êµ¬ì¡° ìƒì„± ì™„ë£Œ")

        except Exception as e:
            logger.error(f"ë°ì´í„° ë ˆì´í¬ êµ¬ì¡° ìƒì„± ì‹¤íŒ¨: {e}")
            raise

    async def _test_connections(self) -> bool:
        """ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            if not QUBOLE_AVAILABLE:
                raise ImportError("Qubole SDKê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            # Qubole ì—°ê²° í…ŒìŠ¤íŠ¸
            test_query = "SELECT 1 as test"
            hive_cmd = HiveCommand.run(query=test_query)
            logger.info(f"Qubole ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ: {hive_cmd.id}")

            # S3 ì—°ê²° í…ŒìŠ¤íŠ¸
            if BOTO3_AVAILABLE and self.aws_access_key and self.aws_secret_key:
                s3_client = boto3.client(
                    's3',
                    aws_access_key_id=self.aws_access_key,
                    aws_secret_access_key=self.aws_secret_key,
                    region_name=self.s3_region
                )

                response = s3_client.list_objects_v2(
                    Bucket=self.s3_bucket,
                    MaxKeys=1
                )
                logger.info("S3 ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ")

            logger.info("ëª¨ë“  ì—°ê²° í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            return True

        except Exception as e:
            logger.error(f"ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return False

    async def create_sample_data(self) -> None:
        """ìƒ˜í”Œ ë°ì´í„° ìƒì„±"""
        try:
            if not QUBOLE_AVAILABLE:
                raise ImportError("Qubole SDKê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            # ìƒ˜í”Œ ë°ì´í„° ì‚½ì… ì¿¼ë¦¬
            sample_data_query = f"""
            INSERT INTO raw_stock_data PARTITION (date='2025-01-27')
            VALUES
                ('005930', '2025-01-27 09:00:00', 75000.0,
                 map('open', '74800', 'high', '75500', 'low', '74500', 'close', '75000', 'volume', '1000000'),
                 map('bid1', '74900', 'ask1', '75100', 'bid_vol1', '100', 'ask_vol1', '150'),
                 'kospi', 'realtime', '20250127_090000_005930'),
                ('000660', '2025-01-27 09:00:00', 150000.0,
                 map('open', '149000', 'high', '152000', 'low', '148000', 'close', '150000', 'volume', '500000'),
                 map('bid1', '149500', 'ask1', '150500', 'bid_vol1', '200', 'ask_vol1', '180'),
                 'kospi', 'realtime', '20250127_090000_000660')
            """

            hive_cmd = HiveCommand.run(query=sample_data_query)
            logger.info(f"ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì™„ë£Œ: {hive_cmd.id}")

        except Exception as e:
            logger.error(f"ìƒ˜í”Œ ë°ì´í„° ìƒì„± ì‹¤íŒ¨: {e}")

    async def show_tables(self) -> None:
        """í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ"""
        try:
            if not QUBOLE_AVAILABLE:
                raise ImportError("Qubole SDKê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            show_tables_query = "SHOW TABLES"
            hive_cmd = HiveCommand.run(query=show_tables_query)
            logger.info(f"í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ ì™„ë£Œ: {hive_cmd.id}")

        except Exception as e:
            logger.error(f"í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")


async def main() -> None:
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ”§ Qubole í´ë¼ìš°ë“œ í™˜ê²½ ì„¤ì • ì‹œì‘")
    print("=" * 60)

    setup = QuboleSetup()

    try:
        # 1. Qubole ì´ˆê¸° ì„¤ì •
        await setup.setup_qubole()

        # 2. ìƒ˜í”Œ ë°ì´í„° ìƒì„±
        await setup.create_sample_data()

        # 3. í…Œì´ë¸” ëª©ë¡ ì¡°íšŒ
        await setup.show_tables()

        print("âœ… Qubole í´ë¼ìš°ë“œ í™˜ê²½ ì„¤ì • ì™„ë£Œ")

    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")


if __name__ == "__main__":
    asyncio.run(main())

