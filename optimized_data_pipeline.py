#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íŒŒì¼ëª…: optimized_data_pipeline.py
ëª¨ë“ˆ: ìµœì í™”ëœ ë°ì´í„° íŒŒì´í”„ë¼ì¸
ëª©ì : ë³‘ë ¬ ì²˜ë¦¬, ë©€í‹°ìŠ¤ë ˆë”©, GPU í™œìš©ì„ í†µí•œ ê³ ì„±ëŠ¥ ë°ì´í„° ìˆ˜ì§‘ ë° ì „ì²˜ë¦¬

Author: AI Assistant
Created: 2025-01-27
Modified: 2025-01-27
Version: 1.0.0

Dependencies:
    - Python 3.11+
    - asyncio, multiprocessing, threading
    - pandas, numpy, pyarrow, dask
    - torch, cudf (GPU ê°€ì†)
    - pykis, aiohttp
    - redis, sqlalchemy
"""

from __future__ import annotations

import asyncio
import json
import logging
import multiprocessing as mp
import os
import threading
import time
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
from contextlib import asynccontextmanager
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

try:
    import aiohttp
    import dask.dataframe as dd
    import numpy as np
    import pandas as pd
    import pyarrow as pa
    import pyarrow.parquet as pq
    import redis.asyncio as redis
    import torch
    from dask.distributed import Client, LocalCluster
    from pykis import KISClient
    from pykis.api import KISApi
    from sqlalchemy import create_engine, text
    from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
except ImportError:
    pass

# GPU_AVAILABLEì€ ì‹¤ì œë¡œ torch, cudf import ì„±ê³µ ì—¬ë¶€ë¡œ ê²°ì •
try:
    import cudf
    GPU_AVAILABLE = True
except ImportError:
    GPU_AVAILABLE = False

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('optimized_pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class PipelineConfig:
    """íŒŒì´í”„ë¼ì¸ ì„¤ì •"""
    # ë°ì´í„° ìˆ˜ì§‘ ì„¤ì •
    batch_size: int = 10000
    max_workers: int = mp.cpu_count()
    chunk_size: int = 100000

    # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
    use_multiprocessing: bool = True
    use_multithreading: bool = True
    use_gpu: bool = GPU_AVAILABLE

    # ìºì‹± ì„¤ì •
    cache_enabled: bool = True
    cache_ttl: int = 3600  # 1ì‹œê°„
    local_cache_dir: str = "./cache"

    # ì €ì¥ ì„¤ì •
    storage_format: str = "parquet"  # parquet, arrow, hdf5
    compression: str = "snappy"

    # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
    enable_profiling: bool = True
    log_performance: bool = True

@dataclass
class DataCollectionConfig:
    """ë°ì´í„° ìˆ˜ì§‘ ì„¤ì •"""
    # ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘
    historical_start_date: str = "2020-01-01"
    historical_end_date: str = "2025-01-27"
    historical_batch_days: int = 30  # 30ì¼ì”© ë°°ì¹˜ ì²˜ë¦¬

    # ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘
    realtime_interval: float = 1.0
    realtime_buffer_size: int = 1000

    # ì¢…ëª© ì„¤ì •
    kospi_symbols: List[str] = field(default_factory=lambda: [
        "005930", "000660", "035420", "051910", "006400",  # ì‚¼ì„±ì „ì, SKí•˜ì´ë‹‰ìŠ¤, NAVER, LGí™”í•™, ì‚¼ì„±SDI
        "035720", "207940", "068270", "323410", "373220",  # ì¹´ì¹´ì˜¤, ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤, ì…€íŠ¸ë¦¬ì˜¨, ì¹´ì¹´ì˜¤ë±…í¬, LGì—ë„ˆì§€ì†”ë£¨ì…˜
        "005380", "000270", "015760", "017670", "032830",  # í˜„ëŒ€ì°¨, ê¸°ì•„, í•œêµ­ì „ë ¥, SKí…”ë ˆì½¤, ì‚¼ì„±ìƒëª…
    ])

    kosdaq_symbols: List[str] = field(default_factory=lambda: [
        "091990", "122870", "086520", "096770", "018260",  # ì…€íŠ¸ë¦¬ì˜¨í—¬ìŠ¤ì¼€ì–´, ì™€ì´ì§€ì—”í„°í…Œì¸ë¨¼íŠ¸, ì—ì½”í”„ë¡œ, SKì´ë…¸ë² ì´ì…˜, ì‚¼ì„±ì—ìŠ¤ë””ì—ìŠ¤
    ])

class OptimizedDataPipeline:
    """ìµœì í™”ëœ ë°ì´í„° íŒŒì´í”„ë¼ì¸"""

    def __init__(self, pipeline_config: PipelineConfig, collection_config: DataCollectionConfig):
        self.pipeline_config = pipeline_config
        self.collection_config = collection_config

        # í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.kis_client = None
        self.kis_api = None
        self.redis_client = None
        self.dask_client = None

        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.performance_stats = {
            'collection_time': [],
            'processing_time': [],
            'storage_time': [],
            'total_records': 0,
            'start_time': None
        }

        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        Path(self.pipeline_config.local_cache_dir).mkdir(parents=True, exist_ok=True)

    async def initialize(self):
        """ì´ˆê¸°í™”"""
        logger.info("ìµœì í™”ëœ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì‹œì‘")

        # KIS API ì´ˆê¸°í™”
        await self._initialize_kis_client()

        # Redis í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        await self._initialize_redis()

        # Dask í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        await self._initialize_dask()

        # GPU í™•ì¸
        if self.pipeline_config.use_gpu and GPU_AVAILABLE:
            logger.info("GPU ê°€ì† í™œì„±í™”")
        else:
            logger.info("CPU ëª¨ë“œë¡œ ì‹¤í–‰")

        self.performance_stats['start_time'] = datetime.now()
        logger.info("ì´ˆê¸°í™” ì™„ë£Œ")

    async def _initialize_kis_client(self):
        """KIS API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            app_key = os.getenv('LIVE_KIS_APP_KEY')
            app_secret = os.getenv('LIVE_KIS_APP_SECRET')
            account_code = os.getenv('LIVE_KIS_ACCOUNT_NUMBER', '')

            if not app_key or not app_secret:
                raise ValueError("KIS API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

            self.kis_client = KISClient(
                api_key=app_key,
                api_secret=app_secret,
                acc_no=account_code,
                mock=False
            )

            self.kis_api = KISApi(self.kis_client)
            logger.info("KIS API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")

        except Exception as e:
            logger.error(f"KIS API í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    async def _initialize_redis(self):
        """Redis í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            redis_url = os.getenv('REDIS_URL', 'redis://localhost:6379/0')
            self.redis_client = redis.from_url(redis_url, decode_responses=True)
            await self.redis_client.ping()
            logger.info("Redis í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì„±ê³µ")

        except Exception as e:
            logger.error(f"Redis í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    async def _initialize_dask(self):
        """Dask í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            # ë¡œì»¬ í´ëŸ¬ìŠ¤í„° ìƒì„±
            cluster = LocalCluster(
                n_workers=self.pipeline_config.max_workers,
                threads_per_worker=2,
                memory_limit='2GB'
            )

            self.dask_client = Client(cluster)
            logger.info(f"Dask í´ëŸ¬ìŠ¤í„° ì´ˆê¸°í™” ì„±ê³µ: {self.dask_client}")

        except Exception as e:
            logger.error(f"Dask í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise

    async def collect_historical_data_parallel(self):
        """ë³‘ë ¬ ì²˜ë¦¬ë¡œ ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘"""
        logger.info("ë³‘ë ¬ ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")

        start_time = time.time()

        # ë‚ ì§œ ë²”ìœ„ ìƒì„±
        start_date = datetime.strptime(self.collection_config.historical_start_date, "%Y-%m-%d")
        end_date = datetime.strptime(self.collection_config.historical_end_date, "%Y-%m-%d")

        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ë‚ ì§œ ë¶„í• 
        date_batches = []
        current_date = start_date
        while current_date <= end_date:
            batch_end = min(
                current_date + timedelta(days=self.collection_config.historical_batch_days),
                end_date
            )
            date_batches.append((current_date, batch_end))
            current_date = batch_end + timedelta(days=1)

        logger.info(f"ì´ {len(date_batches)}ê°œ ë°°ì¹˜ë¡œ ë¶„í• ")

        # ë³‘ë ¬ ì²˜ë¦¬ë¡œ ë°ì´í„° ìˆ˜ì§‘
        all_symbols = (self.collection_config.kospi_symbols +
                      self.collection_config.kosdaq_symbols)

        total_records = 0

        # ProcessPoolExecutorë¡œ ë³‘ë ¬ ì²˜ë¦¬
        with ProcessPoolExecutor(max_workers=self.pipeline_config.max_workers) as executor:
            # ê° ë°°ì¹˜ë³„ë¡œ íƒœìŠ¤í¬ ìƒì„±
            futures = []
            for batch_start, batch_end in date_batches:
                for symbol in all_symbols:
                    future = executor.submit(
                        self._collect_symbol_historical_data,
                        symbol,
                        batch_start.strftime("%Y-%m-%d"),
                        batch_end.strftime("%Y-%m-%d")
                    )
                    futures.append(future)

            # ê²°ê³¼ ìˆ˜ì§‘
            for future in as_completed(futures):
                try:
                    result = future.result()
                    if result:
                        total_records += len(result)
                        # ìºì‹œì— ì €ì¥
                        await self._save_to_cache(result)
                except Exception as e:
                    logger.error(f"ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")

        collection_time = time.time() - start_time
        self.performance_stats['collection_time'].append(collection_time)
        self.performance_stats['total_records'] += total_records

        logger.info(f"ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {total_records:,}ê°œ ë ˆì½”ë“œ, {collection_time:.2f}ì´ˆ")

        return total_records

    def _collect_symbol_historical_data(self, symbol: str, start_date: str, end_date: str) -> List[Dict]:
        """ê°œë³„ ì¢…ëª© ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ (í”„ë¡œì„¸ìŠ¤ë³„ ì‹¤í–‰)"""
        try:
            # KIS API í˜¸ì¶œ (ë™ê¸° ë°©ì‹)
            app_key = os.getenv('LIVE_KIS_APP_KEY')
            app_secret = os.getenv('LIVE_KIS_APP_SECRET')
            account_code = os.getenv('LIVE_KIS_ACCOUNT_NUMBER', '')

            kis_client = KISClient(
                api_key=app_key,
                api_secret=app_secret,
                acc_no=account_code,
                mock=False
            )
            kis_api = KISApi(kis_client)

            # OHLCV ë°ì´í„° ìˆ˜ì§‘
            ohlcv_data = kis_api.get_kr_ohlcv(symbol, "D", 1000)

            # ë°ì´í„° ë³€í™˜
            records = []
            if hasattr(ohlcv_data, 'to_dict'):
                df = ohlcv_data
            else:
                df = pd.DataFrame(ohlcv_data)

            for _, row in df.iterrows():
                record = {
                    'symbol': symbol,
                    'date': row.get('date', start_date),
                    'open': row.get('open', 0),
                    'high': row.get('high', 0),
                    'low': row.get('low', 0),
                    'close': row.get('close', 0),
                    'volume': row.get('volume', 0),
                    'category': 'kospi' if symbol.startswith('00') else 'kosdaq',
                    'data_type': 'historical'
                }
                records.append(record)

            logger.info(f"ì¢…ëª© {symbol} ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {len(records)}ê°œ ë ˆì½”ë“œ")
            return records

        except Exception as e:
            logger.error(f"ì¢…ëª© {symbol} ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []

    async def collect_realtime_data_async(self):
        """ë¹„ë™ê¸° ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘"""
        logger.info("ë¹„ë™ê¸° ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")

        start_time = time.time()

        # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ
        semaphore = asyncio.Semaphore(self.pipeline_config.max_workers)

        all_symbols = (self.collection_config.kospi_symbols +
                      self.collection_config.kosdaq_symbols)

        # ë¹„ë™ê¸° íƒœìŠ¤í¬ ìƒì„±
        tasks = []
        for symbol in all_symbols:
            task = asyncio.create_task(
                self._collect_symbol_realtime_data(symbol, semaphore)
            )
            tasks.append(task)

        # ëª¨ë“  íƒœìŠ¤í¬ ì‹¤í–‰
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # ê²°ê³¼ ì§‘ê³„
        total_records = sum(len(result) for result in results if isinstance(result, list))

        collection_time = time.time() - start_time
        self.performance_stats['collection_time'].append(collection_time)
        self.performance_stats['total_records'] += total_records

        logger.info(f"ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {total_records:,}ê°œ ë ˆì½”ë“œ, {collection_time:.2f}ì´ˆ")

        return total_records

    async def _collect_symbol_realtime_data(self, symbol: str, semaphore: asyncio.Semaphore) -> List[Dict]:
        """ê°œë³„ ì¢…ëª© ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘"""
        async with semaphore:
            try:
                # ìºì‹œ í™•ì¸
                cache_key = f"realtime:{symbol}:{int(time.time())}"
                cached_data = await self.redis_client.get(cache_key)

                if cached_data and self.pipeline_config.cache_enabled:
                    return []

                # í˜„ì¬ê°€ ì¡°íšŒ
                current_price = self.kis_api.get_kr_current_price(symbol)

                # í˜¸ê°€ ë°ì´í„° ì¡°íšŒ
                orderbook = self.kis_api.get_kr_orderbook(symbol)

                # ë°ì´í„° í¬ì¸íŠ¸ ìƒì„±
                timestamp = datetime.now()

                record = {
                    'symbol': symbol,
                    'timestamp': timestamp.isoformat(),
                    'current_price': current_price,
                    'orderbook': orderbook,
                    'category': 'kospi' if symbol.startswith('00') else 'kosdaq',
                    'data_type': 'realtime'
                }

                # ìºì‹œì— ì €ì¥
                if self.pipeline_config.cache_enabled:
                    await self.redis_client.setex(
                        cache_key,
                        self.pipeline_config.cache_ttl,
                        str(record)
                    )

                return [record]

            except Exception as e:
                logger.error(f"ì¢…ëª© {symbol} ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                return []

    async def process_data_parallel(self, data: List[Dict]) -> pd.DataFrame:
        """ë³‘ë ¬ ë°ì´í„° ì „ì²˜ë¦¬"""
        logger.info("ë³‘ë ¬ ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")

        start_time = time.time()

        # Dask DataFrameìœ¼ë¡œ ë³€í™˜
        df = pd.DataFrame(data)
        ddf = dd.from_pandas(df, npartitions=self.pipeline_config.max_workers)

        # GPU ê°€ì† ì‚¬ìš© ì‹œ
        if self.pipeline_config.use_gpu and GPU_AVAILABLE:
            # cuDFë¡œ ë³€í™˜
            gdf = cudf.DataFrame.from_pandas(df)

            # GPUì—ì„œ ì „ì²˜ë¦¬
            processed_gdf = await self._preprocess_gpu(gdf)

            # ë‹¤ì‹œ pandasë¡œ ë³€í™˜
            processed_df = processed_gdf.to_pandas()
        else:
            # CPUì—ì„œ ì „ì²˜ë¦¬
            processed_df = await self._preprocess_cpu(ddf)

        processing_time = time.time() - start_time
        self.performance_stats['processing_time'].append(processing_time)

        logger.info(f"ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: {len(processed_df):,}ê°œ ë ˆì½”ë“œ, {processing_time:.2f}ì´ˆ")

        return processed_df

    async def _preprocess_cpu(self, ddf: dd.DataFrame) -> pd.DataFrame:
        """CPU ê¸°ë°˜ ë°ì´í„° ì „ì²˜ë¦¬"""
        # ë°ì´í„° íƒ€ì… ë³€í™˜
        ddf['timestamp'] = dd.to_datetime(ddf['timestamp'])
        ddf['current_price'] = ddf['current_price'].astype(float)

        # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
        ddf = ddf.map_partitions(self._calculate_technical_indicators)

        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        ddf = ddf.fillna(0)

        # ê³„ì‚° ì‹¤í–‰
        return ddf.compute()

    async def _preprocess_gpu(self, gdf: 'cudf.DataFrame') -> 'cudf.DataFrame':
        """GPU ê¸°ë°˜ ë°ì´í„° ì „ì²˜ë¦¬"""
        # GPUì—ì„œ ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
        gdf = self._calculate_technical_indicators_gpu(gdf)

        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        gdf = gdf.fillna(0)

        return gdf

    def _calculate_technical_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° (CPU)"""
        if len(df) == 0:
            return df

        # ì´ë™í‰ê· 
        df['ma_5'] = df['current_price'].rolling(window=5).mean()
        df['ma_20'] = df['current_price'].rolling(window=20).mean()

        # RSI ê³„ì‚°
        delta = df['current_price'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        df['rsi'] = 100 - (100 / (1 + rs))

        # ë³€ë™ì„±
        df['volatility'] = df['current_price'].rolling(window=20).std()

        return df

    def _calculate_technical_indicators_gpu(self, gdf: 'cudf.DataFrame') -> 'cudf.DataFrame':
        """ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° (GPU)"""
        if len(gdf) == 0:
            return gdf

        # GPUì—ì„œ ì´ë™í‰ê·  ê³„ì‚°
        gdf['ma_5'] = gdf['current_price'].rolling(window=5).mean()
        gdf['ma_20'] = gdf['current_price'].rolling(window=20).mean()

        # GPUì—ì„œ RSI ê³„ì‚°
        delta = gdf['current_price'].diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
        rs = gain / loss
        gdf['rsi'] = 100 - (100 / (1 + rs))

        # GPUì—ì„œ ë³€ë™ì„± ê³„ì‚°
        gdf['volatility'] = gdf['current_price'].rolling(window=20).std()

        return gdf

    async def save_data_optimized(self, df: pd.DataFrame, filename: str):
        """ìµœì í™”ëœ ë°ì´í„° ì €ì¥"""
        logger.info(f"ìµœì í™”ëœ ë°ì´í„° ì €ì¥ ì‹œì‘: {filename}")

        start_time = time.time()

        # ì €ì¥ í˜•ì‹ë³„ ìµœì í™”
        if self.pipeline_config.storage_format == "parquet":
            await self._save_parquet_optimized(df, filename)
        elif self.pipeline_config.storage_format == "arrow":
            await self._save_arrow_optimized(df, filename)
        elif self.pipeline_config.storage_format == "hdf5":
            await self._save_hdf5_optimized(df, filename)
        else:
            await self._save_parquet_optimized(df, filename)

        storage_time = time.time() - start_time
        self.performance_stats['storage_time'].append(storage_time)

        logger.info(f"ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filename}, {storage_time:.2f}ì´ˆ")

    async def _save_parquet_optimized(self, df: pd.DataFrame, filename: str):
        """Parquet ìµœì í™” ì €ì¥"""
        # PyArrow í…Œì´ë¸”ë¡œ ë³€í™˜
        table = pa.Table.from_pandas(df)

        # ì••ì¶• ì„¤ì •
        compression = self.pipeline_config.compression

        # ì²­í¬ ë‹¨ìœ„ë¡œ ì €ì¥
        chunk_size = self.pipeline_config.chunk_size

        if len(df) > chunk_size:
            # ëŒ€ìš©ëŸ‰ ë°ì´í„°ëŠ” ì²­í¬ ë‹¨ìœ„ë¡œ ë¶„í•  ì €ì¥
            chunks = [df[i:i+chunk_size] for i in range(0, len(df), chunk_size)]

            for i, chunk in enumerate(chunks):
                chunk_table = pa.Table.from_pandas(chunk)
                chunk_filename = f"{filename}_part_{i:04d}.parquet"

                pq.write_table(
                    chunk_table,
                    chunk_filename,
                    compression=compression,
                    row_group_size=10000
                )
        else:
            # ì†Œìš©ëŸ‰ ë°ì´í„°ëŠ” ë‹¨ì¼ íŒŒì¼ë¡œ ì €ì¥
            pq.write_table(
                table,
                f"{filename}.parquet",
                compression=compression,
                row_group_size=10000
            )

    async def _save_arrow_optimized(self, df: pd.DataFrame, filename: str):
        """Arrow ìµœì í™” ì €ì¥"""
        table = pa.Table.from_pandas(df)

        # Arrow íŒŒì¼ë¡œ ì €ì¥
        with pa.OSFile(f"{filename}.arrow", 'wb') as sink:
            with pa.RecordBatchStreamWriter(sink, table.schema) as writer:
                writer.write_table(table)

    async def _save_hdf5_optimized(self, df: pd.DataFrame, filename: str):
        """HDF5 ìµœì í™” ì €ì¥"""
        # HDF5ë¡œ ì €ì¥ (ì••ì¶• í¬í•¨)
        df.to_hdf(
            f"{filename}.h5",
            key='data',
            mode='w',
            complevel=9,
            complib='blosc'
        )

    async def _save_to_cache(self, data: List[Dict]):
        """ìºì‹œì— ë°ì´í„° ì €ì¥"""
        if not self.pipeline_config.cache_enabled:
            return

        try:
            # Redisì— ì €ì¥
            cache_key = f"batch:{int(time.time())}"
            await self.redis_client.setex(
                cache_key,
                self.pipeline_config.cache_ttl,
                str(data)
            )

            # ë¡œì»¬ ìºì‹œì—ë„ ì €ì¥
            cache_file = Path(self.pipeline_config.local_cache_dir) / f"{cache_key}.json"
            with open(cache_file, 'w') as f:
                json.dump(data, f)

        except Exception as e:
            logger.error(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    async def run_optimized_pipeline(self):
        """ìµœì í™”ëœ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info("ğŸš€ ìµœì í™”ëœ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹œì‘")

        try:
            # 1. ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ (ë³‘ë ¬ ì²˜ë¦¬)
            historical_records = await self.collect_historical_data_parallel()

            # 2. ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ (ë¹„ë™ê¸°)
            realtime_records = await self.collect_realtime_data_async()

            # 3. ë°ì´í„° í†µí•©
            all_data = []

            # ìºì‹œì—ì„œ ë°ì´í„° ë¡œë“œ
            if self.pipeline_config.cache_enabled:
                cache_data = await self._load_from_cache()
                all_data.extend(cache_data)

            # 4. ë³‘ë ¬ ì „ì²˜ë¦¬
            if all_data:
                processed_df = await self.process_data_parallel(all_data)

                # 5. ìµœì í™”ëœ ì €ì¥
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                filename = f"optimized_data_{timestamp}"
                await self.save_data_optimized(processed_df, filename)

            # 6. ì„±ëŠ¥ í†µê³„ ì¶œë ¥
            await self._print_performance_stats()

        except Exception as e:
            logger.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            raise

    async def _load_from_cache(self) -> List[Dict]:
        """ìºì‹œì—ì„œ ë°ì´í„° ë¡œë“œ"""
        try:
            all_data = []

            # Redis ìºì‹œì—ì„œ ë¡œë“œ
            keys = await self.redis_client.keys("batch:*")
            for key in keys:
                data_str = await self.redis_client.get(key)
                if data_str:
                    data = json.loads(data_str)
                    all_data.extend(data)

            # ë¡œì»¬ ìºì‹œì—ì„œ ë¡œë“œ
            cache_dir = Path(self.pipeline_config.local_cache_dir)
            for cache_file in cache_dir.glob("*.json"):
                with open(cache_file, 'r') as f:
                    data = json.load(f)
                    all_data.extend(data)

            logger.info(f"ìºì‹œì—ì„œ {len(all_data)}ê°œ ë ˆì½”ë“œ ë¡œë“œ")
            return all_data

        except Exception as e:
            logger.error(f"ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return []

    async def _print_performance_stats(self):
        """ì„±ëŠ¥ í†µê³„ ì¶œë ¥"""
        if not self.performance_stats['start_time']:
            return

        total_time = datetime.now() - self.performance_stats['start_time']

        avg_collection_time = np.mean(self.performance_stats['collection_time']) if self.performance_stats['collection_time'] else 0
        avg_processing_time = np.mean(self.performance_stats['processing_time']) if self.performance_stats['processing_time'] else 0
        avg_storage_time = np.mean(self.performance_stats['storage_time']) if self.performance_stats['storage_time'] else 0

        logger.info("ğŸ¯ ìµœì í™”ëœ íŒŒì´í”„ë¼ì¸ ì„±ëŠ¥ í†µê³„:")
        logger.info(f"   ì´ ì‹¤í–‰ ì‹œê°„: {total_time}")
        logger.info(f"   ì´ ë ˆì½”ë“œ ìˆ˜: {self.performance_stats['total_records']:,}")
        logger.info(f"   í‰ê·  ìˆ˜ì§‘ ì‹œê°„: {avg_collection_time:.2f}ì´ˆ")
        logger.info(f"   í‰ê·  ì²˜ë¦¬ ì‹œê°„: {avg_processing_time:.2f}ì´ˆ")
        logger.info(f"   í‰ê·  ì €ì¥ ì‹œê°„: {avg_storage_time:.2f}ì´ˆ")
        logger.info(f"   ì²˜ë¦¬ ì†ë„: {self.performance_stats['total_records'] / total_time.total_seconds():.0f} ë ˆì½”ë“œ/ì´ˆ")

        if self.pipeline_config.use_gpu and GPU_AVAILABLE:
            logger.info("   GPU ê°€ì†: í™œì„±í™”")
        else:
            logger.info("   GPU ê°€ì†: ë¹„í™œì„±í™”")

async def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ ìµœì í™”ëœ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹œì‘")
    print("=" * 60)

    # ì„¤ì • ìƒì„±
    pipeline_config = PipelineConfig()
    collection_config = DataCollectionConfig()

    # íŒŒì´í”„ë¼ì¸ ìƒì„± ë° ì‹¤í–‰
    pipeline = OptimizedDataPipeline(pipeline_config, collection_config)

    try:
        await pipeline.initialize()
        await pipeline.run_optimized_pipeline()
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        print("âœ… ìµœì í™”ëœ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")

if __name__ == "__main__":
    asyncio.run(main())

