"""
íˆ¬ì ë‰´ìŠ¤ ìˆ˜ì§‘ ëª¨ë“ˆ
ì‹¤ì‹œê°„ ê¸ˆìœµ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ê°ì • ë¶„ì„
ğŸš€ Gemini AI ìµœì í™”ë¥¼ ìœ„í•œ ê³ í’ˆì§ˆ ë‰´ìŠ¤ ë°ì´í„° ê°€ê³µ ì‹œìŠ¤í…œ
"""

import asyncio
import logging
from typing import List, Dict, Optional, Tuple, Set
from dataclasses import dataclass, field
from datetime import datetime, timedelta
import pandas as pd
import numpy as np
import requests
from bs4 import BeautifulSoup
import aiohttp
import feedparser
import re
import json
from urllib.parse import urljoin, urlparse
import time
import warnings
from concurrent.futures import ThreadPoolExecutor, as_completed
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, WebDriverException
import os

warnings.filterwarnings('ignore')
logger = logging.getLogger(__name__)

@dataclass
class NewsQualityMetrics:
    """ë‰´ìŠ¤ í’ˆì§ˆ ì§€í‘œ"""
    relevance_score: float = 0.0      # ê´€ë ¨ì„± ì ìˆ˜ (0-100)
    credibility_score: float = 0.0    # ì‹ ë¢°ë„ ì ìˆ˜ (0-100)
    freshness_score: float = 0.0      # ì‹ ì„ ë„ ì ìˆ˜ (0-100)
    sentiment_confidence: float = 0.0  # ê°ì • ë¶„ì„ ì‹ ë¢°ë„ (0-100)
    overall_quality: float = 0.0      # ì „ì²´ í’ˆì§ˆ ì ìˆ˜ (0-100)
    word_count: int = 0               # ë‹¨ì–´ ìˆ˜
    has_financial_keywords: bool = False  # ê¸ˆìœµ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€

@dataclass
class NewsData:
    """ë‰´ìŠ¤ ë°ì´í„° í´ë˜ìŠ¤ - Gemini AI ìµœì í™”"""
    # ê¸°ë³¸ ì •ë³´
    title: str
    content: str
    url: str
    source: str
    published_date: datetime
    
    # ë¶„ë¥˜ ë° íƒœê·¸
    category: str = "general"
    tags: List[str] = field(default_factory=list)
    related_stocks: List[str] = field(default_factory=list)
    market_sector: Optional[str] = None
    
    # ê°ì • ë¶„ì„
    sentiment_score: float = 0.0      # -1(ë¶€ì •) ~ 1(ê¸ì •)
    sentiment_label: str = "neutral"   # positive, negative, neutral
    confidence_score: float = 0.0     # ì‹ ë¢°ë„ ì ìˆ˜
    
    # ì¤‘ìš”ë„ ë° ì˜í–¥ë„
    importance_score: float = 0.0     # 0-100
    market_impact_score: float = 0.0  # ì‹œì¥ ì˜í–¥ë„ 0-100
    
    # í‚¤ì›Œë“œ ë° ì—”í‹°í‹°
    financial_keywords: List[str] = field(default_factory=list)
    company_mentions: List[str] = field(default_factory=list)
    numeric_data: Dict[str, float] = field(default_factory=dict)
    
    # í’ˆì§ˆ ë° ë©”íƒ€ë°ì´í„°
    quality_metrics: NewsQualityMetrics = field(default_factory=NewsQualityMetrics)
    timestamp: datetime = field(default_factory=datetime.now)
    data_source: str = "web_scraping"
    
    def calculate_quality_score(self) -> float:
        """ë‰´ìŠ¤ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°"""
        try:
            # ê¸°ë³¸ í’ˆì§ˆ ìš”ì†Œë“¤
            title_quality = min(100, len(self.title) * 2) if self.title else 0
            content_quality = min(100, len(self.content) / 10) if self.content else 0
            freshness = max(0, 100 - (datetime.now() - self.published_date).days * 10)
            
            # ê¸ˆìœµ ê´€ë ¨ì„±
            financial_relevance = 50 if self.financial_keywords else 0
            if self.related_stocks:
                financial_relevance += 30
            if self.company_mentions:
                financial_relevance += 20
            
            # ì „ì²´ í’ˆì§ˆ ì ìˆ˜
            overall_quality = (
                title_quality * 0.2 +
                content_quality * 0.3 +
                freshness * 0.2 +
                financial_relevance * 0.3
            )
            
            self.quality_metrics.overall_quality = min(100, overall_quality)
            self.quality_metrics.relevance_score = financial_relevance
            self.quality_metrics.freshness_score = freshness
            self.quality_metrics.word_count = len(self.content.split()) if self.content else 0
            self.quality_metrics.has_financial_keywords = bool(self.financial_keywords)
            
            return self.quality_metrics.overall_quality
            
        except Exception as e:
            logger.warning(f"ë‰´ìŠ¤ í’ˆì§ˆ ì ìˆ˜ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return 0.0

class KoreanNewsCollector:
    """í•œêµ­ ê¸ˆìœµ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        self.sources = {
            "ë„¤ì´ë²„ê¸ˆìœµ": {
                "url": "https://finance.naver.com/news/news_list.naver?mode=LSS2D&section_id=101&section_id2=258",
                "rss": "https://finance.naver.com/news/rss.naver?section_id=101&section_id2=258"
            },
            "í•œêµ­ê²½ì œ": {
                "url": "https://www.hankyung.com/finance",
                "rss": "https://www.hankyung.com/feed/finance"
            },
            "ë§¤ì¼ê²½ì œ": {
                "url": "https://www.mk.co.kr/news/stock/",
                "rss": "https://www.mk.co.kr/rss/40300001/"
            },
            "ì´ë°ì¼ë¦¬": {
                "url": "https://www.edaily.co.kr/news/newsList.asp?newsType=&sub_cd=AA01",
                "rss": "https://www.edaily.co.kr/rss/rss.asp?sub_cd=AA01"
            }
        }
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'ko-KR,ko;q=0.8,en-US;q=0.5,en;q=0.3'
        })
        
        # ê¸ˆìœµ í‚¤ì›Œë“œ ì‚¬ì „
        self.financial_keywords = {
            "ì‹œì¥": ["ì½”ìŠ¤í”¼", "ì½”ìŠ¤ë‹¥", "ë‚˜ìŠ¤ë‹¥", "ë‹¤ìš°", "S&P", "ì¦ì‹œ", "ì£¼ê°€", "ì§€ìˆ˜"],
            "ê¸°ì—…": ["ì‚¼ì„±", "LG", "í˜„ëŒ€", "SK", "í¬ìŠ¤ì½”", "ë„¤ì´ë²„", "ì¹´ì¹´ì˜¤"],
            "ê²½ì œ": ["ê¸ˆë¦¬", "í™˜ìœ¨", "ì¸í”Œë ˆì´ì…˜", "GDP", "ìˆ˜ì¶œ", "ìˆ˜ì…", "ë¬´ì—­"],
            "íˆ¬ì": ["íˆ¬ì", "í€ë“œ", "ì±„ê¶Œ", "ì£¼ì‹", "ë°°ë‹¹", "ìƒì¥", "IPO"],
            "ì •ì±…": ["í•œêµ­ì€í–‰", "ê¸ˆí†µìœ„", "ê¸°ì¤€ê¸ˆë¦¬", "ì •ë¶€", "ì •ì±…", "ê·œì œ"]
        }
    
    async def collect_rss_news(self, source_name: str, rss_url: str, limit: int = 20) -> List[NewsData]:
        """RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        news_list = []
        try:
            logger.info(f"{source_name} RSS ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘: {rss_url}")
            
            # RSS í”¼ë“œ íŒŒì‹±
            feed = feedparser.parse(rss_url)
            
            for entry in feed.entries[:limit]:
                try:
                    # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
                    title = entry.get('title', '').strip()
                    link = entry.get('link', '')
                    summary = entry.get('summary', '').strip()
                    
                    # ë°œí–‰ì¼ íŒŒì‹±
                    published_date = datetime.now()
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        published_date = datetime(*entry.published_parsed[:6])
                    
                    # ìƒì„¸ ë‚´ìš© ìˆ˜ì§‘
                    content = await self._fetch_article_content(link)
                    if not content:
                        content = summary
                    
                    # ë‰´ìŠ¤ ë°ì´í„° ìƒì„±
                    news_data = NewsData(
                        title=title,
                        content=content,
                        url=link,
                        source=source_name,
                        published_date=published_date,
                        category="finance",
                        data_source="rss"
                    )
                    
                    # í‚¤ì›Œë“œ ë° ê°ì • ë¶„ì„
                    self._analyze_news_content(news_data)
                    
                    # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
                    quality_score = news_data.calculate_quality_score()
                    if quality_score > 30:  # ìµœì†Œ í’ˆì§ˆ ê¸°ì¤€
                        news_list.append(news_data)
                        
                except Exception as e:
                    logger.warning(f"RSS í•­ëª© ì²˜ë¦¬ ì‹¤íŒ¨ ({source_name}): {e}")
                    continue
            
            logger.info(f"{source_name} RSS ë‰´ìŠ¤ {len(news_list)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            return news_list
            
        except Exception as e:
            logger.error(f"{source_name} RSS ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    async def _fetch_article_content(self, url: str) -> str:
        """ê¸°ì‚¬ ë³¸ë¬¸ ë‚´ìš© ì¶”ì¶œ"""
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, timeout=10) as response:
                    if response.status == 200:
                        html = await response.text()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # ì¼ë°˜ì ì¸ ê¸°ì‚¬ ë³¸ë¬¸ ì„ íƒìë“¤
                        content_selectors = [
                            '.article_body',
                            '.news_content',
                            '.article-content',
                            '.content',
                            '#articleBodyContents',
                            '.article_txt'
                        ]
                        
                        for selector in content_selectors:
                            content_elem = soup.select_one(selector)
                            if content_elem:
                                # ë¶ˆí•„ìš”í•œ íƒœê·¸ ì œê±°
                                for tag in content_elem.find_all(['script', 'style', 'iframe', 'img']):
                                    tag.decompose()
                                
                                text = content_elem.get_text(strip=True)
                                if len(text) > 100:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                                    return text
                        
                        # ê¸°ë³¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                        return soup.get_text()[:1000]  # ìµœëŒ€ 1000ì
                        
        except Exception as e:
            logger.warning(f"ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ {url}: {e}")
            return ""
    
    def _analyze_news_content(self, news_data: NewsData):
        """ë‰´ìŠ¤ ë‚´ìš© ë¶„ì„ (í‚¤ì›Œë“œ, ê°ì •, ê´€ë ¨ ì£¼ì‹)"""
        try:
            full_text = f"{news_data.title} {news_data.content}".lower()
            
            # ê¸ˆìœµ í‚¤ì›Œë“œ ì¶”ì¶œ
            found_keywords = []
            for category, keywords in self.financial_keywords.items():
                for keyword in keywords:
                    if keyword.lower() in full_text:
                        found_keywords.append(keyword)
            
            news_data.financial_keywords = list(set(found_keywords))
            
            # íšŒì‚¬ëª… ì¶”ì¶œ
            company_patterns = [
                r'([ê°€-í£]+)(?:ì „ì|í™”í•™|ë¬¼ì‚°|ê±´ì„¤|ê·¸ë£¹|í™€ë”©ìŠ¤?)',
                r'(ì‚¼ì„±|LG|í˜„ëŒ€|SK|í¬ìŠ¤ì½”|ë„¤ì´ë²„|ì¹´ì¹´ì˜¤|ì…€íŠ¸ë¦¬ì˜¨)'
            ]
            
            companies = []
            for pattern in company_patterns:
                matches = re.findall(pattern, full_text)
                companies.extend(matches)
            
            news_data.company_mentions = list(set(companies))
            
            # ìˆ«ì ë°ì´í„° ì¶”ì¶œ (ì£¼ê°€, ê±°ë˜ëŸ‰ ë“±)
            numeric_patterns = {
                'price': r'(\d+(?:,\d+)*)\s*ì›',
                'percentage': r'(\d+\.?\d*)\s*%',
                'volume': r'(\d+(?:,\d+)*)\s*ì£¼'
            }
            
            numeric_data = {}
            for data_type, pattern in numeric_patterns.items():
                matches = re.findall(pattern, full_text)
                if matches:
                    try:
                        # ì²« ë²ˆì§¸ ë§¤ì¹˜ ê°’ì„ ìˆ«ìë¡œ ë³€í™˜
                        value = float(matches[0].replace(',', ''))
                        numeric_data[data_type] = value
                    except ValueError:
                        pass
            
            news_data.numeric_data = numeric_data
            
            # ê°„ë‹¨í•œ ê°ì • ë¶„ì„ (í‚¤ì›Œë“œ ê¸°ë°˜)
            positive_words = ['ìƒìŠ¹', 'ì¦ê°€', 'í˜¸ì¡°', 'ì„±ì¥', 'ê°œì„ ', 'í™•ëŒ€', 'ì‹ ê³ ê°€']
            negative_words = ['í•˜ë½', 'ê°ì†Œ', 'ë¶€ì§„', 'ì•…í™”', 'ìœ„ê¸°', 'ì†ì‹¤', 'ì‹ ì €ê°€']
            
            positive_count = sum(1 for word in positive_words if word in full_text)
            negative_count = sum(1 for word in negative_words if word in full_text)
            
            if positive_count > negative_count:
                news_data.sentiment_label = "positive"
                news_data.sentiment_score = min(1.0, (positive_count - negative_count) / 10)
            elif negative_count > positive_count:
                news_data.sentiment_label = "negative"
                news_data.sentiment_score = max(-1.0, (positive_count - negative_count) / 10)
            else:
                news_data.sentiment_label = "neutral"
                news_data.sentiment_score = 0.0
            
            # ì¤‘ìš”ë„ ì ìˆ˜ ê³„ì‚°
            importance_factors = [
                len(news_data.financial_keywords) * 5,
                len(news_data.company_mentions) * 10,
                len(news_data.numeric_data) * 15,
                min(50, len(news_data.content) / 20)
            ]
            
            news_data.importance_score = min(100, sum(importance_factors))
            
        except Exception as e:
            logger.warning(f"ë‰´ìŠ¤ ë‚´ìš© ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    async def collect_all_korean_news(self, limit_per_source: int = 20) -> List[NewsData]:
        """ëª¨ë“  í•œêµ­ ë‰´ìŠ¤ ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        all_news = []
        
        tasks = []
        for source_name, source_info in self.sources.items():
            if 'rss' in source_info:
                task = self.collect_rss_news(source_name, source_info['rss'], limit_per_source)
                tasks.append(task)
        
        # ë¹„ë™ê¸° ì‹¤í–‰
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in results:
            if isinstance(result, list):
                all_news.extend(result)
            elif isinstance(result, Exception):
                logger.error(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {result}")
        
        # ì¤‘ë³µ ì œê±° (URL ê¸°ì¤€)
        seen_urls = set()
        unique_news = []
        for news in all_news:
            if news.url not in seen_urls:
                seen_urls.add(news.url)
                unique_news.append(news)
        
        # í’ˆì§ˆ ì ìˆ˜ ê¸°ì¤€ ì •ë ¬
        unique_news.sort(key=lambda x: x.quality_metrics.overall_quality, reverse=True)
        
        logger.info(f"í•œêµ­ ë‰´ìŠ¤ ì´ {len(unique_news)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
        return unique_news

class GlobalNewsCollector:
    """ê¸€ë¡œë²Œ ê¸ˆìœµ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        self.sources = {
            "Yahoo Finance": "https://feeds.finance.yahoo.com/rss/2.0/headline",
            "MarketWatch": "http://feeds.marketwatch.com/marketwatch/topstories/",
            "Reuters Business": "http://feeds.reuters.com/reuters/businessNews",
            "Bloomberg": "https://feeds.bloomberg.com/markets/news.rss",
            "CNBC": "https://www.cnbc.com/id/100003114/device/rss/rss.html"
        }
        
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        })
    
    async def collect_global_news(self, limit_per_source: int = 15) -> List[NewsData]:
        """ê¸€ë¡œë²Œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        all_news = []
        
        for source_name, rss_url in self.sources.items():
            try:
                logger.info(f"{source_name} ê¸€ë¡œë²Œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
                
                feed = feedparser.parse(rss_url)
                
                for entry in feed.entries[:limit_per_source]:
                    try:
                        title = entry.get('title', '').strip()
                        link = entry.get('link', '')
                        summary = entry.get('summary', '').strip()
                        
                        # ë°œí–‰ì¼ íŒŒì‹±
                        published_date = datetime.now()
                        if hasattr(entry, 'published_parsed') and entry.published_parsed:
                            published_date = datetime(*entry.published_parsed[:6])
                        
                        news_data = NewsData(
                            title=title,
                            content=summary,
                            url=link,
                            source=source_name,
                            published_date=published_date,
                            category="global_finance",
                            data_source="global_rss"
                        )
                        
                        # ì˜ë¬¸ í‚¤ì›Œë“œ ë¶„ì„
                        self._analyze_global_content(news_data)
                        
                        if news_data.calculate_quality_score() > 25:
                            all_news.append(news_data)
                            
                    except Exception as e:
                        logger.warning(f"ê¸€ë¡œë²Œ ë‰´ìŠ¤ í•­ëª© ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        continue
                
                logger.info(f"{source_name} ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
                
            except Exception as e:
                logger.error(f"{source_name} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                continue
        
        return all_news
    
    def _analyze_global_content(self, news_data: NewsData):
        """ê¸€ë¡œë²Œ ë‰´ìŠ¤ ë‚´ìš© ë¶„ì„"""
        try:
            full_text = f"{news_data.title} {news_data.content}".lower()
            
            # ê¸€ë¡œë²Œ ê¸ˆìœµ í‚¤ì›Œë“œ
            global_keywords = {
                "markets": ["nasdaq", "dow", "s&p", "ftse", "nikkei", "dax"],
                "companies": ["apple", "microsoft", "amazon", "google", "tesla", "nvidia"],
                "economics": ["fed", "interest rate", "inflation", "gdp", "unemployment"],
                "crypto": ["bitcoin", "ethereum", "cryptocurrency", "blockchain"]
            }
            
            found_keywords = []
            for category, keywords in global_keywords.items():
                for keyword in keywords:
                    if keyword in full_text:
                        found_keywords.append(keyword)
            
            news_data.financial_keywords = found_keywords
            
            # ê°„ë‹¨í•œ ì˜ë¬¸ ê°ì • ë¶„ì„
            positive_words = ['gain', 'rise', 'surge', 'bull', 'growth', 'profit']
            negative_words = ['fall', 'drop', 'crash', 'bear', 'loss', 'decline']
            
            positive_count = sum(1 for word in positive_words if word in full_text)
            negative_count = sum(1 for word in negative_words if word in full_text)
            
            if positive_count > negative_count:
                news_data.sentiment_label = "positive"
                news_data.sentiment_score = min(1.0, (positive_count - negative_count) / 5)
            elif negative_count > positive_count:
                news_data.sentiment_label = "negative"
                news_data.sentiment_score = max(-1.0, (positive_count - negative_count) / 5)
            
        except Exception as e:
            logger.warning(f"ê¸€ë¡œë²Œ ë‰´ìŠ¤ ë¶„ì„ ì‹¤íŒ¨: {e}")

class InvestingNewsCollector:
    """í†µí•© íˆ¬ì ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° - Gemini AI ìµœì í™”"""
    
    def __init__(self):
        self.korean_collector = KoreanNewsCollector()
        self.global_collector = GlobalNewsCollector()
        self.news_cache: List[NewsData] = []
        self.last_update = datetime.now() - timedelta(hours=1)
        
        # ì„¤ì •ê°’ë“¤
        self.update_interval = int(os.getenv('NEWS_UPDATE_INTERVAL', 5))  # ë¶„
        self.news_limit = int(os.getenv('NEWS_LIMIT', 50))
        
        logger.info(f"InvestingNewsCollector ì´ˆê¸°í™” ì™„ë£Œ (ì—…ë°ì´íŠ¸ ê°„ê²©: {self.update_interval}ë¶„)")
    
    async def collect_all_news(self, force_update: bool = False) -> List[NewsData]:
        """ëª¨ë“  ë‰´ìŠ¤ ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        try:
            # ìºì‹œ í™•ì¸
            if not force_update and self._is_cache_valid():
                logger.info("ìºì‹œëœ ë‰´ìŠ¤ ë°ì´í„° ë°˜í™˜")
                return self.news_cache[:self.news_limit]
            
            logger.info("ìƒˆë¡œìš´ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
            
            # ë³‘ë ¬ë¡œ í•œêµ­ ë° ê¸€ë¡œë²Œ ë‰´ìŠ¤ ìˆ˜ì§‘
            korean_task = self.korean_collector.collect_all_korean_news(20)
            global_task = self.global_collector.collect_global_news(15)
            
            korean_news, global_news = await asyncio.gather(
                korean_task, global_task, return_exceptions=True
            )
            
            # ê²°ê³¼ í•©ì¹˜ê¸°
            all_news = []
            if isinstance(korean_news, list):
                all_news.extend(korean_news)
            if isinstance(global_news, list):
                all_news.extend(global_news)
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            all_news = self._deduplicate_and_sort(all_news)
            
            # ìºì‹œ ì—…ë°ì´íŠ¸
            self.news_cache = all_news
            self.last_update = datetime.now()
            
            logger.info(f"ì´ {len(all_news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
            return all_news[:self.news_limit]
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜: {e}")
            return self.news_cache[:self.news_limit] if self.news_cache else []
    
    def _is_cache_valid(self) -> bool:
        """ìºì‹œ ìœ íš¨ì„± í™•ì¸"""
        time_diff = datetime.now() - self.last_update
        return time_diff.total_seconds() < (self.update_interval * 60) and bool(self.news_cache)
    
    def _deduplicate_and_sort(self, news_list: List[NewsData]) -> List[NewsData]:
        """ì¤‘ë³µ ì œê±° ë° ì •ë ¬"""
        # URL ê¸°ì¤€ ì¤‘ë³µ ì œê±°
        seen_urls = set()
        unique_news = []
        
        for news in news_list:
            if news.url not in seen_urls:
                seen_urls.add(news.url)
                unique_news.append(news)
        
        # í’ˆì§ˆ ì ìˆ˜ì™€ ì¤‘ìš”ë„ ê¸°ì¤€ ì •ë ¬
        unique_news.sort(
            key=lambda x: (x.quality_metrics.overall_quality, x.importance_score), 
            reverse=True
        )
        
        return unique_news
    
    def get_market_sentiment(self) -> Dict[str, any]:
        """ì‹œì¥ ê°ì • ë¶„ì„ ê²°ê³¼"""
        if not self.news_cache:
            return {"sentiment": "neutral", "confidence": 0.0, "news_count": 0}
        
        try:
            sentiments = [news.sentiment_score for news in self.news_cache if news.sentiment_score != 0]
            
            if not sentiments:
                return {"sentiment": "neutral", "confidence": 0.0, "news_count": len(self.news_cache)}
            
            avg_sentiment = np.mean(sentiments)
            confidence = min(100, len(sentiments) * 2)
            
            if avg_sentiment > 0.1:
                sentiment_label = "positive"
            elif avg_sentiment < -0.1:
                sentiment_label = "negative"
            else:
                sentiment_label = "neutral"
            
            return {
                "sentiment": sentiment_label,
                "score": float(avg_sentiment),
                "confidence": float(confidence),
                "news_count": len(self.news_cache),
                "positive_news": len([n for n in self.news_cache if n.sentiment_score > 0]),
                "negative_news": len([n for n in self.news_cache if n.sentiment_score < 0]),
                "neutral_news": len([n for n in self.news_cache if n.sentiment_score == 0])
            }
            
        except Exception as e:
            logger.error(f"ì‹œì¥ ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {"sentiment": "neutral", "confidence": 0.0, "news_count": 0}
    
    def prepare_gemini_news_dataset(self) -> Dict[str, any]:
        """Gemini AIë¥¼ ìœ„í•œ ë‰´ìŠ¤ ë°ì´í„°ì…‹ ì¤€ë¹„"""
        try:
            if not self.news_cache:
                return {"error": "ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤"}
            
            # ê³ í’ˆì§ˆ ë‰´ìŠ¤ë§Œ ì„ ë³„ (í’ˆì§ˆ ì ìˆ˜ 50 ì´ìƒ)
            high_quality_news = [
                news for news in self.news_cache 
                if news.quality_metrics.overall_quality >= 50
            ]
            
            # Gemini AI í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            gemini_dataset = {
                "market_sentiment": self.get_market_sentiment(),
                "news_summary": {
                    "total_news": len(self.news_cache),
                    "high_quality_news": len(high_quality_news),
                    "korean_news": len([n for n in self.news_cache if n.category == "finance"]),
                    "global_news": len([n for n in self.news_cache if n.category == "global_finance"]),
                    "last_update": self.last_update.isoformat()
                },
                "top_news": [
                    {
                        "title": news.title,
                        "source": news.source,
                        "sentiment": news.sentiment_label,
                        "sentiment_score": news.sentiment_score,
                        "importance": news.importance_score,
                        "keywords": news.financial_keywords,
                        "companies": news.company_mentions,
                        "published": news.published_date.isoformat(),
                        "url": news.url
                    }
                    for news in high_quality_news[:20]  # ìƒìœ„ 20ê°œ
                ],
                "keyword_analysis": self._analyze_trending_keywords(),
                "company_mentions": self._analyze_company_mentions()
            }
            
            return gemini_dataset
            
        except Exception as e:
            logger.error(f"Gemini ë°ì´í„°ì…‹ ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    def _analyze_trending_keywords(self) -> Dict[str, int]:
        """íŠ¸ë Œë”© í‚¤ì›Œë“œ ë¶„ì„"""
        try:
            keyword_counts = {}
            
            for news in self.news_cache:
                for keyword in news.financial_keywords:
                    keyword_counts[keyword] = keyword_counts.get(keyword, 0) + 1
            
            # ìƒìœ„ 15ê°œ í‚¤ì›Œë“œ
            sorted_keywords = sorted(keyword_counts.items(), key=lambda x: x[1], reverse=True)
            return dict(sorted_keywords[:15])
            
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def _analyze_company_mentions(self) -> Dict[str, int]:
        """ê¸°ì—… ì–¸ê¸‰ ë¶„ì„"""
        try:
            company_counts = {}
            
            for news in self.news_cache:
                for company in news.company_mentions:
                    company_counts[company] = company_counts.get(company, 0) + 1
            
            # ìƒìœ„ 10ê°œ ê¸°ì—…
            sorted_companies = sorted(company_counts.items(), key=lambda x: x[1], reverse=True)
            return dict(sorted_companies[:10])
            
        except Exception as e:
            logger.error(f"ê¸°ì—… ì–¸ê¸‰ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def get_news_by_category(self, category: str) -> List[NewsData]:
        """ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ ì¡°íšŒ"""
        return [news for news in self.news_cache if news.category == category]
    
    def get_news_by_sentiment(self, sentiment: str) -> List[NewsData]:
        """ê°ì •ë³„ ë‰´ìŠ¤ ì¡°íšŒ"""
        return [news for news in self.news_cache if news.sentiment_label == sentiment]
    
    def search_news(self, keyword: str) -> List[NewsData]:
        """í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ ê²€ìƒ‰"""
        keyword_lower = keyword.lower()
        results = []
        
        for news in self.news_cache:
            if (keyword_lower in news.title.lower() or 
                keyword_lower in news.content.lower() or
                keyword_lower in [k.lower() for k in news.financial_keywords]):
                results.append(news)
        
        return sorted(results, key=lambda x: x.quality_metrics.overall_quality, reverse=True)

# í¸ì˜ í•¨ìˆ˜ë“¤
async def collect_latest_news(limit: int = 50) -> List[NewsData]:
    """ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘ (í¸ì˜ í•¨ìˆ˜)"""
    collector = InvestingNewsCollector()
    return await collector.collect_all_news()

def get_market_sentiment_summary() -> Dict[str, any]:
    """ì‹œì¥ ê°ì • ìš”ì•½ (í¸ì˜ í•¨ìˆ˜)"""
    collector = InvestingNewsCollector()
    return collector.get_market_sentiment()

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    async def test_news_collection():
        collector = InvestingNewsCollector()
        news_list = await collector.collect_all_news()
        
        print(f"ìˆ˜ì§‘ëœ ë‰´ìŠ¤: {len(news_list)}ê°œ")
        
        if news_list:
            print("\n=== ìƒìœ„ 5ê°œ ë‰´ìŠ¤ ===")
            for i, news in enumerate(news_list[:5], 1):
                print(f"{i}. {news.title}")
                print(f"   ì¶œì²˜: {news.source}")
                print(f"   ê°ì •: {news.sentiment_label} ({news.sentiment_score:.2f})")
                print(f"   í’ˆì§ˆ: {news.quality_metrics.overall_quality:.1f}")
                print(f"   í‚¤ì›Œë“œ: {', '.join(news.financial_keywords[:5])}")
                print()
        
        # ì‹œì¥ ê°ì • ë¶„ì„
        sentiment = collector.get_market_sentiment()
        print("=== ì‹œì¥ ê°ì • ë¶„ì„ ===")
        print(f"ì „ì²´ ê°ì •: {sentiment['sentiment']}")
        print(f"ì‹ ë¢°ë„: {sentiment['confidence']:.1f}%")
        print(f"ê¸ì • ë‰´ìŠ¤: {sentiment.get('positive_news', 0)}ê°œ")
        print(f"ë¶€ì • ë‰´ìŠ¤: {sentiment.get('negative_news', 0)}ê°œ")
    
    # asyncio.run(test_news_collection()) 