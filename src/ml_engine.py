#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ§  ë¨¸ì‹ ëŸ¬ë‹ ì—”ì§„ (Machine Learning Engine)
=======================================

íˆ¬ì ë¶„ì„ì„ ìœ„í•œ ê²½ëŸ‰í™”ëœ ë¨¸ì‹ ëŸ¬ë‹ ì—”ì§„ì…ë‹ˆë‹¤.
TensorFlow ëŒ€ì‹  scikit-learnì„ ì‚¬ìš©í•˜ì—¬ ì‹œìŠ¤í…œ ìì›ì„ íš¨ìœ¨ì ìœ¼ë¡œ í™œìš©í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
1. ê²½ëŸ‰ ML ëª¨ë¸ (Lightweight ML Models)
   - Random Forest: ì•™ìƒë¸” ê¸°ë°˜ ì˜ˆì¸¡
   - Gradient Boosting: ë¶€ìŠ¤íŒ… ê¸°ë°˜ ì˜ˆì¸¡  
   - Linear Regression: ì„ í˜• ê´€ê³„ ëª¨ë¸
   - ì•™ìƒë¸” ì˜ˆì¸¡: ì—¬ëŸ¬ ëª¨ë¸ì˜ ê°€ì¤‘ í‰ê· 

2. íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ (Feature Engineering)
   - ì£¼ì‹ ë°ì´í„°ë¥¼ ML íŠ¹ì„±ìœ¼ë¡œ ë³€í™˜
   - ê¸°ìˆ ì  ì§€í‘œ ê¸°ë°˜ íŠ¹ì„± ìƒì„±
   - ë°ì´í„° ì •ê·œí™” ë° ìŠ¤ì¼€ì¼ë§
   - ì‹œê³„ì—´ íŠ¹ì„± ì¶”ì¶œ

3. ìë™ í•™ìŠµ ì‹œìŠ¤í…œ (Auto Learning System)
   - í•©ì„± ë°ì´í„° ìƒì„±ìœ¼ë¡œ ì´ˆê¸° í•™ìŠµ
   - ì‹¤ì œ ë°ì´í„° ëˆ„ì ìœ¼ë¡œ ì ì§„ì  í•™ìŠµ
   - ëª¨ë¸ ì„±ëŠ¥ ìë™ í‰ê°€ ë° ê°œì„ 
   - ì˜ˆì¸¡ ì‹ ë¢°ë„ ê³„ì‚°

4. ì˜ˆì¸¡ ë° ë¶„ì„ (Prediction & Analysis)
   - ì£¼ê°€ ë°©í–¥ ì˜ˆì¸¡ (ìƒìŠ¹/í•˜ë½/ì¤‘ë¦½)
   - ì˜ˆì¸¡ ì‹ ë¢°ë„ ë° í™•ë¥  ì œê³µ
   - íŠ¹ì„± ì¤‘ìš”ë„ ë¶„ì„
   - ëª¨ë¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì¶”ì 

5. ëª¨ë¸ ê´€ë¦¬ (Model Management)
   - ëª¨ë¸ ì €ì¥ ë° ë¡œë“œ
   - ì„±ëŠ¥ íˆìŠ¤í† ë¦¬ ê´€ë¦¬
   - ìë™ ì¬í•™ìŠµ ìŠ¤ì¼€ì¤„ë§
   - ëª¨ë¸ ìƒíƒœ ëª¨ë‹ˆí„°ë§

íŠ¹ì§•:
- ë©”ëª¨ë¦¬ íš¨ìœ¨ì : TensorFlow ì—†ì´ ê²½ëŸ‰ êµ¬í˜„
- ì‹¤ì‹œê°„ ì˜ˆì¸¡: ë¹ ë¥¸ ì¶”ë¡  ì†ë„
- ì ì‘ì  í•™ìŠµ: ì‹œì¥ ë³€í™”ì— ìë™ ì ì‘
- ì•ˆì •ì„±: ê²¬ê³ í•œ ì˜¤ë¥˜ ì²˜ë¦¬

ì´ ì—”ì§„ì€ íˆ¬ì ì „ëµê³¼ ê¸°ìˆ ì  ë¶„ì„ ê²°ê³¼ë¥¼ MLë¡œ ë³´ê°•í•˜ì—¬
ë” ì •í™•í•œ íˆ¬ì ì˜ì‚¬ê²°ì •ì„ ì§€ì›í•©ë‹ˆë‹¤.
"""

import os
import logging
import pickle
import numpy as np
import pandas as pd
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
from datetime import datetime, timedelta
import json
from pathlib import Path

# scikit-learn ê¸°ë°˜ ML
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

logger = logging.getLogger(__name__)

@dataclass
class MLPrediction:
    """ML ì˜ˆì¸¡ ê²°ê³¼"""
    direction: str  # "up", "down", "neutral"
    confidence: float  # 0.0 ~ 1.0
    price_change_percent: float
    probability_up: float
    probability_down: float
    features_importance: Dict[str, float]

@dataclass  
class ModelPerformance:
    """ëª¨ë¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­"""
    mae: float
    mse: float
    r2: float
    accuracy: float
    training_date: datetime

class LightweightMLEngine:
    """ê²½ëŸ‰í™”ëœ ML ì—”ì§„ (scikit-learn ê¸°ë°˜)"""
    
    def __init__(self, model_dir: str = "models"):
        self.model_dir = Path(model_dir)
        self.model_dir.mkdir(exist_ok=True)
        
        # ìŠ¤ì¼€ì¼ëŸ¬
        self.feature_scaler = StandardScaler()
        self.target_scaler = MinMaxScaler()
        
        # ëª¨ë¸ë“¤
        self.models = {
            'random_forest': RandomForestRegressor(
                n_estimators=50,  # ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ ì¶•ì†Œ
                max_depth=10,
                random_state=42,
                n_jobs=1  # CPU ì•ˆì •ì„± ê³ ë ¤
            ),
            'gradient_boost': GradientBoostingRegressor(
                n_estimators=30,  # ë©”ëª¨ë¦¬ ì ˆì•½
                max_depth=6,
                random_state=42
            ),
            'linear': Ridge(alpha=1.0)
        }
        
        # ì•™ìƒë¸” ê°€ì¤‘ì¹˜
        self.ensemble_weights = {
            'random_forest': 0.4,
            'gradient_boost': 0.4,
            'linear': 0.2
        }
        
        # ì„±ëŠ¥ ê¸°ë¡
        self.performance_history = []
        self.is_trained = False
        
        # íŠ¹ì„±ëª…
        self.feature_names = [
            'trend_score', 'momentum_score', 'returns_1w', 
            'returns_1m', 'volatility', 'volume_change'
        ]
        
        logger.info("ğŸ§  ê²½ëŸ‰ ML ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def prepare_features(self, stock_data: Dict[str, Any]) -> np.ndarray:
        """ì£¼ì‹ ë°ì´í„°ë¥¼ ML íŠ¹ì„±ìœ¼ë¡œ ë³€í™˜"""
        
        features = []
        
        for symbol, data in stock_data.items():
            feature_vector = [
                data.get('trend_score', 50),
                data.get('momentum_score', 50),
                data.get('returns_1w', 0),
                data.get('returns_1m', 0),
                data.get('volatility', 20),
                data.get('volume_change', 0)
            ]
            features.append(feature_vector)
        
        return np.array(features)
    
    def generate_synthetic_training_data(self, n_samples: int = 1000) -> Tuple[np.ndarray, np.ndarray]:
        """í•©ì„± í•™ìŠµ ë°ì´í„° ìƒì„± (ì‹¤ì œ ë°ì´í„° ë¶€ì¡±ì‹œ ì‚¬ìš©)"""
        
        logger.info(f"ğŸ“Š í•©ì„± í•™ìŠµ ë°ì´í„° {n_samples}ê°œ ìƒì„± ì¤‘...")
        
        np.random.seed(42)
        
        # íŠ¹ì„± ìƒì„±
        X = np.random.rand(n_samples, len(self.feature_names))
        
        # íŠ¹ì„±ë³„ í˜„ì‹¤ì ì¸ ë²”ìœ„ ì ìš©
        X[:, 0] = X[:, 0] * 100  # trend_score: 0-100
        X[:, 1] = X[:, 1] * 100  # momentum_score: 0-100
        X[:, 2] = (X[:, 2] - 0.5) * 20  # returns_1w: -10% ~ +10%
        X[:, 3] = (X[:, 3] - 0.5) * 40  # returns_1m: -20% ~ +20%
        X[:, 4] = X[:, 4] * 50 + 10  # volatility: 10-60%
        X[:, 5] = (X[:, 5] - 0.5) * 100  # volume_change: -50% ~ +50%
        
        # íƒ€ê²Ÿ ìƒì„± (ë‹¤ìŒ ê¸°ê°„ ìˆ˜ìµë¥ )
        # ê°„ë‹¨í•œ ì„ í˜• ê´€ê³„ + ë…¸ì´ì¦ˆ
        y = (
            X[:, 0] * 0.3 +  # ì¶”ì„¸ê°€ ë†’ìœ¼ë©´ ìˆ˜ìµë¥  ì¦ê°€
            X[:, 1] * 0.2 +  # ëª¨ë©˜í…€ì´ ë†’ìœ¼ë©´ ìˆ˜ìµë¥  ì¦ê°€
            X[:, 2] * 0.5 +  # ìµœê·¼ ìˆ˜ìµë¥ ê³¼ ì—°ê´€
            np.random.normal(0, 5, n_samples)  # ë…¸ì´ì¦ˆ ì¶”ê°€
        ) / 100
        
        return X, y
    
    def train_models(self, X: np.ndarray, y: np.ndarray) -> Dict[str, ModelPerformance]:
        """ëª¨ë¸ í•™ìŠµ"""
        
        logger.info(f"ğŸ¯ ëª¨ë¸ í•™ìŠµ ì‹œì‘ - ë°ì´í„°: {X.shape}")
        
        # ë°ì´í„° ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # íŠ¹ì„± ìŠ¤ì¼€ì¼ë§
        X_train_scaled = self.feature_scaler.fit_transform(X_train)
        X_test_scaled = self.feature_scaler.transform(X_test)
        
        # íƒ€ê²Ÿ ìŠ¤ì¼€ì¼ë§ (íšŒê·€ìš©)
        y_train_scaled = self.target_scaler.fit_transform(y_train.reshape(-1, 1)).ravel()
        y_test_scaled = self.target_scaler.transform(y_test.reshape(-1, 1)).ravel()
        
        performance_results = {}
        
        # ê° ëª¨ë¸ í•™ìŠµ
        for model_name, model in self.models.items():
            logger.info(f"  ğŸ“ˆ {model_name} í•™ìŠµ ì¤‘...")
            
            try:
                # ëª¨ë¸ í•™ìŠµ
                model.fit(X_train_scaled, y_train_scaled)
                
                # ì˜ˆì¸¡
                y_pred_scaled = model.predict(X_test_scaled)
                y_pred = self.target_scaler.inverse_transform(y_pred_scaled.reshape(-1, 1)).ravel()
                
                # ì„±ëŠ¥ ê³„ì‚°
                mae = mean_absolute_error(y_test, y_pred)
                mse = mean_squared_error(y_test, y_pred)
                r2 = r2_score(y_test, y_pred)
                
                # ë°©í–¥ ì •í™•ë„ ê³„ì‚°
                direction_accuracy = np.mean(
                    (y_test > 0) == (y_pred > 0)
                )
                
                performance = ModelPerformance(
                    mae=mae,
                    mse=mse,
                    r2=r2,
                    accuracy=direction_accuracy,
                    training_date=datetime.now()
                )
                
                performance_results[model_name] = performance
                
                logger.info(f"    âœ… {model_name}: ì •í™•ë„ {direction_accuracy:.3f}, RÂ² {r2:.3f}")
                
            except Exception as e:
                logger.error(f"    âŒ {model_name} í•™ìŠµ ì‹¤íŒ¨: {e}")
        
        self.is_trained = True
        self.performance_history.append({
            'timestamp': datetime.now(),
            'models': performance_results
        })
        
        # ëª¨ë¸ ì €ì¥
        self.save_models()
        
        logger.info("âœ… ëª¨ë¸ í•™ìŠµ ì™„ë£Œ")
        return performance_results
    
    def predict_price_direction(self, features: np.ndarray) -> MLPrediction:
        """ê°€ê²© ë°©í–¥ ì˜ˆì¸¡"""
        
        if not self.is_trained:
            logger.warning("âš ï¸ ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•ŠìŒ - ê¸°ë³¸ ì˜ˆì¸¡ ë°˜í™˜")
            return MLPrediction(
                direction="neutral",
                confidence=0.5,
                price_change_percent=0.0,
                probability_up=0.5,
                probability_down=0.5,
                features_importance={}
            )
        
        try:
            # íŠ¹ì„± ìŠ¤ì¼€ì¼ë§
            features_scaled = self.feature_scaler.transform(features)
            
            # ì•™ìƒë¸” ì˜ˆì¸¡
            ensemble_prediction = 0
            model_predictions = {}
            
            for model_name, model in self.models.items():
                pred_scaled = model.predict(features_scaled)[0]
                pred = self.target_scaler.inverse_transform([[pred_scaled]])[0][0]
                
                model_predictions[model_name] = pred
                ensemble_prediction += pred * self.ensemble_weights[model_name]
            
            # ë°©í–¥ ê²°ì •
            if ensemble_prediction > 0.02:  # 2% ì´ìƒ ìƒìŠ¹ ì˜ˆìƒ
                direction = "up"
                confidence = min(0.9, abs(ensemble_prediction) * 10)
            elif ensemble_prediction < -0.02:  # 2% ì´ìƒ í•˜ë½ ì˜ˆìƒ
                direction = "down"
                confidence = min(0.9, abs(ensemble_prediction) * 10)
            else:
                direction = "neutral"
                confidence = 0.5
            
            # í™•ë¥  ê³„ì‚°
            prob_up = max(0.1, min(0.9, 0.5 + ensemble_prediction * 5))
            prob_down = 1 - prob_up
            
            # íŠ¹ì„± ì¤‘ìš”ë„ (Random Forest ê¸°ì¤€)
            if 'random_forest' in self.models:
                feature_importance = dict(zip(
                    self.feature_names,
                    self.models['random_forest'].feature_importances_
                ))
            else:
                feature_importance = {}
            
            return MLPrediction(
                direction=direction,
                confidence=confidence,
                price_change_percent=ensemble_prediction * 100,
                probability_up=prob_up,
                probability_down=prob_down,
                features_importance=feature_importance
            )
            
        except Exception as e:
            logger.error(f"ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
            return MLPrediction(
                direction="neutral",
                confidence=0.3,
                price_change_percent=0.0,
                probability_up=0.5,
                probability_down=0.5,
                features_importance={}
            )
    
    def save_models(self):
        """ëª¨ë¸ ì €ì¥"""
        
        try:
            # ëª¨ë¸ë“¤ ì €ì¥
            for model_name, model in self.models.items():
                model_path = self.model_dir / f"{model_name}.pkl"
                with open(model_path, 'wb') as f:
                    pickle.dump(model, f)
            
            # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
            scaler_path = self.model_dir / "scalers.pkl"
            with open(scaler_path, 'wb') as f:
                pickle.dump({
                    'feature_scaler': self.feature_scaler,
                    'target_scaler': self.target_scaler
                }, f)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata_path = self.model_dir / "metadata.json"
            metadata = {
                'feature_names': self.feature_names,
                'ensemble_weights': self.ensemble_weights,
                'is_trained': self.is_trained,
                'last_update': datetime.now().isoformat()
            }
            
            with open(metadata_path, 'w') as f:
                json.dump(metadata, f, indent=2)
            
            logger.info(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {self.model_dir}")
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    def load_models(self) -> bool:
        """ëª¨ë¸ ë¡œë“œ"""
        
        try:
            # ë©”íƒ€ë°ì´í„° í™•ì¸
            metadata_path = self.model_dir / "metadata.json"
            if not metadata_path.exists():
                logger.warning("ë©”íƒ€ë°ì´í„° íŒŒì¼ ì—†ìŒ")
                return False
            
            with open(metadata_path, 'r') as f:
                metadata = json.load(f)
            
            # ëª¨ë¸ë“¤ ë¡œë“œ
            for model_name in self.models.keys():
                model_path = self.model_dir / f"{model_name}.pkl"
                if model_path.exists():
                    with open(model_path, 'rb') as f:
                        self.models[model_name] = pickle.load(f)
            
            # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
            scaler_path = self.model_dir / "scalers.pkl"
            if scaler_path.exists():
                with open(scaler_path, 'rb') as f:
                    scalers = pickle.load(f)
                    self.feature_scaler = scalers['feature_scaler']
                    self.target_scaler = scalers['target_scaler']
            
            self.is_trained = metadata.get('is_trained', False)
            
            logger.info("âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            return True
            
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return False
    
    def get_model_status(self) -> Dict[str, Any]:
        """ëª¨ë¸ ìƒíƒœ ë°˜í™˜"""
        
        return {
            'is_trained': self.is_trained,
            'models_loaded': {name: model is not None for name, model in self.models.items()},
            'feature_count': len(self.feature_names),
            'last_training': self.performance_history[-1]['timestamp'].isoformat() if self.performance_history else None,
            'model_directory': str(self.model_dir)
        }

# ì „ì—­ ML ì—”ì§„ ì¸ìŠ¤í„´ìŠ¤ (ì˜µì…˜)
class AdaptiveLearningEngine(LightweightMLEngine):
    """ì ì‘í˜• í•™ìŠµ ì—”ì§„ (í˜¸í™˜ì„± ìœ ì§€)"""
    
    def __init__(self):
        super().__init__()
        
        # ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì‹œë„
        if not self.load_models():
            logger.info("ğŸ”„ ê¸°ì¡´ ëª¨ë¸ ì—†ìŒ - ìƒˆë¡œ í•™ìŠµ í•„ìš”")
            # í•©ì„± ë°ì´í„°ë¡œ ì´ˆê¸° í•™ìŠµ
            X_synthetic, y_synthetic = self.generate_synthetic_training_data(500)
            self.train_models(X_synthetic, y_synthetic)

if __name__ == "__main__":
    print("ğŸ§  ê²½ëŸ‰ ML ì—”ì§„ v1.0")
    print("=" * 50)
    
    # í…ŒìŠ¤íŠ¸
    ml_engine = AdaptiveLearningEngine()
    status = ml_engine.get_model_status()
    
    print(f"\nğŸ“Š ML ì—”ì§„ ìƒíƒœ:")
    print(f"  â€¢ ë¡œë“œëœ ëª¨ë¸: {sum(status['models_loaded'].values())}ê°œ")
    print(f"  â€¢ ë°ì´í„° íŒŒì¼: {status['data_files']}ê°œ")
    print(f"  â€¢ ì„±ëŠ¥ ê¸°ë¡: {len(status['performances'])}ê°œ")
    
    print("\nâœ… ML ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ!") 