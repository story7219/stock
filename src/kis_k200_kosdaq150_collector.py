#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íŒŒì¼ëª…: kis_k200_kosdaq150_collector.py
ëª©ì : KOSPI200/KOSDAQ150 ì „ì²´ ì¢…ëª©ì˜ ê³¼ê±° ì¼ë³„ ì‹œì„¸(OHLCV) ìë™ìˆ˜ì§‘ (KIS OpenAPI, ë³‘ë ¬, World-Class)
Author: Auto Trading System
Created: 2025-07-13
Version: 1.0.0

- detailed_report_20250712_054858_backup_backup.jsonì—ì„œ ì¢…ëª©ì½”ë“œ ìë™ ì¶”ì¶œ
- KIS OpenAPI ì‹¤ê³„ì •ìœ¼ë¡œ 2020~2024ë…„ ì¼ë³„ ì‹œì„¸ parquet ì €ì¥
- ë³‘ë ¬ì²˜ë¦¬, ì—ëŸ¬/ë¡œê¹…, ì¬ì‹œë„, ì„±ëŠ¥ ìµœì í™”
"""

from __future__ import annotations
import os
import json
import time
import logging
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta

import requests
import pandas as pd
import numpy as np
from collections import defaultdict
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error, r2_score
import warnings
warnings.filterwarnings('ignore')

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
KIS_APP_KEY = os.getenv("LIVE_KIS_APP_KEY")
KIS_APP_SECRET = os.getenv("LIVE_KIS_APP_SECRET")
KIS_ACC_NO = os.getenv("LIVE_KIS_ACCOUNT_NUMBER")

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    handlers=[
        logging.FileHandler("logs/kis_k200_kosdaq150_collector.log"),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)
Path("logs").mkdir(exist_ok=True)

# ìƒìˆ˜
DETAILED_REPORT_PATH = "backup/krx_k200_kosdaq50/krx_backup_20250712_054858/detailed_report_20250712_054858_backup_backup.json"
SAVE_DIR = Path("datasets/kis_k200_kosdaq150")
SAVE_DIR.mkdir(parents=True, exist_ok=True)
START_DATE = "2020-01-01"
END_DATE = "2024-12-31"
MAX_WORKERS = 8
REQUEST_INTERVAL = 0.25
MAX_RETRY = 3

RETRY_WAIT_BASE = 0.5  # Adaptive Throttling base
MAX_TOTAL_RETRY = 3

# í’ˆì§ˆ ë¦¬í¬íŠ¸ ì €ì¥ ê²½ë¡œ
QUALITY_REPORT_PATH = SAVE_DIR / "quality_report.csv"
ERROR_REPORT_PATH = SAVE_DIR / "error_report.csv"
MERGED_CSV_PATH = SAVE_DIR / "merged_all.csv"

# 1ë¶„ë´‰ ì„¤ì • ì¶”ê°€
MINUTE_DATA_ENABLED = True
MINUTE_DAYS_BACK = 30  # ìµœê·¼ 30ì¼ 1ë¶„ë´‰ ìˆ˜ì§‘

class KISAPIClient:
    def __init__(self):
        self.access_token: Optional[str] = None
        self.token_expires_at: Optional[datetime] = None
        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
        })
    def get_access_token(self) -> str:
        if self.access_token and self.token_expires_at and datetime.now() < self.token_expires_at:
            return self.access_token
        url = "https://openapi.koreainvestment.com:9443/oauth2/tokenP"
        headers = {"content-type": "application/json"}
        payload = {
            "grant_type": "client_credentials",
            "appkey": KIS_APP_KEY,
            "appsecret": KIS_APP_SECRET
        }
        try:
            response = self.session.post(url, json=payload, headers=headers, timeout=30)
            response.raise_for_status()
            data = response.json()
            token = data["access_token"]
            self.access_token = token
            self.token_expires_at = datetime.now() + timedelta(hours=23)
            logger.info("KIS API í† í° ë°œê¸‰ ì„±ê³µ")
            return token
        except Exception as e:
            logger.error(f"í† í° ë°œê¸‰ ì‹¤íŒ¨: {e}")
            raise
    def make_request(self, url: str, params: Dict[str, Any], tr_id: str, retry_count: int = 0) -> Dict[str, Any]:
        if retry_count >= MAX_RETRY:
            raise Exception(f"ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {url}")
        headers = {
            "authorization": f"Bearer {self.get_access_token()}",
            "appkey": KIS_APP_KEY,
            "appsecret": KIS_APP_SECRET,
            "tr_id": tr_id,
        }
        try:
            response = self.session.get(url, headers=headers, params=params, timeout=30)
            if response.status_code == 401:
                logger.warning("í† í° ë§Œë£Œ, ì¬ë°œê¸‰ í›„ ì¬ì‹œë„")
                self.access_token = None
                return self.make_request(url, params, tr_id, retry_count + 1)
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            logger.error(f"API ìš”ì²­ ì‹¤íŒ¨ (ì¬ì‹œë„ {retry_count + 1}/{MAX_RETRY}): {e}")
            time.sleep(REQUEST_INTERVAL * (retry_count + 1))
            return self.make_request(url, params, tr_id, retry_count + 1)

def extract_stock_codes_from_report(path: str) -> List[str]:
    with open(path, "r", encoding="utf-8") as f:
        data = json.load(f)
    codes = [item["stock_code"] for item in data["quality_reports"] if "stock_code" in item]
    logger.info(f"ì¢…ëª©ì½”ë“œ {len(codes)}ê°œ ì¶”ì¶œ ì™„ë£Œ")
    return codes

def get_stock_ohlcv(api: KISAPIClient, symbol: str, start: str, end: str) -> pd.DataFrame:
    url = "https://openapi.koreainvestment.com:9443/uapi/domestic-stock/v1/quotations/inquire-daily-price"
    tr_id = "FHKST01010400"
    params = {
        "fid_cond_mrkt_div_code": "J",
        "fid_input_iscd": symbol,
        "fid_org_adj_prc": "0",
        "fid_period_div_code": "D",
        "fid_input_date_1": start.replace("-", ""),
        "fid_input_date_2": end.replace("-", ""),
        "fid_vol_cond_code": "0"
    }
    data = api.make_request(url, params, tr_id)
    rows = data.get("output", [])
    df = pd.DataFrame(rows)
    if not df.empty:
        df.columns = [col.lower() for col in df.columns]
        if 'stck_bsop_dt' in df.columns:
            df['date'] = pd.to_datetime(df['stck_bsop_dt'], format='%Y%m%d')
        numeric_columns = ['stck_prpr', 'stck_oprc', 'stck_hgpr', 'stck_lwpr', 'acml_vol']
        for col in numeric_columns:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
    return df

def save_to_parquet(df: pd.DataFrame, symbol: str, start: str, end: str):
    fname = f"{symbol}_daily_{start.replace('-', '')}_{end.replace('-', '')}.parquet"
    df.to_parquet(SAVE_DIR / fname, index=False)
    logger.info(f"{symbol} ì €ì¥ ì™„ë£Œ: {fname}")

def collect_one(symbol: str, api: KISAPIClient, start: str, end: str, retry: int = 0, error_log: Optional[dict] = None) -> str:
    try:
        df = get_stock_ohlcv(api, symbol, start, end)
        if not df.empty:
            save_to_parquet(df, symbol, start, end)
            return f"{symbol}: OK"
        else:
            logger.warning(f"{symbol}: ë°ì´í„° ì—†ìŒ")
            if error_log is not None:
                error_log[symbol].append("NO DATA")
            return f"{symbol}: NO DATA"
    except Exception as e:
        logger.error(f"{symbol} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        if error_log is not None:
            error_log[symbol].append(str(e))
        if retry < MAX_TOTAL_RETRY:
            wait = RETRY_WAIT_BASE * (2 ** retry)
            logger.info(f"{symbol} ì¬ì‹œë„ {retry+1}/{MAX_TOTAL_RETRY} (ëŒ€ê¸° {wait:.1f}s)")
            time.sleep(wait)
            return collect_one(symbol, api, start, end, retry+1, error_log)
        return f"{symbol}: FAIL"

def quality_report():
    """ìˆ˜ì§‘ëœ parquet ì „ì²´ í’ˆì§ˆ/ê²°ì¸¡/ì´ìƒì¹˜/ì¤‘ë³µ ë¦¬í¬íŠ¸ ìƒì„± ë° ì‹œê°í™”"""
    rows = []
    for f in SAVE_DIR.glob("*_daily_*.parquet"):
        try:
            df = pd.read_parquet(f)
            n = len(df)
            n_null = df.isnull().sum().sum()
            n_dup = df.duplicated().sum()
            n_outlier = 0
            for col in [c for c in df.columns if df[c].dtype in [np.float64, np.int64]]:
                q1, q3 = df[col].quantile([0.25, 0.75])
                iqr = q3 - q1
                out = ((df[col] < (q1 - 1.5*iqr)) | (df[col] > (q3 + 1.5*iqr))).sum()
                n_outlier += out
            rows.append({
                "file": f.name, "rows": n, "nulls": n_null, "dups": n_dup, "outliers": n_outlier
            })
        except Exception as e:
            logger.error(f"í’ˆì§ˆ ë¦¬í¬íŠ¸ ì˜¤ë¥˜: {f.name}: {e}")
    
    quality_df = pd.DataFrame(rows)
    quality_df.to_csv(QUALITY_REPORT_PATH, index=False)
    logger.info(f"í’ˆì§ˆ ë¦¬í¬íŠ¸ ì €ì¥: {QUALITY_REPORT_PATH}")
    
    # ì‹œê°í™” ìƒì„±
    create_quality_visualization(quality_df)

def create_quality_visualization(quality_df: pd.DataFrame):
    """í’ˆì§ˆ ë¦¬í¬íŠ¸ ì‹œê°í™” ëŒ€ì‹œë³´ë“œ ìƒì„±"""
    if quality_df.empty:
        logger.warning("í’ˆì§ˆ ë¦¬í¬íŠ¸ ë°ì´í„°ê°€ ì—†ì–´ ì‹œê°í™”ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return
    
    # ì‹œê°í™” ì„¤ì •
    plt.style.use('seaborn-v0_8')
    fig, axes = plt.subplots(2, 2, figsize=(15, 12))
    fig.suptitle('ë°ì´í„° í’ˆì§ˆ ë¦¬í¬íŠ¸ ëŒ€ì‹œë³´ë“œ', fontsize=16, fontweight='bold')
    
    # 1. ë°ì´í„° í–‰ ìˆ˜ ë¶„í¬
    axes[0, 0].hist(quality_df['rows'], bins=20, alpha=0.7, color='skyblue', edgecolor='black')
    axes[0, 0].set_title('ë°ì´í„° í–‰ ìˆ˜ ë¶„í¬')
    axes[0, 0].set_xlabel('í–‰ ìˆ˜')
    axes[0, 0].set_ylabel('ë¹ˆë„')
    
    # 2. ê²°ì¸¡ê°’ ë¶„í¬
    axes[0, 1].scatter(quality_df['rows'], quality_df['nulls'], alpha=0.6, color='red')
    axes[0, 1].set_title('ê²°ì¸¡ê°’ vs ë°ì´í„° í–‰ ìˆ˜')
    axes[0, 1].set_xlabel('í–‰ ìˆ˜')
    axes[0, 1].set_ylabel('ê²°ì¸¡ê°’ ìˆ˜')
    
    # 3. ì´ìƒì¹˜ ë¶„í¬
    axes[1, 0].boxplot([quality_df['outliers']], labels=['ì´ìƒì¹˜'])
    axes[1, 0].set_title('ì´ìƒì¹˜ ë¶„í¬')
    axes[1, 0].set_ylabel('ì´ìƒì¹˜ ìˆ˜')
    
    # 4. ì¤‘ë³µê°’ ë¶„í¬
    axes[1, 1].hist(quality_df['dups'], bins=15, alpha=0.7, color='lightgreen', edgecolor='black')
    axes[1, 1].set_title('ì¤‘ë³µê°’ ë¶„í¬')
    axes[1, 1].set_xlabel('ì¤‘ë³µê°’ ìˆ˜')
    axes[1, 1].set_ylabel('ë¹ˆë„')
    
    plt.tight_layout()
    viz_path = SAVE_DIR / "quality_dashboard.png"
    plt.savefig(viz_path, dpi=300, bbox_inches='tight')
    plt.close()
    logger.info(f"í’ˆì§ˆ ë¦¬í¬íŠ¸ ì‹œê°í™” ì €ì¥: {viz_path}")
    
    # ìš”ì•½ í†µê³„ ì¶œë ¥
    print("\n" + "="*50)
    print("ğŸ“Š ë°ì´í„° í’ˆì§ˆ ìš”ì•½ í†µê³„")
    print("="*50)
    print(f"ì´ íŒŒì¼ ìˆ˜: {len(quality_df)}")
    print(f"í‰ê·  í–‰ ìˆ˜: {quality_df['rows'].mean():.1f}")
    print(f"ì´ ê²°ì¸¡ê°’: {quality_df['nulls'].sum()}")
    print(f"ì´ ì´ìƒì¹˜: {quality_df['outliers'].sum()}")
    print(f"ì´ ì¤‘ë³µê°’: {quality_df['dups'].sum()}")
    print("="*50)

def merge_all_to_csv():
    """ëª¨ë“  parquet â†’ í†µí•© CSV/ML ë°ì´í„°ì…‹ ë³€í™˜"""
    dfs = []
    for f in SAVE_DIR.glob("*_daily_*.parquet"):
        try:
            df = pd.read_parquet(f)
            df['symbol'] = f.name.split('_')[0]
            dfs.append(df)
        except Exception as e:
            logger.error(f"CSV ë³‘í•© ì˜¤ë¥˜: {f.name}: {e}")
    if dfs:
        pd.concat(dfs, ignore_index=True).to_csv(MERGED_CSV_PATH, index=False)
        logger.info(f"í†µí•© CSV ì €ì¥: {MERGED_CSV_PATH}")

def error_report(error_log: dict):
    """ì‹¤íŒ¨ ì¢…ëª©ë³„ ìƒì„¸ ì—ëŸ¬ ë¦¬í¬íŠ¸ ìë™ ìƒì„±"""
    rows = [{"symbol": k, "errors": '|'.join(v)} for k, v in error_log.items() if v]
    pd.DataFrame(rows).to_csv(ERROR_REPORT_PATH, index=False)
    logger.info(f"ì—ëŸ¬ ë¦¬í¬íŠ¸ ì €ì¥: {ERROR_REPORT_PATH}")

def get_stock_minute_data(api: KISAPIClient, symbol: str, days_back: int = 30) -> pd.DataFrame:
    """1ë¶„ë´‰ ë°ì´í„° ìˆ˜ì§‘"""
    url = "https://openapi.koreainvestment.com:9443/uapi/domestic-stock/v1/quotations/inquire-time-price"
    tr_id = "FHKST01010200"
    
    # ìµœê·¼ Nì¼ ì „ë¶€í„° ì˜¤ëŠ˜ê¹Œì§€
    end_date = datetime.now()
    start_date = end_date - timedelta(days=days_back)
    
    params = {
        "fid_etc_cls_code": "",
        "fid_cond_mrkt_div_code": "J",
        "fid_input_iscd": symbol,
        "fid_input_hour_1": start_date.strftime("%Y%m%d"),
        "fid_pw_data_incu_yn": "Y"
    }
    
    try:
        data = api.make_request(url, params, tr_id)
        rows = data.get("output2", [])
        df = pd.DataFrame(rows)
        
        if not df.empty:
            df.columns = [col.lower() for col in df.columns]
            if 'stck_cntg_hour' in df.columns:
                df['datetime'] = pd.to_datetime(df['stck_cntg_hour'], format='%Y%m%d%H%M%S')
            numeric_columns = ['stck_prpr', 'stck_oprc', 'stck_hgpr', 'stck_lwpr', 'cntg_vol']
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
        
        return df
    except Exception as e:
        logger.error(f"1ë¶„ë´‰ ìˆ˜ì§‘ ì‹¤íŒ¨ {symbol}: {e}")
        return pd.DataFrame()

def save_minute_data_to_parquet(df: pd.DataFrame, symbol: str):
    """1ë¶„ë´‰ ë°ì´í„°ë¥¼ parquetë¡œ ì €ì¥"""
    if not df.empty:
        fname = f"{symbol}_minute_{datetime.now().strftime('%Y%m%d')}.parquet"
        df.to_parquet(SAVE_DIR / fname, index=False)
        logger.info(f"{symbol} 1ë¶„ë´‰ ì €ì¥ ì™„ë£Œ: {fname}")

def collect_minute_data(symbol: str, api: KISAPIClient) -> str:
    """1ë¶„ë´‰ ë°ì´í„° ìˆ˜ì§‘"""
    try:
        df = get_stock_minute_data(api, symbol, MINUTE_DAYS_BACK)
        if not df.empty:
            save_minute_data_to_parquet(df, symbol)
            return f"{symbol}: MINUTE OK"
        else:
            return f"{symbol}: MINUTE NO DATA"
    except Exception as e:
        logger.error(f"{symbol} 1ë¶„ë´‰ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return f"{symbol}: MINUTE FAIL"

def ml_auto_training():
    """ML í•™ìŠµ ìë™í™” - ê°€ê²© ì˜ˆì¸¡ ëª¨ë¸"""
    try:
        # í†µí•© ë°ì´í„° ë¡œë“œ
        if not MERGED_CSV_PATH.exists():
            logger.warning("í†µí•© CSV íŒŒì¼ì´ ì—†ì–´ ML í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        df = pd.read_csv(MERGED_CSV_PATH)
        logger.info(f"ML í•™ìŠµ ì‹œì‘: {len(df)} í–‰, {len(df.columns)} ì»¬ëŸ¼")
        
        # ë°ì´í„° ì „ì²˜ë¦¬
        df_processed = preprocess_for_ml(df)
        if df_processed.empty:
            logger.warning("ì „ì²˜ë¦¬ í›„ ë°ì´í„°ê°€ ì—†ì–´ ML í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return
        
        # ëª¨ë¸ í•™ìŠµ ë° í‰ê°€
        results = train_price_prediction_model(df_processed)
        
        # ê²°ê³¼ ì €ì¥
        save_ml_results(results)
        
        logger.info("ML í•™ìŠµ ìë™í™” ì™„ë£Œ!")
        
    except Exception as e:
        logger.error(f"ML í•™ìŠµ ìë™í™” ì‹¤íŒ¨: {e}")

def preprocess_for_ml(df: pd.DataFrame) -> pd.DataFrame:
    """ML í•™ìŠµì„ ìœ„í•œ ë°ì´í„° ì „ì²˜ë¦¬"""
    try:
        # í•„ìš”í•œ ì»¬ëŸ¼ë§Œ ì„ íƒ
        required_cols = ['symbol', 'date', 'stck_prpr', 'stck_oprc', 'stck_hgpr', 'stck_lwpr', 'acml_vol']
        available_cols = [col for col in required_cols if col in df.columns]
        
        if len(available_cols) < 4:
            logger.warning("í•„ìš”í•œ ì»¬ëŸ¼ì´ ë¶€ì¡±í•©ë‹ˆë‹¤.")
            return pd.DataFrame()
        
        df_subset = df[available_cols].copy()
        
        # ê²°ì¸¡ê°’ ì²˜ë¦¬
        df_subset = df_subset.dropna()
        
        # ë‚ ì§œ ë³€í™˜
        if 'date' in df_subset.columns:
            df_subset['date'] = pd.to_datetime(df_subset['date'])
            df_subset['year'] = df_subset['date'].dt.year
            df_subset['month'] = df_subset['date'].dt.month
            df_subset['day'] = df_subset['date'].dt.day
        
        # ê¸°ìˆ ì  ì§€í‘œ ì¶”ê°€
        if 'stck_prpr' in df_subset.columns:
            df_subset['price_change'] = df_subset.groupby('symbol')['stck_prpr'].diff()
            df_subset['price_change_pct'] = df_subset.groupby('symbol')['stck_prpr'].pct_change()
            
            # ì´ë™í‰ê· 
            df_subset['ma_5'] = df_subset.groupby('symbol')['stck_prpr'].rolling(5).mean().reset_index(0, drop=True)
            df_subset['ma_20'] = df_subset.groupby('symbol')['stck_prpr'].rolling(20).mean().reset_index(0, drop=True)
        
        # ë²”ì£¼í˜• ë³€ìˆ˜ ì¸ì½”ë”©
        if 'symbol' in df_subset.columns:
            df_subset['symbol_encoded'] = df_subset['symbol'].astype('category').cat.codes
        
        # ìµœì¢… ì •ë¦¬
        numeric_cols = df_subset.select_dtypes(include=[np.number]).columns
        df_final = df_subset[numeric_cols].dropna()
        
        logger.info(f"ì „ì²˜ë¦¬ ì™„ë£Œ: {len(df_final)} í–‰, {len(df_final.columns)} ì»¬ëŸ¼")
        return df_final
        
    except Exception as e:
        logger.error(f"ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

def train_price_prediction_model(df: pd.DataFrame) -> dict:
    """ê°€ê²© ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ë° í‰ê°€"""
    try:
        # íƒ€ê²Ÿ ë³€ìˆ˜ ì„¤ì • (ë‹¤ìŒë‚  ì¢…ê°€ ì˜ˆì¸¡)
        if 'stck_prpr' not in df.columns:
            logger.warning("ê°€ê²© ì»¬ëŸ¼ì´ ì—†ì–´ ëª¨ë¸ í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.")
            return {}
        
        # íŠ¹ì„±ê³¼ íƒ€ê²Ÿ ë¶„ë¦¬
        feature_cols = [col for col in df.columns if col not in ['stck_prpr']]
        X = df[feature_cols]
        y = df['stck_prpr']
        
        # í›ˆë ¨/í…ŒìŠ¤íŠ¸ ë¶„í• 
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # ëª¨ë¸ í•™ìŠµ
        model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
        model.fit(X_train, y_train)
        
        # ì˜ˆì¸¡
        y_pred = model.predict(X_test)
        
        # í‰ê°€
        mse = mean_squared_error(y_test, y_pred)
        r2 = r2_score(y_test, y_pred)
        rmse = np.sqrt(mse)
        
        # íŠ¹ì„± ì¤‘ìš”ë„
        feature_importance = pd.DataFrame({
            'feature': feature_cols,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False)
        
        results = {
            'model': model,
            'mse': mse,
            'rmse': rmse,
            'r2': r2,
            'feature_importance': feature_importance,
            'X_test': X_test,
            'y_test': y_test,
            'y_pred': y_pred
        }
        
        logger.info(f"ëª¨ë¸ í•™ìŠµ ì™„ë£Œ - RÂ²: {r2:.4f}, RMSE: {rmse:.2f}")
        return results
        
    except Exception as e:
        logger.error(f"ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {e}")
        return {}

def save_ml_results(results: dict):
    """ML ê²°ê³¼ ì €ì¥ ë° ì‹œê°í™”"""
    if not results:
        return
    
    # ê²°ê³¼ ì €ì¥
    ml_results_path = SAVE_DIR / "ml_results.json"
    ml_results = {
        'mse': results['mse'],
        'rmse': results['rmse'],
        'r2': results['r2'],
        'feature_importance': results['feature_importance'].to_dict('records')
    }
    
    with open(ml_results_path, 'w') as f:
        json.dump(ml_results, f, indent=2)
    
    logger.info(f"ML ê²°ê³¼ ì €ì¥: {ml_results_path}")
    
    # ì‹œê°í™”
    create_ml_visualization(results)

def create_ml_visualization(results: dict):
    """ML ê²°ê³¼ ì‹œê°í™”"""
    try:
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle('ML ëª¨ë¸ ì„±ëŠ¥ ëŒ€ì‹œë³´ë“œ', fontsize=16, fontweight='bold')
        
        # 1. ì‹¤ì œ vs ì˜ˆì¸¡
        axes[0, 0].scatter(results['y_test'], results['y_pred'], alpha=0.6, color='blue')
        axes[0, 0].plot([results['y_test'].min(), results['y_test'].max()], 
                        [results['y_test'].min(), results['y_test'].max()], 'r--', lw=2)
        axes[0, 0].set_title(f'ì‹¤ì œ vs ì˜ˆì¸¡ (RÂ² = {results["r2"]:.4f})')
        axes[0, 0].set_xlabel('ì‹¤ì œ ê°€ê²©')
        axes[0, 0].set_ylabel('ì˜ˆì¸¡ ê°€ê²©')
        
        # 2. íŠ¹ì„± ì¤‘ìš”ë„
        top_features = results['feature_importance'].head(10)
        axes[0, 1].barh(range(len(top_features)), top_features['importance'])
        axes[0, 1].set_yticks(range(len(top_features)))
        axes[0, 1].set_yticklabels(top_features['feature'])
        axes[0, 1].set_title('ìƒìœ„ 10ê°œ íŠ¹ì„± ì¤‘ìš”ë„')
        axes[0, 1].set_xlabel('ì¤‘ìš”ë„')
        
        # 3. ì˜ˆì¸¡ ì˜¤ì°¨ ë¶„í¬
        residuals = results['y_test'] - results['y_pred']
        axes[1, 0].hist(residuals, bins=30, alpha=0.7, color='green', edgecolor='black')
        axes[1, 0].set_title(f'ì˜ˆì¸¡ ì˜¤ì°¨ ë¶„í¬ (RMSE = {results["rmse"]:.2f})')
        axes[1, 0].set_xlabel('ì˜¤ì°¨')
        axes[1, 0].set_ylabel('ë¹ˆë„')
        
        # 4. ì„±ëŠ¥ ë©”íŠ¸ë¦­
        metrics = ['MSE', 'RMSE', 'RÂ²']
        values = [results['mse'], results['rmse'], results['r2']]
        axes[1, 1].bar(metrics, values, color=['red', 'orange', 'green'])
        axes[1, 1].set_title('ëª¨ë¸ ì„±ëŠ¥ ë©”íŠ¸ë¦­')
        axes[1, 1].set_ylabel('ê°’')
        
        plt.tight_layout()
        ml_viz_path = SAVE_DIR / "ml_dashboard.png"
        plt.savefig(ml_viz_path, dpi=300, bbox_inches='tight')
        plt.close()
        logger.info(f"ML ì‹œê°í™” ì €ì¥: {ml_viz_path}")
        
        # ì„±ëŠ¥ ì¶œë ¥
        print("\n" + "="*50)
        print("ğŸ¤– ML ëª¨ë¸ ì„±ëŠ¥ ê²°ê³¼")
        print("="*50)
        print(f"RÂ² Score: {results['r2']:.4f}")
        print(f"RMSE: {results['rmse']:.2f}")
        print(f"MSE: {results['mse']:.2f}")
        print("\nìƒìœ„ 5ê°œ ì¤‘ìš” íŠ¹ì„±:")
        for i, row in results['feature_importance'].head().iterrows():
            print(f"  {row['feature']}: {row['importance']:.4f}")
        print("="*50)
        
    except Exception as e:
        logger.error(f"ML ì‹œê°í™” ì‹¤íŒ¨: {e}")

def main():
    if not all([KIS_APP_KEY, KIS_APP_SECRET, KIS_ACC_NO]):
        raise ValueError("KIS API í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    codes = extract_stock_codes_from_report(DETAILED_REPORT_PATH)
    api = KISAPIClient()
    results = []
    error_log = defaultdict(list)
    
    # 1. ì¼ë³„ ë°ì´í„° ìˆ˜ì§‘
    logger.info("=== 1ë‹¨ê³„: ì¼ë³„ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ===")
    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
        futures = [executor.submit(collect_one, code, api, START_DATE, END_DATE, 0, error_log) for code in codes]
        for i, f in enumerate(as_completed(futures), 1):
            result = f.result()
            logger.info(f"[{i}/{len(codes)}] {result}")
            results.append(result)
    
    # ì‹¤íŒ¨ ì¢…ëª©ë§Œ ì¬ì‹œë„ (ìµœëŒ€ 3íšŒ)
    failed = [r.split(':')[0] for r in results if ("FAIL" in r or "NO DATA" in r)]
    for retry in range(1, MAX_TOTAL_RETRY):
        if not failed:
            break
        logger.info(f"ì‹¤íŒ¨ ì¢…ëª© {len(failed)}ê°œ ì¬ì‹œë„ {retry}/{MAX_TOTAL_RETRY}")
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = [executor.submit(collect_one, code, api, START_DATE, END_DATE, retry, error_log) for code in failed]
            failed = []
            for i, f in enumerate(as_completed(futures), 1):
                result = f.result()
                logger.info(f"[ì¬ì‹œë„ {retry}] {result}")
                if ("FAIL" in result or "NO DATA" in result):
                    failed.append(result.split(':')[0])
    
    # 2. 1ë¶„ë´‰ ë°ì´í„° ìˆ˜ì§‘ (ì˜µì…˜)
    if MINUTE_DATA_ENABLED:
        logger.info("=== 2ë‹¨ê³„: 1ë¶„ë´‰ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ===")
        minute_results = []
        with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:
            futures = [executor.submit(collect_minute_data, code, api) for code in codes[:50]]  # ìƒìœ„ 50ê°œë§Œ
            for i, f in enumerate(as_completed(futures), 1):
                result = f.result()
                logger.info(f"[1ë¶„ë´‰ {i}/50] {result}")
                minute_results.append(result)
    
    # 3. í’ˆì§ˆ/ê²°ì¸¡/ì´ìƒì¹˜ ë¦¬í¬íŠ¸
    logger.info("=== 3ë‹¨ê³„: í’ˆì§ˆ ë¦¬í¬íŠ¸ ìƒì„± ===")
    quality_report()
    
    # 4. í†µí•© CSV/ML ë°ì´í„°ì…‹
    logger.info("=== 4ë‹¨ê³„: í†µí•© CSV ìƒì„± ===")
    merge_all_to_csv()
    
    # 5. ì—ëŸ¬ ìƒì„¸ ë¦¬í¬íŠ¸
    logger.info("=== 5ë‹¨ê³„: ì—ëŸ¬ ë¦¬í¬íŠ¸ ìƒì„± ===")
    error_report(error_log)
    
    # 6. ML í•™ìŠµ ìë™í™”
    logger.info("=== 6ë‹¨ê³„: ML í•™ìŠµ ìë™í™” ===")
    ml_auto_training()
    
    logger.info("ğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ! ëª¨ë“  í™•ì¥ ê¸°ëŠ¥ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 