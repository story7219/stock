#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                      ğŸš€ Smart Data Storage System v1.0                      â•‘
â•‘                     êµ¬ê¸€ì‹œíŠ¸ ê¸°ë°˜ íš¨ìœ¨ì  ë°ì´í„° ì €ì¥ì†Œ                        â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  â€¢ ğŸ“Š ì‹¤ì‹œê°„ ë°ì´í„° ìºì‹± ë° ì €ì¥                                             â•‘
â•‘  â€¢ ğŸ”„ ìŠ¤ë§ˆíŠ¸ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (í•„ìš”ì‹œë§Œ)                                      â•‘
â•‘  â€¢ ğŸ’¾ íˆìŠ¤í† ë¦¬ ë°ì´í„° ê´€ë¦¬                                                   â•‘
â•‘  â€¢ âš¡ ì´ˆê³ ì† ì¡°íšŒ ì‹œìŠ¤í…œ                                                     â•‘
â•‘  â€¢ ğŸ¯ AI ë¶„ì„ ìµœì í™” ë°ì´í„°ì…‹                                                â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import os
import json
import pandas as pd
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
import logging
from dataclasses import dataclass, asdict
from dotenv import load_dotenv
import gspread
from google.oauth2.service_account import Credentials
import asyncio
from concurrent.futures import ThreadPoolExecutor
import pickle
import hashlib
import time
from pathlib import Path

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()


@dataclass
class CacheEntry:
    """ìºì‹œ ì—”íŠ¸ë¦¬ í´ë˜ìŠ¤"""

    data: Any
    timestamp: datetime
    ttl: int  # seconds
    data_hash: str
    access_count: int = 0
    last_access: datetime = None

    def is_expired(self) -> bool:
        """ìºì‹œ ë§Œë£Œ ì—¬ë¶€ í™•ì¸"""
        return (datetime.now() - self.timestamp).total_seconds() > self.ttl

    def is_valid(self) -> bool:
        """ìºì‹œ ìœ íš¨ì„± í™•ì¸"""
        return not self.is_expired()


@dataclass
class DataRequest:
    """ë°ì´í„° ìš”ì²­ í´ë˜ìŠ¤"""

    query_type: str
    parameters: Dict[str, Any]
    priority: int = 1  # 1=high, 2=medium, 3=low
    timestamp: datetime = None

    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now()


class SmartDataStorage:
    """ìŠ¤ë§ˆíŠ¸ ë°ì´í„° ì €ì¥ì†Œ - êµ¬ê¸€ì‹œíŠ¸ ê¸°ë°˜"""

    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.logger = self._setup_logger()

        # ìºì‹œ ì„¤ì •
        self.cache_dir = Path("data_cache")
        self.cache_dir.mkdir(exist_ok=True)

        # ë©”ëª¨ë¦¬ ìºì‹œ
        self.memory_cache: Dict[str, CacheEntry] = {}
        self.max_memory_cache_size = 100  # ìµœëŒ€ 100ê°œ í•­ëª©

        # ë””ìŠ¤í¬ ìºì‹œ
        self.disk_cache_dir = self.cache_dir / "disk_cache"
        self.disk_cache_dir.mkdir(exist_ok=True)

        # êµ¬ê¸€ ì‹œíŠ¸ ì„¤ì •
        self.credentials_path = os.getenv(
            "GOOGLE_SHEETS_CREDENTIALS_PATH", "credentials.json"
        )
        self.spreadsheet_id = os.getenv("GOOGLE_SHEETS_SPREADSHEET_ID")

        # ìºì‹œ TTL ì„¤ì • (ì´ˆ)
        self.cache_ttl = {
            "stock_data": 300,  # 5ë¶„
            "analysis_results": 1800,  # 30ë¶„
            "market_data": 600,  # 10ë¶„
            "news_data": 900,  # 15ë¶„
            "historical_data": 86400,  # 24ì‹œê°„
            "dashboard_data": 60,  # 1ë¶„
        }

        # êµ¬ê¸€ ì‹œíŠ¸ í´ë¼ì´ì–¸íŠ¸
        self.sheets_client = None
        self.spreadsheet = None
        self.executor = ThreadPoolExecutor(max_workers=5)

        # í†µê³„
        self.stats = {
            "cache_hits": 0,
            "cache_misses": 0,
            "sheets_reads": 0,
            "sheets_writes": 0,
            "data_efficiency": 0.0,
        }

        self._initialize_sheets_client()
        self._load_disk_cache_index()

        self.logger.info("ğŸš€ Smart Data Storage ì´ˆê¸°í™” ì™„ë£Œ")

    def _setup_logger(self) -> logging.Logger:
        """ë¡œê±° ì„¤ì •"""
        logger = logging.getLogger("SmartDataStorage")
        logger.setLevel(logging.INFO)

        if not logger.handlers:
            # ë¡œê·¸ ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs("logs", exist_ok=True)

            # íŒŒì¼ í•¸ë“¤ëŸ¬
            file_handler = logging.FileHandler(
                "logs/smart_storage.log", encoding="utf-8"
            )
            file_formatter = logging.Formatter(
                "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
            )
            file_handler.setFormatter(file_formatter)
            logger.addHandler(file_handler)

            # ì½˜ì†” í•¸ë“¤ëŸ¬
            console_handler = logging.StreamHandler()
            console_formatter = logging.Formatter(
                "%(asctime)s - %(levelname)s - %(message)s"
            )
            console_handler.setFormatter(console_formatter)
            logger.addHandler(console_handler)

        return logger

    def _initialize_sheets_client(self):
        """êµ¬ê¸€ ì‹œíŠ¸ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        try:
            if not self.credentials_path or not os.path.exists(self.credentials_path):
                self.logger.warning("âš ï¸ êµ¬ê¸€ ì¸ì¦ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤")
                return

            if not self.spreadsheet_id:
                self.logger.warning("âš ï¸ ìŠ¤í”„ë ˆë“œì‹œíŠ¸ IDê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
                return

            # ì¸ì¦ ì„¤ì •
            scopes = [
                "https://www.googleapis.com/auth/spreadsheets",
                "https://www.googleapis.com/auth/drive",
            ]

            credentials = Credentials.from_service_account_file(
                self.credentials_path, scopes=scopes
            )

            self.sheets_client = gspread.authorize(credentials)
            self.spreadsheet = self.sheets_client.open_by_key(self.spreadsheet_id)

            self.logger.info("âœ… êµ¬ê¸€ ì‹œíŠ¸ í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")

        except Exception as e:
            self.logger.error(f"âŒ êµ¬ê¸€ ì‹œíŠ¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.sheets_client = None
            self.spreadsheet = None

    def _load_disk_cache_index(self):
        """ë””ìŠ¤í¬ ìºì‹œ ì¸ë±ìŠ¤ ë¡œë“œ"""
        try:
            index_file = self.disk_cache_dir / "cache_index.json"
            if index_file.exists():
                with open(index_file, "r", encoding="utf-8") as f:
                    self.disk_cache_index = json.load(f)
            else:
                self.disk_cache_index = {}
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë””ìŠ¤í¬ ìºì‹œ ì¸ë±ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            self.disk_cache_index = {}

    def _save_disk_cache_index(self):
        """ë””ìŠ¤í¬ ìºì‹œ ì¸ë±ìŠ¤ ì €ì¥"""
        try:
            index_file = self.disk_cache_dir / "cache_index.json"
            with open(index_file, "w", encoding="utf-8") as f:
                json.dump(self.disk_cache_index, f, ensure_ascii=False, indent=2)
        except Exception as e:
            self.logger.warning(f"âš ï¸ ë””ìŠ¤í¬ ìºì‹œ ì¸ë±ìŠ¤ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _generate_cache_key(self, data_type: str, parameters: Dict[str, Any]) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        key_data = f"{data_type}:{json.dumps(parameters, sort_keys=True)}"
        return hashlib.md5(key_data.encode()).hexdigest()

    def _calculate_data_hash(self, data: Any) -> str:
        """ë°ì´í„° í•´ì‹œ ê³„ì‚°"""
        data_str = json.dumps(data, sort_keys=True, default=str)
        return hashlib.sha256(data_str.encode()).hexdigest()

    async def get_data(
        self,
        data_type: str,
        parameters: Dict[str, Any] = None,
        force_refresh: bool = False,
    ) -> Optional[Any]:
        """ìŠ¤ë§ˆíŠ¸ ë°ì´í„° ì¡°íšŒ - ìºì‹œ ìš°ì„ , í•„ìš”ì‹œ êµ¬ê¸€ì‹œíŠ¸ì—ì„œ ì¡°íšŒ"""
        if parameters is None:
            parameters = {}

        cache_key = self._generate_cache_key(data_type, parameters)

        # ê°•ì œ ìƒˆë¡œê³ ì¹¨ì´ ì•„ë‹Œ ê²½ìš° ìºì‹œ í™•ì¸
        if not force_refresh:
            # 1. ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
            if cache_key in self.memory_cache:
                entry = self.memory_cache[cache_key]
                if entry.is_valid():
                    entry.access_count += 1
                    entry.last_access = datetime.now()
                    self.stats["cache_hits"] += 1
                    self.logger.debug(f"ğŸ’¾ ë©”ëª¨ë¦¬ ìºì‹œ íˆíŠ¸: {data_type}")
                    return entry.data
                else:
                    # ë§Œë£Œëœ ìºì‹œ ì œê±°
                    del self.memory_cache[cache_key]

            # 2. ë””ìŠ¤í¬ ìºì‹œ í™•ì¸
            disk_data = await self._load_from_disk_cache(cache_key, data_type)
            if disk_data is not None:
                # ë©”ëª¨ë¦¬ ìºì‹œì—ë„ ì €ì¥
                self._store_in_memory_cache(cache_key, disk_data, data_type)
                self.stats["cache_hits"] += 1
                self.logger.debug(f"ğŸ’¿ ë””ìŠ¤í¬ ìºì‹œ íˆíŠ¸: {data_type}")
                return disk_data

        # 3. êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ë°ì´í„° ì¡°íšŒ
        self.stats["cache_misses"] += 1
        self.logger.info(f"ğŸ” êµ¬ê¸€ì‹œíŠ¸ì—ì„œ ë°ì´í„° ì¡°íšŒ: {data_type}")

        sheet_data = await self._fetch_from_sheets(data_type, parameters)

        if sheet_data is not None:
            # ìºì‹œì— ì €ì¥
            await self._store_data(cache_key, sheet_data, data_type)
            self.stats["sheets_reads"] += 1
            return sheet_data

        return None

    async def store_data(
        self, data_type: str, data: Any, parameters: Dict[str, Any] = None
    ) -> bool:
        """ë°ì´í„° ì €ì¥ - êµ¬ê¸€ì‹œíŠ¸ + ìºì‹œ"""
        if parameters is None:
            parameters = {}

        try:
            # 1. êµ¬ê¸€ ì‹œíŠ¸ì— ì €ì¥
            success = await self._save_to_sheets(data_type, data, parameters)

            if success:
                # 2. ìºì‹œì—ë„ ì €ì¥
                cache_key = self._generate_cache_key(data_type, parameters)
                await self._store_data(cache_key, data, data_type)
                self.stats["sheets_writes"] += 1

                self.logger.info(f"âœ… ë°ì´í„° ì €ì¥ ì™„ë£Œ: {data_type}")
                return True

            return False

        except Exception as e:
            self.logger.error(f"âŒ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    async def _fetch_from_sheets(
        self, data_type: str, parameters: Dict[str, Any]
    ) -> Optional[Any]:
        """êµ¬ê¸€ ì‹œíŠ¸ì—ì„œ ë°ì´í„° ì¡°íšŒ"""
        if not self.spreadsheet:
            return None

        try:
            # ë°ì´í„° íƒ€ì…ë³„ ì‹œíŠ¸ ë§¤í•‘
            sheet_mapping = {
                "stock_data": "ì£¼ì‹ë°ì´í„°",
                "analysis_results": "AIë¶„ì„ê²°ê³¼",
                "korean_market_top5": "í•œêµ­ì‹œì¥TOP5",
                "us_market_top5": "ë¯¸êµ­ì‹œì¥TOP5",
                "strategy_summary": "ì „ëµìš”ì•½",
                "master_recommendation": "ë§ˆìŠ¤í„°ì¶”ì²œ",
                "daily_summary": "ì¼ì¼ìš”ì•½",
                "dashboard_data": "ëŒ€ì‹œë³´ë“œ",
            }

            sheet_name = sheet_mapping.get(data_type)
            if not sheet_name:
                return None

            # ë¹„ë™ê¸°ë¡œ ì‹œíŠ¸ ë°ì´í„° ì¡°íšŒ
            loop = asyncio.get_event_loop()
            worksheet = await loop.run_in_executor(
                self.executor, self.spreadsheet.worksheet, sheet_name
            )

            # íŒŒë¼ë¯¸í„°ì— ë”°ë¥¸ í•„í„°ë§
            if parameters.get("limit"):
                records = await loop.run_in_executor(
                    self.executor,
                    lambda: worksheet.get_all_records()[: parameters["limit"]],
                )
            elif parameters.get("date_from"):
                # ë‚ ì§œ ë²”ìœ„ í•„í„°ë§
                all_records = await loop.run_in_executor(
                    self.executor, worksheet.get_all_records
                )

                date_from = datetime.strptime(parameters["date_from"], "%Y-%m-%d")
                records = [
                    record
                    for record in all_records
                    if datetime.strptime(record.get("ë‚ ì§œ", "1900-01-01"), "%Y-%m-%d")
                    >= date_from
                ]
            else:
                records = await loop.run_in_executor(
                    self.executor, worksheet.get_all_records
                )

            return records

        except Exception as e:
            self.logger.error(f"âŒ êµ¬ê¸€ì‹œíŠ¸ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return None

    async def _save_to_sheets(
        self, data_type: str, data: Any, parameters: Dict[str, Any]
    ) -> bool:
        """êµ¬ê¸€ ì‹œíŠ¸ì— ë°ì´í„° ì €ì¥"""
        if not self.spreadsheet:
            return False

        try:
            # ì—¬ê¸°ì„œëŠ” ê¸°ì¡´ GoogleSheetsManagerë¥¼ í™œìš©
            from google_sheets_manager import GoogleSheetsManager

            sheets_manager = GoogleSheetsManager()

            if data_type == "stock_data":
                return await sheets_manager.save_stock_data(data)
            elif data_type == "analysis_results":
                return await sheets_manager.update_analysis_results(data)
            elif data_type == "daily_summary":
                return await sheets_manager.save_daily_summary(data)

            return True

        except Exception as e:
            self.logger.error(f"âŒ êµ¬ê¸€ì‹œíŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False

    def _store_in_memory_cache(self, cache_key: str, data: Any, data_type: str):
        """ë©”ëª¨ë¦¬ ìºì‹œì— ì €ì¥"""
        try:
            # ë©”ëª¨ë¦¬ ìºì‹œ í¬ê¸° ì œí•œ
            if len(self.memory_cache) >= self.max_memory_cache_size:
                # LRU ë°©ì‹ìœ¼ë¡œ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                oldest_key = min(
                    self.memory_cache.keys(),
                    key=lambda k: self.memory_cache[k].last_access or datetime.min,
                )
                del self.memory_cache[oldest_key]

            ttl = self.cache_ttl.get(data_type, 300)
            data_hash = self._calculate_data_hash(data)

            entry = CacheEntry(
                data=data,
                timestamp=datetime.now(),
                ttl=ttl,
                data_hash=data_hash,
                access_count=1,
                last_access=datetime.now(),
            )

            self.memory_cache[cache_key] = entry

        except Exception as e:
            self.logger.warning(f"âš ï¸ ë©”ëª¨ë¦¬ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    async def _load_from_disk_cache(
        self, cache_key: str, data_type: str
    ) -> Optional[Any]:
        """ë””ìŠ¤í¬ ìºì‹œì—ì„œ ë¡œë“œ"""
        try:
            cache_file = self.disk_cache_dir / f"{cache_key}.pkl"

            if not cache_file.exists():
                return None

            # íŒŒì¼ ìˆ˜ì • ì‹œê°„ í™•ì¸
            file_mtime = datetime.fromtimestamp(cache_file.stat().st_mtime)
            ttl = self.cache_ttl.get(data_type, 300)

            if (datetime.now() - file_mtime).total_seconds() > ttl:
                # ë§Œë£Œëœ ìºì‹œ íŒŒì¼ ì‚­ì œ
                cache_file.unlink()
                return None

            # ë°ì´í„° ë¡œë“œ
            loop = asyncio.get_event_loop()
            data = await loop.run_in_executor(
                self.executor, self._load_pickle_file, cache_file
            )

            return data

        except Exception as e:
            self.logger.warning(f"âš ï¸ ë””ìŠ¤í¬ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

    def _load_pickle_file(self, file_path: Path) -> Any:
        """í”¼í´ íŒŒì¼ ë¡œë“œ (ë™ê¸° í•¨ìˆ˜)"""
        with open(file_path, "rb") as f:
            return pickle.load(f)

    async def _store_data(self, cache_key: str, data: Any, data_type: str):
        """ìºì‹œì— ë°ì´í„° ì €ì¥ (ë©”ëª¨ë¦¬ + ë””ìŠ¤í¬)"""
        # ë©”ëª¨ë¦¬ ìºì‹œ
        self._store_in_memory_cache(cache_key, data, data_type)

        # ë””ìŠ¤í¬ ìºì‹œ
        await self._store_in_disk_cache(cache_key, data, data_type)

    async def _store_in_disk_cache(self, cache_key: str, data: Any, data_type: str):
        """ë””ìŠ¤í¬ ìºì‹œì— ì €ì¥"""
        try:
            cache_file = self.disk_cache_dir / f"{cache_key}.pkl"

            loop = asyncio.get_event_loop()
            await loop.run_in_executor(
                self.executor, self._save_pickle_file, cache_file, data
            )

            # ì¸ë±ìŠ¤ ì—…ë°ì´íŠ¸
            self.disk_cache_index[cache_key] = {
                "data_type": data_type,
                "timestamp": datetime.now().isoformat(),
                "file_path": str(cache_file),
            }

            self._save_disk_cache_index()

        except Exception as e:
            self.logger.warning(f"âš ï¸ ë””ìŠ¤í¬ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")

    def _save_pickle_file(self, file_path: Path, data: Any):
        """í”¼í´ íŒŒì¼ ì €ì¥ (ë™ê¸° í•¨ìˆ˜)"""
        with open(file_path, "wb") as f:
            pickle.dump(data, f)

    async def get_latest_stock_data(
        self, symbols: List[str] = None, limit: int = 100
    ) -> List[Dict[str, Any]]:
        """ìµœì‹  ì£¼ì‹ ë°ì´í„° ì¡°íšŒ"""
        parameters = {"limit": limit}
        if symbols:
            parameters["symbols"] = symbols

        return await self.get_data("stock_data", parameters)

    async def get_analysis_results(
        self, date_from: str = None, limit: int = 50
    ) -> List[Dict[str, Any]]:
        """AI ë¶„ì„ ê²°ê³¼ ì¡°íšŒ"""
        parameters = {"limit": limit}
        if date_from:
            parameters["date_from"] = date_from

        return await self.get_data("analysis_results", parameters)

    async def get_market_top5(self, market: str = "korean") -> List[Dict[str, Any]]:
        """ì‹œì¥ë³„ Top5 ì¡°íšŒ"""
        data_type = f"{market}_market_top5"
        return await self.get_data(data_type, {"limit": 5})

    async def get_dashboard_data(self) -> Dict[str, Any]:
        """ëŒ€ì‹œë³´ë“œ ë°ì´í„° ì¡°íšŒ"""
        return await self.get_data("dashboard_data", {})

    def get_cache_stats(self) -> Dict[str, Any]:
        """ìºì‹œ í†µê³„ ì¡°íšŒ"""
        total_requests = self.stats["cache_hits"] + self.stats["cache_misses"]
        hit_rate = (
            (self.stats["cache_hits"] / total_requests * 100)
            if total_requests > 0
            else 0
        )

        return {
            "cache_hits": self.stats["cache_hits"],
            "cache_misses": self.stats["cache_misses"],
            "hit_rate": f"{hit_rate:.1f}%",
            "sheets_reads": self.stats["sheets_reads"],
            "sheets_writes": self.stats["sheets_writes"],
            "memory_cache_size": len(self.memory_cache),
            "disk_cache_size": len(self.disk_cache_index),
            "efficiency_score": hit_rate,
        }

    async def cleanup_cache(self, max_age_hours: int = 24):
        """ìºì‹œ ì •ë¦¬"""
        try:
            cutoff_time = datetime.now() - timedelta(hours=max_age_hours)

            # ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬
            expired_keys = [
                key
                for key, entry in self.memory_cache.items()
                if entry.timestamp < cutoff_time
            ]

            for key in expired_keys:
                del self.memory_cache[key]

            # ë””ìŠ¤í¬ ìºì‹œ ì •ë¦¬
            expired_files = []
            for cache_key, info in self.disk_cache_index.items():
                timestamp = datetime.fromisoformat(info["timestamp"])
                if timestamp < cutoff_time:
                    file_path = Path(info["file_path"])
                    if file_path.exists():
                        file_path.unlink()
                    expired_files.append(cache_key)

            for key in expired_files:
                del self.disk_cache_index[key]

            self._save_disk_cache_index()

            self.logger.info(
                f"ğŸ§¹ ìºì‹œ ì •ë¦¬ ì™„ë£Œ: ë©”ëª¨ë¦¬ {len(expired_keys)}ê°œ, ë””ìŠ¤í¬ {len(expired_files)}ê°œ ì‚­ì œ"
            )

        except Exception as e:
            self.logger.error(f"âŒ ìºì‹œ ì •ë¦¬ ì‹¤íŒ¨: {e}")

    async def prefetch_data(self, data_requests: List[DataRequest]):
        """ë°ì´í„° ë¯¸ë¦¬ ê°€ì ¸ì˜¤ê¸° (ë°±ê·¸ë¼ìš´ë“œ)"""
        try:
            # ìš°ì„ ìˆœìœ„ë³„ ì •ë ¬
            sorted_requests = sorted(data_requests, key=lambda x: x.priority)

            tasks = []
            for request in sorted_requests:
                task = self.get_data(request.query_type, request.parameters)
                tasks.append(task)

            # ë³‘ë ¬ ì‹¤í–‰
            await asyncio.gather(*tasks, return_exceptions=True)

            self.logger.info(f"ğŸš€ ë°ì´í„° í”„ë¦¬í˜ì¹˜ ì™„ë£Œ: {len(data_requests)}ê°œ ìš”ì²­")

        except Exception as e:
            self.logger.error(f"âŒ ë°ì´í„° í”„ë¦¬í˜ì¹˜ ì‹¤íŒ¨: {e}")

    async def close(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            # ìŠ¤ë ˆë“œí’€ ì¢…ë£Œ
            self.executor.shutdown(wait=True)

            # ìºì‹œ ì¸ë±ìŠ¤ ì €ì¥
            self._save_disk_cache_index()

            self.logger.info("âœ… Smart Data Storage ì¢…ë£Œ ì™„ë£Œ")

        except Exception as e:
            self.logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}")


# ì‚¬ìš© ì˜ˆì‹œ
async def test_smart_storage():
    """ìŠ¤ë§ˆíŠ¸ ë°ì´í„° ì €ì¥ì†Œ í…ŒìŠ¤íŠ¸"""
    storage = SmartDataStorage()

    try:
        # 1. ìµœì‹  ì£¼ì‹ ë°ì´í„° ì¡°íšŒ
        print("ğŸ“Š ìµœì‹  ì£¼ì‹ ë°ì´í„° ì¡°íšŒ...")
        stock_data = await storage.get_latest_stock_data(limit=10)
        print(f"ì¡°íšŒ ê²°ê³¼: {len(stock_data) if stock_data else 0}ê°œ")

        # 2. AI ë¶„ì„ ê²°ê³¼ ì¡°íšŒ
        print("ğŸ¤– AI ë¶„ì„ ê²°ê³¼ ì¡°íšŒ...")
        analysis_data = await storage.get_analysis_results(limit=5)
        print(f"ë¶„ì„ ê²°ê³¼: {len(analysis_data) if analysis_data else 0}ê°œ")

        # 3. ìºì‹œ í†µê³„ í™•ì¸
        print("ğŸ“ˆ ìºì‹œ í†µê³„:")
        stats = storage.get_cache_stats()
        for key, value in stats.items():
            print(f"  {key}: {value}")

        # 4. ë°ì´í„° í”„ë¦¬í˜ì¹˜ í…ŒìŠ¤íŠ¸
        print("ğŸš€ ë°ì´í„° í”„ë¦¬í˜ì¹˜ í…ŒìŠ¤íŠ¸...")
        prefetch_requests = [
            DataRequest("korean_market_top5", {}, priority=1),
            DataRequest("us_market_top5", {}, priority=1),
            DataRequest("dashboard_data", {}, priority=2),
        ]
        await storage.prefetch_data(prefetch_requests)

        print("âœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")

    finally:
        await storage.close()


if __name__ == "__main__":
    asyncio.run(test_smart_storage())
