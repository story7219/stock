#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ“° ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘ ëª¨ë“ˆ v2.0
í•œêµ­ ë° ê¸€ë¡œë²Œ ê¸ˆìœµ ë‰´ìŠ¤ ì‹¤ì‹œê°„ ìˆ˜ì§‘
"""

import asyncio
import aiohttp
import feedparser
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
from dataclasses import dataclass, asdict
import re
from urllib.parse import urljoin, urlparse
import hashlib
import time
from bs4 import BeautifulSoup
import requests
from textblob import TextBlob
import yfinance as yf
import os

logger = logging.getLogger(__name__)

@dataclass
class NewsItem:
    """ë‰´ìŠ¤ ì•„ì´í…œ ë°ì´í„° í´ë˜ìŠ¤"""
    title: str
    content: str
    url: str
    source: str
    published_date: datetime
    symbols: List[str]
    sentiment_score: float
    importance_score: float
    category: str
    language: str
    hash_id: str

class NewsCollector:
    """ğŸ”¥ ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        """ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”"""
        logger.info("ğŸ“° ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™”")
        
        # í•œêµ­ ë‰´ìŠ¤ ì†ŒìŠ¤
        self.korean_sources = {
            'naver_finance': 'https://finance.naver.com/news/news_list.naver?mode=LSS2D&section_id=101&section_id2=258',
            'hankyung': 'https://www.hankyung.com/feed/economy',
            'maeil_kyung': 'https://www.mk.co.kr/rss/30000001/',
            'yonhap_finance': 'https://www.yna.co.kr/rss/economy.xml',
            'korea_joongang': 'https://rss.joins.com/joins_economy_list.xml',
            'chosun_biz': 'http://biz.chosun.com/rss/economy.xml',
            'seoul_finance': 'https://www.sedaily.com/RSSFeed.xml?DCode=101',
            'fn_news': 'http://www.fnnews.com/rss/fn_realestate_stock.xml'
        }
        
        # ê¸€ë¡œë²Œ ë‰´ìŠ¤ ì†ŒìŠ¤
        self.global_sources = {
            'reuters_business': 'https://www.reuters.com/business/finance/rss',
            'bloomberg': 'https://feeds.bloomberg.com/markets/news.rss',
            'cnbc_markets': 'https://www.cnbc.com/id/10000664/device/rss/rss.html',
            'marketwatch': 'http://feeds.marketwatch.com/marketwatch/marketpulse/',
            'yahoo_finance': 'https://finance.yahoo.com/news/rssindex',
            'seeking_alpha': 'https://seekingalpha.com/market-news.xml',
            'financial_times': 'https://www.ft.com/companies?format=rss',
            'wsj_markets': 'https://feeds.a.dj.com/rss/RSSMarketsMain.xml'
        }
        
        # ìºì‹œ ì„¤ì •
        self.news_cache = {}
        self.cache_duration = 300  # 5ë¶„
        self.duplicate_checker = set()
        
        # ì„¸ì…˜ ì„¤ì •
        self.session = None
        
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            headers={
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
            }
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        if self.session:
            await self.session.close()
    
    async def collect_all_news(self, symbols: List[str] = None) -> List[NewsItem]:
        """ëª¨ë“  ë‰´ìŠ¤ ìˆ˜ì§‘"""
        logger.info("ğŸ”¥ ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
        
        all_news = []
        
        # í•œêµ­ ë‰´ìŠ¤ ìˆ˜ì§‘
        korean_news = await self.collect_korean_market_news(symbols)
        all_news.extend(korean_news)
        
        # ê¸€ë¡œë²Œ ë‰´ìŠ¤ ìˆ˜ì§‘
        global_news = await self.collect_global_market_news(symbols)
        all_news.extend(global_news)
        
        # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
        unique_news = self._remove_duplicates(all_news)
        sorted_news = sorted(unique_news, key=lambda x: x.published_date, reverse=True)
        
        logger.info(f"âœ… ì´ {len(sorted_news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
        return sorted_news[:100]  # ìµœì‹  100ê°œë§Œ ë°˜í™˜
    
    async def collect_korean_market_news(self, symbols: List[str] = None) -> List[NewsItem]:
        """í•œêµ­ ì‹œì¥ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        logger.info("ğŸ‡°ğŸ‡· í•œêµ­ ì‹œì¥ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
        
        news_items = []
        
        # RSS í”¼ë“œ ìˆ˜ì§‘
        for source_name, rss_url in self.korean_sources.items():
            try:
                items = await self._collect_rss_news(source_name, rss_url, 'ko')
                news_items.extend(items)
                logger.info(f"âœ… {source_name}: {len(items)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
            except Exception as e:
                logger.error(f"âŒ {source_name} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        # ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘
        try:
            naver_news = await self._scrape_naver_finance_news()
            news_items.extend(naver_news)
            logger.info(f"âœ… ë„¤ì´ë²„ ê¸ˆìœµ: {len(naver_news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
        except Exception as e:
            logger.error(f"âŒ ë„¤ì´ë²„ ê¸ˆìœµ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
        
        # ì¢…ëª©ë³„ í•„í„°ë§
        if symbols:
            news_items = self._filter_by_symbols(news_items, symbols)
        
        logger.info(f"ğŸ‡°ğŸ‡· í•œêµ­ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {len(news_items)}ê°œ")
        return news_items
    
    async def collect_global_market_news(self, symbols: List[str] = None) -> List[NewsItem]:
        """ê¸€ë¡œë²Œ ì‹œì¥ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        logger.info("ğŸŒ ê¸€ë¡œë²Œ ì‹œì¥ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
        
        news_items = []
        
        # RSS í”¼ë“œ ìˆ˜ì§‘
        for source_name, rss_url in self.global_sources.items():
            try:
                items = await self._collect_rss_news(source_name, rss_url, 'en')
                news_items.extend(items)
                logger.info(f"âœ… {source_name}: {len(items)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
            except Exception as e:
                logger.error(f"âŒ {source_name} ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        # Yahoo Finance ë‰´ìŠ¤ API ì‚¬ìš©
        try:
            yahoo_news = await self._collect_yahoo_finance_news(symbols)
            news_items.extend(yahoo_news)
            logger.info(f"âœ… Yahoo Finance API: {len(yahoo_news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘")
        except Exception as e:
            logger.error(f"âŒ Yahoo Finance API ì‹¤íŒ¨: {e}")
        
        # ì¢…ëª©ë³„ í•„í„°ë§
        if symbols:
            news_items = self._filter_by_symbols(news_items, symbols)
        
        logger.info(f"ğŸŒ ê¸€ë¡œë²Œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {len(news_items)}ê°œ")
        return news_items
    
    async def _collect_rss_news(self, source_name: str, rss_url: str, language: str) -> List[NewsItem]:
        """RSS í”¼ë“œì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        try:
            if self.session is None:
                async with aiohttp.ClientSession() as session:
                    async with session.get(rss_url) as response:
                        content = await response.text()
            else:
                async with self.session.get(rss_url) as response:
                    content = await response.text()
            
            feed = feedparser.parse(content)
            news_items = []
            
            for entry in feed.entries[:20]:  # ìµœì‹  20ê°œë§Œ
                try:
                    # ë‚ ì§œ íŒŒì‹±
                    if hasattr(entry, 'published_parsed') and entry.published_parsed:
                        pub_date = datetime(*entry.published_parsed[:6])
                    else:
                        pub_date = datetime.now()
                    
                    # 24ì‹œê°„ ì´ë‚´ ë‰´ìŠ¤ë§Œ
                    if (datetime.now() - pub_date).days > 1:
                        continue
                    
                    # ë‰´ìŠ¤ ì•„ì´í…œ ìƒì„±
                    news_item = NewsItem(
                        title=entry.title,
                        content=self._extract_content(entry),
                        url=entry.link,
                        source=source_name,
                        published_date=pub_date,
                        symbols=self._extract_symbols(entry.title + " " + self._extract_content(entry)),
                        sentiment_score=self._calculate_sentiment(entry.title + " " + self._extract_content(entry)),
                        importance_score=self._calculate_importance(entry.title),
                        category=self._categorize_news(entry.title),
                        language=language,
                        hash_id=self._generate_hash(entry.title + entry.link)
                    )
                    
                    news_items.append(news_item)
                    
                except Exception as e:
                    logger.warning(f"RSS ì—”íŠ¸ë¦¬ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            return news_items
            
        except Exception as e:
            logger.error(f"RSS ìˆ˜ì§‘ ì‹¤íŒ¨ {source_name}: {e}")
            return []
    
    async def _scrape_naver_finance_news(self) -> List[NewsItem]:
        """ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ìŠ¤í¬ë˜í•‘"""
        try:
            url = "https://finance.naver.com/news/news_list.naver?mode=LSS2D&section_id=101&section_id2=258"
            
            if self.session:
                async with self.session.get(url) as response:
                    html = await response.text()
            else:
                async with aiohttp.ClientSession() as session:
                    async with session.get(url) as response:
                        html = await response.text()
            
            soup = BeautifulSoup(html, 'html.parser')
            news_items = []
            
            # ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ íŒŒì‹±
            news_list = soup.find_all('tr', class_='')
            
            for news in news_list[:10]:  # ìµœì‹  10ê°œë§Œ
                try:
                    title_element = news.find('a', class_='tit')
                    if not title_element:
                        continue
                    
                    title = title_element.text.strip()
                    link = 'https://finance.naver.com' + title_element['href']
                    
                    # ìƒì„¸ í˜ì´ì§€ì—ì„œ ë‚´ìš© ìˆ˜ì§‘
                    content = await self._scrape_news_content(link)
                    
                    news_item = NewsItem(
                        title=title,
                        content=content,
                        url=link,
                        source='naver_finance_scrape',
                        published_date=datetime.now(),
                        symbols=self._extract_symbols(title + " " + content),
                        sentiment_score=self._calculate_sentiment(title + " " + content),
                        importance_score=self._calculate_importance(title),
                        category=self._categorize_news(title),
                        language='ko',
                        hash_id=self._generate_hash(title + link)
                    )
                    
                    news_items.append(news_item)
                    
                except Exception as e:
                    logger.warning(f"ë„¤ì´ë²„ ë‰´ìŠ¤ íŒŒì‹± ì‹¤íŒ¨: {e}")
                    continue
            
            return news_items
            
        except Exception as e:
            logger.error(f"ë„¤ì´ë²„ ê¸ˆìœµ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return []
    
    async def _scrape_news_content(self, url: str) -> str:
        """ë‰´ìŠ¤ ìƒì„¸ ë‚´ìš© ìŠ¤í¬ë˜í•‘"""
        try:
            if self.session:
                async with self.session.get(url) as response:
                    html = await response.text()
            else:
                async with aiohttp.ClientSession() as session:
                    async with session.get(url) as response:
                        html = await response.text()
            
            soup = BeautifulSoup(html, 'html.parser')
            
            # ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ë³¸ë¬¸ ì¶”ì¶œ
            content_div = soup.find('div', class_='scr01') or soup.find('div', id='news_read')
            if content_div:
                return content_div.get_text().strip()[:500]  # 500ì ì œí•œ
            
            return ""
            
        except Exception as e:
            logger.warning(f"ë‰´ìŠ¤ ë‚´ìš© ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
            return ""
    
    async def _collect_yahoo_finance_news(self, symbols: List[str] = None) -> List[NewsItem]:
        """Yahoo Finance APIë¡œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        news_items = []
        
        if not symbols:
            # ì£¼ìš” ì§€ìˆ˜ ë‰´ìŠ¤
            symbols = ['^GSPC', '^IXIC', '^DJI', 'AAPL', 'MSFT', 'GOOGL', 'AMZN', 'TSLA']
        
        for symbol in symbols[:10]:  # ìµœëŒ€ 10ê°œ ì¢…ëª©
            try:
                ticker = yf.Ticker(symbol)
                news = ticker.news
                
                for item in news[:5]:  # ì¢…ëª©ë‹¹ 5ê°œì”©
                    try:
                        news_item = NewsItem(
                            title=item.get('title', ''),
                            content=item.get('summary', '')[:500],
                            url=item.get('link', ''),
                            source='yahoo_finance_api',
                            published_date=datetime.fromtimestamp(item.get('providerPublishTime', time.time())),
                            symbols=[symbol],
                            sentiment_score=self._calculate_sentiment(item.get('title', '') + " " + item.get('summary', '')),
                            importance_score=self._calculate_importance(item.get('title', '')),
                            category=self._categorize_news(item.get('title', '')),
                            language='en',
                            hash_id=self._generate_hash(item.get('title', '') + item.get('link', ''))
                        )
                        
                        news_items.append(news_item)
                        
                    except Exception as e:
                        logger.warning(f"Yahoo ë‰´ìŠ¤ ì•„ì´í…œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                        continue
                
            except Exception as e:
                logger.warning(f"Yahoo Finance {symbol} ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
                continue
        
        return news_items
    
    def _extract_content(self, entry) -> str:
        """RSS ì—”íŠ¸ë¦¬ì—ì„œ ë‚´ìš© ì¶”ì¶œ"""
        content = ""
        
        if hasattr(entry, 'summary'):
            content = entry.summary
        elif hasattr(entry, 'description'):
            content = entry.description
        elif hasattr(entry, 'content'):
            if isinstance(entry.content, list) and len(entry.content) > 0:
                content = entry.content[0].value
            else:
                content = str(entry.content)
        
        # HTML íƒœê·¸ ì œê±°
        if content:
            soup = BeautifulSoup(content, 'html.parser')
            content = soup.get_text().strip()
        
        return content[:500]  # 500ì ì œí•œ
    
    def _extract_symbols(self, text: str) -> List[str]:
        """í…ìŠ¤íŠ¸ì—ì„œ ì¢…ëª© ì‹¬ë³¼ ì¶”ì¶œ"""
        symbols = []
        
        # ë¯¸êµ­ ì£¼ì‹ ì‹¬ë³¼ íŒ¨í„´ (3-5ê¸€ì ëŒ€ë¬¸ì)
        us_pattern = r'\b[A-Z]{3,5}\b'
        us_matches = re.findall(us_pattern, text.upper())
        
        # í•œêµ­ ì¢…ëª©ëª… íŒ¨í„´ (íšŒì‚¬ëª… + ê´€ë ¨ í‚¤ì›Œë“œ)
        korean_companies = [
            'ì‚¼ì„±ì „ì', 'ì‚¼ì„±SDI', 'SKí•˜ì´ë‹‰ìŠ¤', 'NAVER', 'ì¹´ì¹´ì˜¤', 'LGí™”í•™', 'LGì—ë„ˆì§€ì†”ë£¨ì…˜',
            'í˜„ëŒ€ì°¨', 'ê¸°ì•„', 'POSCO', 'í•œêµ­ì „ë ¥', 'KBê¸ˆìœµ', 'ì‹ í•œì§€ì£¼', 'í•˜ë‚˜ê¸ˆìœµì§€ì£¼',
            'ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤', 'ì…€íŠ¸ë¦¬ì˜¨', 'í˜„ëŒ€ëª¨ë¹„ìŠ¤', 'ì‚¼ì„±ë¬¼ì‚°', 'LGì „ì', 'SKí…”ë ˆì½¤'
        ]
        
        for company in korean_companies:
            if company in text:
                symbols.append(company)
        
        # í•„í„°ë§ (ì¼ë°˜ì ì¸ ë‹¨ì–´ ì œì™¸)
        filtered_symbols = []
        exclude_words = {'THE', 'AND', 'FOR', 'ARE', 'BUT', 'NOT', 'YOU', 'ALL', 'CAN', 'HER', 'WAS', 'ONE', 'OUR', 'HAD', 'BY'}
        
        for symbol in us_matches:
            if symbol not in exclude_words:
                filtered_symbols.append(symbol)
        
        filtered_symbols.extend(symbols)
        return list(set(filtered_symbols))[:5]  # ìµœëŒ€ 5ê°œ
    
    def _calculate_sentiment(self, text: str) -> float:
        """ê°ì • ë¶„ì„ ì ìˆ˜ ê³„ì‚°"""
        try:
            # ì˜ì–´ í…ìŠ¤íŠ¸ ê°ì • ë¶„ì„
            blob = TextBlob(text)
            sentiment = blob.sentiment.polarity
            
            # í•œêµ­ì–´ í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì • ë¶„ì„
            positive_keywords = ['ìƒìŠ¹', 'ê¸‰ë“±', 'í˜¸ì¬', 'ê¸ì •', 'ì„±ì¥', 'ì¦ê°€', 'ê°œì„ ', 'ê°•ì„¸', 'ëŒíŒŒ', 'ìƒí–¥']
            negative_keywords = ['í•˜ë½', 'ê¸‰ë½', 'ì•…ì¬', 'ë¶€ì •', 'ê°ì†Œ', 'ìš°ë ¤', 'ìœ„í—˜', 'ì•½ì„¸', 'í•˜í–¥', 'ì¡°ì •']
            
            korean_score = 0
            for word in positive_keywords:
                korean_score += text.count(word) * 0.1
            for word in negative_keywords:
                korean_score -= text.count(word) * 0.1
            
            # ìµœì¢… ì ìˆ˜ (-1 ~ 1)
            final_score = (sentiment + korean_score) / 2
            return max(-1, min(1, final_score))
            
        except Exception as e:
            logger.warning(f"ê°ì • ë¶„ì„ ì‹¤íŒ¨: {e}")
            return 0.0
    
    def _calculate_importance(self, title: str) -> float:
        """ë‰´ìŠ¤ ì¤‘ìš”ë„ ê³„ì‚°"""
        importance = 0.5  # ê¸°ë³¸ê°’
        
        # ì¤‘ìš” í‚¤ì›Œë“œ
        high_importance = ['ì‹¤ì ', 'ë¶„í• ', 'í•©ë³‘', 'IPO', 'ìƒì¥', 'ì¦ì', 'ë°°ë‹¹', 'ê³µì‹œ']
        medium_importance = ['íˆ¬ì', 'í˜‘ë ¥', 'ê³„ì•½', 'ì¶œì‹œ', 'ë°œí‘œ']
        
        title_upper = title.upper()
        
        for keyword in high_importance:
            if keyword in title or keyword.upper() in title_upper:
                importance += 0.3
        
        for keyword in medium_importance:
            if keyword in title or keyword.upper() in title_upper:
                importance += 0.2
        
        return min(1.0, importance)
    
    def _categorize_news(self, title: str) -> str:
        """ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        title_lower = title.lower()
        
        if any(word in title_lower for word in ['ì‹¤ì ', 'earnings', 'ë§¤ì¶œ', 'revenue']):
            return 'EARNINGS'
        elif any(word in title_lower for word in ['ì¸ìˆ˜', 'í•©ë³‘', 'merger', 'acquisition']):
            return 'M&A'
        elif any(word in title_lower for word in ['ì‹ ì œí’ˆ', 'ì¶œì‹œ', 'launch', 'product']):
            return 'PRODUCT'
        elif any(word in title_lower for word in ['ê·œì œ', 'regulation', 'ì •ì±…', 'policy']):
            return 'REGULATION'
        elif any(word in title_lower for word in ['íˆ¬ì', 'investment', 'ìê¸ˆ', 'funding']):
            return 'INVESTMENT'
        else:
            return 'GENERAL'
    
    def _generate_hash(self, text: str) -> str:
        """í…ìŠ¤íŠ¸ í•´ì‹œ ìƒì„±"""
        return hashlib.md5(text.encode()).hexdigest()
    
    def _filter_by_symbols(self, news_items: List[NewsItem], symbols: List[str]) -> List[NewsItem]:
        """ì¢…ëª©ë³„ ë‰´ìŠ¤ í•„í„°ë§"""
        filtered = []
        
        for news in news_items:
            if any(symbol in news.symbols for symbol in symbols):
                filtered.append(news)
            elif any(symbol.lower() in news.title.lower() or symbol.lower() in news.content.lower() for symbol in symbols):
                news.symbols.extend([s for s in symbols if s.lower() in (news.title + news.content).lower()])
                filtered.append(news)
        
        return filtered
    
    def _remove_duplicates(self, news_items: List[NewsItem]) -> List[NewsItem]:
        """ì¤‘ë³µ ë‰´ìŠ¤ ì œê±°"""
        seen_hashes = set()
        unique_news = []
        
        for news in news_items:
            if news.hash_id not in seen_hashes:
                seen_hashes.add(news.hash_id)
                unique_news.append(news)
        
        return unique_news
    
    def get_news_summary(self, symbols: List[str] = None, hours: int = 24) -> Dict[str, Any]:
        """ë‰´ìŠ¤ ìš”ì•½ ì •ë³´"""
        # ìºì‹œëœ ë‰´ìŠ¤ì—ì„œ ìš”ì•½ ìƒì„±
        cutoff_time = datetime.now() - timedelta(hours=hours)
        
        recent_news = []
        for news_list in self.news_cache.values():
            for news in news_list:
                if news.published_date >= cutoff_time:
                    if not symbols or any(symbol in news.symbols for symbol in symbols):
                        recent_news.append(news)
        
        if not recent_news:
            return {
                'total_news': 0,
                'avg_sentiment': 0.0,
                'top_categories': [],
                'summary': 'ìµœê·¼ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.'
            }
        
        # í†µê³„ ê³„ì‚°
        total_news = len(recent_news)
        avg_sentiment = sum(news.sentiment_score for news in recent_news) / total_news
        
        # ì¹´í…Œê³ ë¦¬ë³„ ì§‘ê³„
        categories = {}
        for news in recent_news:
            categories[news.category] = categories.get(news.category, 0) + 1
        
        top_categories = sorted(categories.items(), key=lambda x: x[1], reverse=True)[:3]
        
        return {
            'total_news': total_news,
            'avg_sentiment': round(avg_sentiment, 3),
            'top_categories': top_categories,
            'summary': f'ìµœê·¼ {hours}ì‹œê°„ ë™ì•ˆ {total_news}ê°œ ë‰´ìŠ¤, í‰ê·  ê°ì •ì ìˆ˜: {avg_sentiment:.2f}'
        }

    async def collect_news(self, symbol: str, limit: int = 5) -> List[Dict[str, Any]]:
        """ë‰´ìŠ¤ ìˆ˜ì§‘"""
        try:
            logger.info(f"{symbol} ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
            
            # Mock ëª¨ë“œ ë˜ëŠ” ì‹¤ì œ ë‰´ìŠ¤ ìˆ˜ì§‘
            if os.getenv('IS_MOCK', 'false').lower() == 'true':
                return self._generate_mock_news(symbol, limit)
            
            # ì‹¤ì œ ë‰´ìŠ¤ ìˆ˜ì§‘ ë¡œì§
            news_data = await self._collect_real_news(symbol, limit)
            
            logger.info(f"{symbol} ë‰´ìŠ¤ {len(news_data)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            return news_data
            
        except Exception as e:
            logger.error(f"{symbol} ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return self._generate_mock_news(symbol, limit)
    
    def _generate_mock_news(self, symbol: str, limit: int) -> List[Dict[str, Any]]:
        """Mock ë‰´ìŠ¤ ìƒì„±"""
        mock_news = []
        for i in range(limit):
            mock_news.append({
                'title': f'{symbol} ê´€ë ¨ ë‰´ìŠ¤ {i+1}',
                'content': f'{symbol}ì— ëŒ€í•œ ìµœì‹  ì‹œì¥ ë™í–¥ ë¶„ì„',
                'source': 'Mock News',
                'timestamp': datetime.now().isoformat(),
                'sentiment': 'positive' if i % 2 == 0 else 'neutral'
            })
        return mock_news
    
    async def _collect_real_news(self, symbol: str, limit: int) -> List[Dict[str, Any]]:
        """ì‹¤ì œ ë‰´ìŠ¤ ìˆ˜ì§‘"""
        # ì‹¤ì œ ë‰´ìŠ¤ ìˆ˜ì§‘ ë¡œì§ êµ¬í˜„
        return self._generate_mock_news(symbol, limit)

# ì‚¬ìš© ì˜ˆì‹œ
async def main():
    """ë‰´ìŠ¤ ìˆ˜ì§‘ í…ŒìŠ¤íŠ¸"""
    async with NewsCollector() as collector:
        # ì „ì²´ ë‰´ìŠ¤ ìˆ˜ì§‘
        all_news = await collector.collect_all_news(['AAPL', 'MSFT', 'ì‚¼ì„±ì „ì'])
        
        print(f"ìˆ˜ì§‘ëœ ë‰´ìŠ¤: {len(all_news)}ê°œ")
        
        for news in all_news[:5]:
            print(f"\nì œëª©: {news.title}")
            print(f"ì†ŒìŠ¤: {news.source}")
            print(f"ê°ì •ì ìˆ˜: {news.sentiment_score}")
            print(f"ì¤‘ìš”ë„: {news.importance_score}")
            print(f"ê´€ë ¨ì¢…ëª©: {news.symbols}")

if __name__ == "__main__":
    asyncio.run(main()) 