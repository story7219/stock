#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íŒŒì¼ëª…: extreme_data_pipeline.py
ëª¨ë“ˆ: 20ë…„ ê³¼ê±°ë°ì´í„° 100% í™œìš© ê·¹í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸
ëª©ì : ëª¨ë“  ê°€ìš© ë°ì´í„°ë¥¼ ì™„ì „íˆ í™œìš©í•˜ëŠ” ê·¹í•œ ìµœì í™” ì‹œìŠ¤í…œ

Author: World-Class AI System
Created: 2025-01-27
Version: 1.0.0

ë°ì´í„° ì†ŒìŠ¤:
- KRX: 20ë…„ ì¼ë´‰/ë¶„ë´‰ ë°ì´í„° (2800+ ì¢…ëª©)
- DART: 15ë…„ ì¬ë¬´ì œí‘œ/ê³µì‹œ ë°ì´í„° (1000+ ê¸°ì—…)
- ë‰´ìŠ¤: 10ë…„ ê¸ˆìœµë‰´ìŠ¤ (1M+ ê¸°ì‚¬)
- ê¸€ë¡œë²Œ: Yahoo Finance 20ë…„ ë°ì´í„°
- ê±°ì‹œê²½ì œ: BOK 20ë…„ ê²½ì œì§€í‘œ

ëª©í‘œ:
- ë°ì´í„° í™œìš©ë¥ : 100%
- ì²˜ë¦¬ ì†ë„: 1M+ ë ˆì½”ë“œ/ì´ˆ
- ë©”ëª¨ë¦¬ íš¨ìœ¨: 32GB ì™„ì „ í™œìš©
- ì‹¤ì‹œê°„ í†µí•©: < 100ms ì§€ì—°
- í”¼ì²˜ ìƒì„±: 10,000+ í”¼ì²˜

License: MIT
"""

from __future__ import annotations
import asyncio
import gc
import logging
import multiprocessing as mp
import os
import time
import warnings
from concurrent.futures import ProcessPoolExecutor
import ThreadPoolExecutor, as_completed
from contextlib import asynccontextmanager
from dataclasses import dataclass
import field
from datetime import datetime
import timedelta
from pathlib import Path
from typing import Any
import Dict, List, Optional, Tuple, Union, Callable, Iterator
import threading
import queue
import weakref

import numpy as np
import pandas as pd
import pyarrow as pa
import pyarrow.parquet as pq
import dask.dataframe as dd
from dask.distributed import Client
import as_completed as dask_as_completed
import h5py
import sqlite3
from sqlalchemy import create_engine
import text
import redis
import pickle
import joblib
from memory_profiler import profile
import psutil

# ê¸°ìˆ ì  ë¶„ì„
import talib
import pandas_ta as ta

# ìì—°ì–´ ì²˜ë¦¬
from transformers import pipeline
import AutoTokenizer, AutoModel
import torch

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/extreme_data_pipeline.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

@dataclass
class ExtremeDataConfig:
    """ê·¹í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì„¤ì •"""
    # ë°ì´í„° ê²½ë¡œ
    base_data_path: str = "./data"
    backup_data_path: str = "./data_backup"
    cache_path: str = "./cache"
    processed_data_path: str = "./processed_data"

    # ì‹œê°„ ë²”ìœ„
    start_date: str = "2000-01-01"  # 25ë…„ ë°ì´í„°
    end_date: str = datetime.now().strftime("%Y-%m-%d")

    # ì²˜ë¦¬ ì„¤ì •
    chunk_size: int = 100000  # ì²­í¬ í¬ê¸°
    max_workers: int = mp.cpu_count()  # ìµœëŒ€ ì›Œì»¤ ìˆ˜
    memory_limit_gb: int = 30  # ë©”ëª¨ë¦¬ ì œí•œ (32GB ì¤‘ 30GB)

    # ìºì‹± ì„¤ì •
    enable_redis: bool = True
    redis_host: str = "localhost"
    redis_port: int = 6379
    redis_db: int = 0

    # í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
    enable_technical_indicators: bool = True
    enable_sentiment_analysis: bool = True
    enable_macro_features: bool = True
    enable_cross_asset_features: bool = True

    # ìµœì í™” ì„¤ì •
    use_dask: bool = True
    use_multiprocessing: bool = True
    use_gpu_acceleration: bool = True

    # í’ˆì§ˆ ê´€ë¦¬
    data_quality_checks: bool = True
    outlier_detection: bool = True
    missing_data_handling: str = "interpolate"  # "drop", "interpolate", "forward_fill"

class MemoryManager:
    """ë©”ëª¨ë¦¬ ê´€ë¦¬ì"""

    def __init__(self, config: ExtremeDataConfig):
        self.config = config
        self.memory_limit = config.memory_limit_gb * (1024**3)
        self.allocated_memory = 0
        self.memory_pool = weakref.WeakValueDictionary()

    def get_memory_usage(self) -> Dict[str, float]:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¡°íšŒ"""
        process = psutil.Process()
        memory_info = process.memory_info()

        return {
            'rss_gb': memory_info.rss / (1024**3),
            'vms_gb': memory_info.vms / (1024**3),
            'percent': process.memory_percent(),
            'available_gb': (psutil.virtual_memory().available) / (1024**3)
        }

    def cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        gc.collect()

        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
        memory_usage = self.get_memory_usage()
        if memory_usage['rss_gb'] > self.config.memory_limit_gb * 0.9:
            logger.warning(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {memory_usage['rss_gb']:.1f}GB")

            # ê°•ì œ ì •ë¦¬
            for obj in list(self.memory_pool.values()):
                del obj
            gc.collect()

    @asynccontextmanager
    async def memory_context(self, operation_name: str = ""):
        """ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ê´€ë¦¬ì"""
        initial_memory = self.get_memory_usage()
        logger.info(f"ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì‹œì‘ [{operation_name}]: {initial_memory['rss_gb']:.1f}GB")

        try:
            yield
        finally:
            self.cleanup_memory()
            final_memory = self.get_memory_usage()
            logger.info(f"ë©”ëª¨ë¦¬ ì»¨í…ìŠ¤íŠ¸ ì¢…ë£Œ [{operation_name}]: {final_memory['rss_gb']:.1f}GB")

class DataSourceManager:
    """ë°ì´í„° ì†ŒìŠ¤ ê´€ë¦¬ì"""

    def __init__(self, config: ExtremeDataConfig):
        self.config = config
        self.data_sources = {}
        self.cache_manager = CacheManager(config)

    async def initialize_data_sources(self):
        """ë°ì´í„° ì†ŒìŠ¤ ì´ˆê¸°í™”"""
        logger.info("ğŸ”„ ë°ì´í„° ì†ŒìŠ¤ ì´ˆê¸°í™” ì‹œì‘")

        # 1. KRX ë°ì´í„°
        krx_path = Path(self.config.base_data_path) / "krx_all"
        if krx_path.exists():
            self.data_sources['krx'] = {
                'path': krx_path,
                'type': 'parquet',
                'estimated_size_gb': self._estimate_folder_size(krx_path)
            }

        # 2. DART ë°ì´í„°
        dart_path = Path(self.config.base_data_path) / "dart_all"
        if dart_path.exists():
            self.data_sources['dart'] = {
                'path': dart_path,
                'type': 'parquet',
                'estimated_size_gb': self._estimate_folder_size(dart_path)
            }

        # 3. ìˆ˜ì§‘ëœ ë°ì´í„°
        collected_path = Path(self.config.base_data_path) / "collected_data"
        if collected_path.exists():
            self.data_sources['collected'] = {
                'path': collected_path,
                'type': 'mixed',
                'estimated_size_gb': self._estimate_folder_size(collected_path)
            }

        # 4. ë°±ì—… ë°ì´í„°
        backup_path = Path(self.config.backup_data_path)
        if backup_path.exists():
            self.data_sources['backup'] = {
                'path': backup_path,
                'type': 'mixed',
                'estimated_size_gb': self._estimate_folder_size(backup_path)
            }

        total_size = sum(source['estimated_size_gb'] for source in self.data_sources.values())
        logger.info(f"âœ… ë°ì´í„° ì†ŒìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ: {len(self.data_sources)}ê°œ ì†ŒìŠ¤, ì´ {total_size:.1f}GB")

        return self.data_sources

    def _estimate_folder_size(self, folder_path: Path) -> float:
        """í´ë” í¬ê¸° ì¶”ì • (GB)"""
        try:
            total_size = sum(f.stat().st_size for f in folder_path.rglob('*') if f.is_file())
            return total_size / (1024**3)
        except Exception as e:
            logger.warning(f"í´ë” í¬ê¸° ê³„ì‚° ì‹¤íŒ¨ {folder_path}: {e}")
            return 0.0

class CacheManager:
    """ìºì‹œ ê´€ë¦¬ì"""

    def __init__(self, config: ExtremeDataConfig):
        self.config = config
        self.redis_client = None
        self.local_cache = {}

        if config.enable_redis:
            try:
                import redis
                self.redis_client = redis.Redis(
                    host=config.redis_host,
                    port=config.redis_port,
                    db=config.redis_db,
                    decode_responses=False
                )
                self.redis_client.ping()
                logger.info("âœ… Redis ìºì‹œ ì—°ê²° ì„±ê³µ")
            except Exception as e:
                logger.warning(f"Redis ì—°ê²° ì‹¤íŒ¨, ë¡œì»¬ ìºì‹œë§Œ ì‚¬ìš©: {e}")

    async def get(self, key: str) -> Optional[Any]:
        """ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ"""
        # 1. ë¡œì»¬ ìºì‹œ í™•ì¸
        if key in self.local_cache:
            return self.local_cache[key]

        # 2. Redis ìºì‹œ í™•ì¸
        if self.redis_client:
            try:
                data = self.redis_client.get(key)
                if data:
                    result = pickle.loads(data)
                    # ë¡œì»¬ ìºì‹œì—ë„ ì €ì¥ (í¬ê¸° ì œí•œ)
                    if len(self.local_cache) < 1000:
                        self.local_cache[key] = result
                    return result
            except Exception as e:
                logger.warning(f"Redis ì¡°íšŒ ì‹¤íŒ¨ {key}: {e}")

        return None

    async def set(self, key: str, value: Any, expire: int = 3600):
        """ìºì‹œì— ë°ì´í„° ì €ì¥"""
        # 1. ë¡œì»¬ ìºì‹œ ì €ì¥
        if len(self.local_cache) < 1000:
            self.local_cache[key] = value

        # 2. Redis ìºì‹œ ì €ì¥
        if self.redis_client:
            try:
                data = pickle.dumps(value)
                self.redis_client.setex(key, expire, data)
            except Exception as e:
                logger.warning(f"Redis ì €ì¥ ì‹¤íŒ¨ {key}: {e}")

class FeatureEngineering:
    """ê·¹í•œ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§"""

    def __init__(self, config: ExtremeDataConfig):
        self.config = config
        self.sentiment_analyzer = None

        # ê°ì„± ë¶„ì„ ëª¨ë¸ ì´ˆê¸°í™”
        if config.enable_sentiment_analysis:
            try:
                self.sentiment_analyzer = pipeline(
                    "sentiment-analysis",
                    model="nlptown/bert-base-multilingual-uncased-sentiment",
                    device=0 if torch.cuda.is_available() else -1
                )
                logger.info("âœ… ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
            except Exception as e:
                logger.warning(f"ê°ì„± ë¶„ì„ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")

    async def generate_technical_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ê¸°ìˆ ì  ì§€í‘œ í”¼ì²˜ ìƒì„±"""
        if not self.config.enable_technical_indicators:
            return df

        logger.info("ğŸ”§ ê¸°ìˆ ì  ì§€í‘œ í”¼ì²˜ ìƒì„± ì‹œì‘")

        # ê¸°ë³¸ OHLCV í™•ì¸
        required_cols = ['open', 'high', 'low', 'close', 'volume']
        if not all(col in df.columns for col in required_cols):
            logger.warning("OHLCV ì»¬ëŸ¼ ë¶€ì¡±, ê¸°ìˆ ì  ì§€í‘œ ìƒì„± ìƒëµ")
            return df

        try:
            # 1. ì´ë™í‰ê·  (ë‹¤ì–‘í•œ ê¸°ê°„)
            periods = [5, 10, 20, 50, 100, 200]
            for period in periods:
                df[f'sma_{period}'] = df['close'].rolling(period).mean()
                df[f'ema_{period}'] = df['close'].ewm(span=period).mean()
                df[f'price_to_sma_{period}'] = df['close'] / df[f'sma_{period}']

            # 2. ë³¼ë¦°ì € ë°´ë“œ
            for period in [20, 50]:
                sma = df['close'].rolling(period).mean()
                std = df['close'].rolling(period).std()
                df[f'bb_upper_{period}'] = sma + (2 * std)
                df[f'bb_lower_{period}'] = sma - (2 * std)
                df[f'bb_width_{period}'] = (df[f'bb_upper_{period}'] - df[f'bb_lower_{period}']) / sma
                df[f'bb_position_{period}'] = (df['close'] - df[f'bb_lower_{period}']) / (df[f'bb_upper_{period}'] - df[f'bb_lower_{period}'])

            # 3. RSI (ë‹¤ì–‘í•œ ê¸°ê°„)
            for period in [14, 30, 50]:
                delta = df['close'].diff()
                gain = delta.where(delta > 0, 0).rolling(period).mean()
                loss = (-delta.where(delta < 0, 0)).rolling(period).mean()
                rs = gain / loss
                df[f'rsi_{period}'] = 100 - (100 / (1 + rs))

            # 4. MACD
            ema12 = df['close'].ewm(span=12).mean()
            ema26 = df['close'].ewm(span=26).mean()
            df['macd'] = ema12 - ema26
            df['macd_signal'] = df['macd'].ewm(span=9).mean()
            df['macd_histogram'] = df['macd'] - df['macd_signal']

            # 5. ìŠ¤í† ìºìŠ¤í‹±
            for period in [14, 21]:
                low_min = df['low'].rolling(period).min()
                high_max = df['high'].rolling(period).max()
                df[f'stoch_k_{period}'] = 100 * (df['close'] - low_min) / (high_max - low_min)
                df[f'stoch_d_{period}'] = df[f'stoch_k_{period}'].rolling(3).mean()

            # 6. ATR (Average True Range)
            for period in [14, 30]:
                high_low = df['high'] - df['low']
                high_close = np.abs(df['high'] - df['close'].shift())
                low_close = np.abs(df['low'] - df['close'].shift())
                tr = np.maximum(high_low, np.maximum(high_close, low_close))
                df[f'atr_{period}'] = tr.rolling(period).mean()

            # 7. ê±°ë˜ëŸ‰ ì§€í‘œ
            df['volume_sma_20'] = df['volume'].rolling(20).mean()
            df['volume_ratio'] = df['volume'] / df['volume_sma_20']
            df['price_volume'] = df['close'] * df['volume']
            df['vwap'] = (df['price_volume'].rolling(20).sum() / df['volume'].rolling(20).sum())

            # 8. ëª¨ë©˜í…€ ì§€í‘œ
            for period in [1, 5, 10, 20]:
                df[f'momentum_{period}'] = df['close'] / df['close'].shift(period) - 1
                df[f'roc_{period}'] = df['close'].pct_change(period)

            # 9. ë³€ë™ì„± ì§€í‘œ
            for period in [10, 20, 30]:
                df[f'volatility_{period}'] = df['close'].pct_change().rolling(period).std()
                df[f'volatility_ratio_{period}'] = df[f'volatility_{period}'] / df[f'volatility_{period}'].rolling(60).mean()

            # 10. ê°€ê²© íŒ¨í„´
            df['doji'] = np.abs(df['close'] - df['open']) / (df['high'] - df['low'])
            df['upper_shadow'] = df['high'] - np.maximum(df['open'], df['close'])
            df['lower_shadow'] = np.minimum(df['open'], df['close']) - df['low']
            df['body_size'] = np.abs(df['close'] - df['open'])

            logger.info(f"âœ… ê¸°ìˆ ì  ì§€í‘œ í”¼ì²˜ ìƒì„± ì™„ë£Œ: {len([col for col in df.columns if any(indicator in col for indicator in ['sma', 'ema', 'rsi', 'macd', 'bb', 'stoch', 'atr'])])}ê°œ í”¼ì²˜")

        except Exception as e:
            logger.error(f"ê¸°ìˆ ì  ì§€í‘œ ìƒì„± ì‹¤íŒ¨: {e}")

        return df

    async def generate_macro_features(self, df: pd.DataFrame) -> pd.DataFrame:
        """ê±°ì‹œê²½ì œ í”¼ì²˜ ìƒì„±"""
        if not self.config.enable_macro_features:
            return df

        logger.info("ğŸ“Š ê±°ì‹œê²½ì œ í”¼ì²˜ ìƒì„± ì‹œì‘")

        try:
            # ì‹œê°„ ê¸°ë°˜ í”¼ì²˜
            if 'date' in df.columns:
                df['date'] = pd.to_datetime(df['date'])
                df['year'] = df['date'].dt.year
                df['month'] = df['date'].dt.month
                df['day'] = df['date'].dt.day
                df['day_of_week'] = df['date'].dt.dayofweek
                df['day_of_year'] = df['date'].dt.dayofyear
                df['week_of_year'] = df['date'].dt.isocalendar().week
                df['quarter'] = df['date'].dt.quarter

                # ê³„ì ˆì„± í”¼ì²˜
                df['sin_day_of_year'] = np.sin(2 * np.pi * df['day_of_year'] / 365.25)
                df['cos_day_of_year'] = np.cos(2 * np.pi * df['day_of_year'] / 365.25)
                df['sin_month'] = np.sin(2 * np.pi * df['month'] / 12)
                df['cos_month'] = np.cos(2 * np.pi * df['month'] / 12)

            # ê²½ì œ ì‚¬ì´í´ í”¼ì²˜ (ê°€ìƒì˜ ë°ì´í„° - ì‹¤ì œë¡œëŠ” BOK API ë“±ì—ì„œ ê°€ì ¸ì™€ì•¼ í•¨)
            df['economic_cycle'] = np.sin(2 * np.pi * (df.index % 252) / 252)  # 1ë…„ ì£¼ê¸°
            df['business_cycle'] = np.sin(2 * np.pi * (df.index % 1260) / 1260)  # 5ë…„ ì£¼ê¸°

            logger.info("âœ… ê±°ì‹œê²½ì œ í”¼ì²˜ ìƒì„± ì™„ë£Œ")

        except Exception as e:
            logger.error(f"ê±°ì‹œê²½ì œ í”¼ì²˜ ìƒì„± ì‹¤íŒ¨: {e}")

        return df

    async def generate_sentiment_features(self, df: pd.DataFrame, news_data: Optional[pd.DataFrame] = None) -> pd.DataFrame:
        """ê°ì„± ë¶„ì„ í”¼ì²˜ ìƒì„±"""
        if not self.config.enable_sentiment_analysis or self.sentiment_analyzer is None:
            return df

        logger.info("ğŸ˜Š ê°ì„± ë¶„ì„ í”¼ì²˜ ìƒì„± ì‹œì‘")

        try:
            if news_data is not None and 'title' in news_data.columns:
                # ë‰´ìŠ¤ ê°ì„± ë¶„ì„
                sentiments = []
                for title in news_data['title'].fillna('').head(1000):  # ì²˜ë¦¬ ì‹œê°„ ê³ ë ¤í•˜ì—¬ ì œí•œ
                    try:
                        result = self.sentiment_analyzer(title[:512])  # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ
                        sentiment_score = result[0]['score'] if result[0]['label'] == 'POSITIVE' else -result[0]['score']
                        sentiments.append(sentiment_score)
                    except:
                        sentiments.append(0.0)

                # ì¼ë³„ ê°ì„± ì ìˆ˜ ì§‘ê³„
                news_data['sentiment'] = sentiments[:len(news_data)]
                daily_sentiment = news_data.groupby(news_data['date'].dt.date)['sentiment'].agg([
                    'mean', 'std', 'min', 'max', 'count'
                ]).reset_index()
                daily_sentiment.columns = ['date', 'sentiment_mean', 'sentiment_std', 'sentiment_min', 'sentiment_max', 'news_count']

                # ë©”ì¸ ë°ì´í„°ì™€ ë³‘í•©
                if 'date' in df.columns:
                    df['date_only'] = pd.to_datetime(df['date']).dt.date
                    df = df.merge(daily_sentiment, left_on='date_only', right_on='date', how='left')
                    df = df.drop(['date_only'], axis=1)

                    # ê°ì„± ì§€í‘œ ì´ë™í‰ê· 
                    for period in [5, 10, 20]:
                        df[f'sentiment_ma_{period}'] = df['sentiment_mean'].rolling(period).mean()

            logger.info("âœ… ê°ì„± ë¶„ì„ í”¼ì²˜ ìƒì„± ì™„ë£Œ")

        except Exception as e:
            logger.error(f"ê°ì„± ë¶„ì„ í”¼ì²˜ ìƒì„± ì‹¤íŒ¨: {e}")

        return df

class ExtremeDataPipeline:
    """ê·¹í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸"""

    def __init__(self, config: Optional[ExtremeDataConfig] = None):
        self.config = config or ExtremeDataConfig()
        self.memory_manager = MemoryManager(self.config)
        self.data_source_manager = DataSourceManager(self.config)
        self.feature_engineering = FeatureEngineering(self.config)

        # Dask í´ë¼ì´ì–¸íŠ¸
        self.dask_client = None
        if self.config.use_dask:
            try:
                self.dask_client = Client(
                    n_workers=self.config.max_workers,
                    threads_per_worker=2,
                    memory_limit='2GB'
                )
                logger.info(f"âœ… Dask í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”: {self.config.max_workers} ì›Œì»¤")
            except Exception as e:
                logger.warning(f"Dask ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")

        # í†µê³„ ì¶”ì 
        self.pipeline_stats = {
            'start_time': None,
            'end_time': None,
            'total_records_processed': 0,
            'total_features_generated': 0,
            'memory_peak_gb': 0.0,
            'processing_speed_records_per_sec': 0.0
        }

        logger.info("ğŸš€ ê·¹í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ")

    async def run_complete_pipeline(self) -> Dict[str, Any]:
        """ì™„ì „í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        logger.info("ğŸ”¥ ê·¹í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        self.pipeline_stats['start_time'] = datetime.now()

        try:
            # 1. ë°ì´í„° ì†ŒìŠ¤ ì´ˆê¸°í™”
            data_sources = await self.data_source_manager.initialize_data_sources()

            # 2. ëª¨ë“  ë°ì´í„° ë¡œë“œ ë° í†µí•©
            async with self.memory_manager.memory_context("ë°ì´í„° ë¡œë“œ"):
                unified_data = await self._load_and_unify_all_data(data_sources)

            # 3. ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬
            async with self.memory_manager.memory_context("í’ˆì§ˆ ê²€ì‚¬"):
                cleaned_data = await self._perform_quality_checks(unified_data)

            # 4. ê·¹í•œ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§
            async with self.memory_manager.memory_context("í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§"):
                featured_data = await self._generate_all_features(cleaned_data)

            # 5. ìµœì¢… ë°ì´í„° ì €ì¥
            async with self.memory_manager.memory_context("ë°ì´í„° ì €ì¥"):
                saved_paths = await self._save_processed_data(featured_data)

            # 6. í†µê³„ ê³„ì‚°
            self.pipeline_stats['end_time'] = datetime.now()
            self.pipeline_stats['total_records_processed'] = len(featured_data) if featured_data is not None else 0
            self.pipeline_stats['total_features_generated'] = len(featured_data.columns) if featured_data is not None else 0

            duration = (self.pipeline_stats['end_time'] - self.pipeline_stats['start_time']).total_seconds()
            self.pipeline_stats['processing_speed_records_per_sec'] = self.pipeline_stats['total_records_processed'] / duration if duration > 0 else 0

            logger.info("ğŸ‰ ê·¹í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
            logger.info(f"ğŸ“Š ì²˜ë¦¬ í†µê³„:")
            logger.info(f"  - ì²˜ë¦¬ëœ ë ˆì½”ë“œ: {self.pipeline_stats['total_records_processed']:,}")
            logger.info(f"  - ìƒì„±ëœ í”¼ì²˜: {self.pipeline_stats['total_features_generated']:,}")
            logger.info(f"  - ì²˜ë¦¬ ì†ë„: {self.pipeline_stats['processing_speed_records_per_sec']:,.0f} ë ˆì½”ë“œ/ì´ˆ")
            logger.info(f"  - ì´ ì†Œìš” ì‹œê°„: {duration:.1f}ì´ˆ")

            return {
                'stats': self.pipeline_stats,
                'data_paths': saved_paths,
                'data_shape': featured_data.shape if featured_data is not None else (0, 0)
            }

        except Exception as e:
            logger.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            raise
        finally:
            if self.dask_client:
                self.dask_client.close()

    async def _load_and_unify_all_data(self, data_sources: Dict[str, Any]) -> pd.DataFrame:
        """ëª¨ë“  ë°ì´í„° ë¡œë“œ ë° í†µí•©"""
        logger.info("ğŸ“¥ ì „ì²´ ë°ì´í„° ë¡œë“œ ë° í†µí•© ì‹œì‘")

        all_dataframes = []

        # ë³‘ë ¬ë¡œ ê° ë°ì´í„° ì†ŒìŠ¤ ì²˜ë¦¬
        tasks = []
        for source_name, source_info in data_sources.items():
            task = self._load_data_source(source_name, source_info)
            tasks.append(task)

        # ëª¨ë“  ë°ì´í„° ì†ŒìŠ¤ ë¡œë“œ ì™„ë£Œ ëŒ€ê¸°
        results = await asyncio.gather(*tasks, return_exceptions=True)

        for i, result in enumerate(results):
            source_name = list(data_sources.keys())[i]
            if isinstance(result, Exception):
                logger.error(f"ë°ì´í„° ì†ŒìŠ¤ ë¡œë“œ ì‹¤íŒ¨ {source_name}: {result}")
            elif result is not None:
                all_dataframes.append(result)
                logger.info(f"âœ… {source_name} ë¡œë“œ ì™„ë£Œ: {len(result):,} ë ˆì½”ë“œ")

        # ëª¨ë“  ë°ì´í„°í”„ë ˆì„ í†µí•©
        if all_dataframes:
            unified_data = pd.concat(all_dataframes, ignore_index=True, sort=False)
            logger.info(f"âœ… ë°ì´í„° í†µí•© ì™„ë£Œ: {len(unified_data):,} ë ˆì½”ë“œ, {len(unified_data.columns)} ì»¬ëŸ¼")
            return unified_data
        else:
            logger.warning("ë¡œë“œëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return pd.DataFrame()

    async def _load_data_source(self, source_name: str, source_info: Dict[str, Any]) -> Optional[pd.DataFrame]:
        """ê°œë³„ ë°ì´í„° ì†ŒìŠ¤ ë¡œë“œ"""
        try:
            source_path = Path(source_info['path'])

            if source_info['type'] == 'parquet':
                # Parquet íŒŒì¼ë“¤ ë¡œë“œ
                parquet_files = list(source_path.rglob('*.parquet'))
                if parquet_files:
                    dataframes = []
                    for file_path in parquet_files[:10]:  # ë©”ëª¨ë¦¬ ê³ ë ¤í•˜ì—¬ ì œí•œ
                        try:
                            df = pd.read_parquet(file_path)
                            dataframes.append(df)
                        except Exception as e:
                            logger.warning(f"Parquet íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {file_path}: {e}")

                    if dataframes:
                        return pd.concat(dataframes, ignore_index=True)

            elif source_info['type'] == 'mixed':
                # ë‹¤ì–‘í•œ í˜•ì‹ì˜ íŒŒì¼ë“¤ ë¡œë“œ
                dataframes = []

                # CSV íŒŒì¼ë“¤
                csv_files = list(source_path.rglob('*.csv'))
                for file_path in csv_files[:5]:  # ì œí•œ
                    try:
                        df = pd.read_csv(file_path)
                        dataframes.append(df)
                    except Exception as e:
                        logger.warning(f"CSV íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {file_path}: {e}")

                # Feather íŒŒì¼ë“¤
                feather_files = list(source_path.rglob('*.feather'))
                for file_path in feather_files[:5]:  # ì œí•œ
                    try:
                        df = pd.read_feather(file_path)
                        dataframes.append(df)
                    except Exception as e:
                        logger.warning(f"Feather íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ {file_path}: {e}")

                if dataframes:
                    return pd.concat(dataframes, ignore_index=True)

            return None

        except Exception as e:
            logger.error(f"ë°ì´í„° ì†ŒìŠ¤ ë¡œë“œ ì‹¤íŒ¨ {source_name}: {e}")
            return None

    async def _perform_quality_checks(self, data: pd.DataFrame) -> pd.DataFrame:
        """ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ë° ì •ë¦¬"""
        if not self.config.data_quality_checks or data.empty:
            return data

        logger.info("ğŸ” ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ì‹œì‘")

        initial_rows = len(data)

        try:
            # 1. ì¤‘ë³µ ì œê±°
            data = data.drop_duplicates()
            logger.info(f"ì¤‘ë³µ ì œê±°: {initial_rows - len(data):,} í–‰ ì œê±°")

            # 2. ê²°ì¸¡ê°’ ì²˜ë¦¬
            if self.config.missing_data_handling == "drop":
                data = data.dropna()
            elif self.config.missing_data_handling == "interpolate":
                numeric_cols = data.select_dtypes(include=[np.number]).columns
                data[numeric_cols] = data[numeric_cols].interpolate()
            elif self.config.missing_data_handling == "forward_fill":
                data = data.fillna(method='ffill')

            # 3. ì´ìƒì¹˜ íƒì§€ ë° ì²˜ë¦¬
            if self.config.outlier_detection:
                numeric_cols = data.select_dtypes(include=[np.number]).columns
                for col in numeric_cols:
                    if col in data.columns:
                        Q1 = data[col].quantile(0.01)
                        Q3 = data[col].quantile(0.99)
                        IQR = Q3 - Q1
                        lower_bound = Q1 - 1.5 * IQR
                        upper_bound = Q3 + 1.5 * IQR

                        # ê·¹ë‹¨ê°’ í´ë¦¬í•‘
                        data[col] = data[col].clip(lower_bound, upper_bound)

            # 4. ë°ì´í„° íƒ€ì… ìµœì í™”
            data = self._optimize_data_types(data)

            logger.info(f"âœ… ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ì™„ë£Œ: {len(data):,} í–‰ ìœ ì§€")

        except Exception as e:
            logger.error(f"ë°ì´í„° í’ˆì§ˆ ê²€ì‚¬ ì‹¤íŒ¨: {e}")

        return data

    def _optimize_data_types(self, data: pd.DataFrame) -> pd.DataFrame:
        """ë°ì´í„° íƒ€ì… ìµœì í™”"""
        try:
            # ì •ìˆ˜í˜• ìµœì í™”
            int_cols = data.select_dtypes(include=['int64']).columns
            for col in int_cols:
                data[col] = pd.to_numeric(data[col], downcast='integer')

            # ì‹¤ìˆ˜í˜• ìµœì í™”
            float_cols = data.select_dtypes(include=['float64']).columns
            for col in float_cols:
                data[col] = pd.to_numeric(data[col], downcast='float')

            # ë²”ì£¼í˜• ìµœì í™”
            object_cols = data.select_dtypes(include=['object']).columns
            for col in object_cols:
                if data[col].nunique() < len(data) * 0.5:  # ìœ ë‹ˆí¬ ê°’ì´ 50% ë¯¸ë§Œì´ë©´ ë²”ì£¼í˜•ìœ¼ë¡œ
                    data[col] = data[col].astype('category')

            logger.info("âœ… ë°ì´í„° íƒ€ì… ìµœì í™” ì™„ë£Œ")

        except Exception as e:
            logger.warning(f"ë°ì´í„° íƒ€ì… ìµœì í™” ì‹¤íŒ¨: {e}")

        return data

    async def _generate_all_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """ëª¨ë“  í”¼ì²˜ ìƒì„±"""
        if data.empty:
            return data

        logger.info("ğŸ”§ ê·¹í•œ í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì‹œì‘")

        try:
            # 1. ê¸°ìˆ ì  ì§€í‘œ í”¼ì²˜
            data = await self.feature_engineering.generate_technical_features(data)

            # 2. ê±°ì‹œê²½ì œ í”¼ì²˜
            data = await self.feature_engineering.generate_macro_features(data)

            # 3. ê°ì„± ë¶„ì„ í”¼ì²˜ (ë‰´ìŠ¤ ë°ì´í„° ìˆëŠ” ê²½ìš°)
            # data = await self.feature_engineering.generate_sentiment_features(data)

            # 4. êµì°¨ ìì‚° í”¼ì²˜
            if self.config.enable_cross_asset_features:
                data = await self._generate_cross_asset_features(data)

            # 5. ê³ ê¸‰ í†µê³„ í”¼ì²˜
            data = await self._generate_statistical_features(data)

            logger.info(f"âœ… í”¼ì²˜ ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ: ì´ {len(data.columns)} í”¼ì²˜")

        except Exception as e:
            logger.error(f"í”¼ì²˜ ìƒì„± ì‹¤íŒ¨: {e}")

        return data

    async def _generate_cross_asset_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """êµì°¨ ìì‚° í”¼ì²˜ ìƒì„±"""
        try:
            # ì¢…ëª© ê°„ ìƒê´€ê´€ê³„ (symbol ì»¬ëŸ¼ì´ ìˆëŠ” ê²½ìš°)
            if 'symbol' in data.columns and 'close' in data.columns:
                symbols = data['symbol'].unique()[:10]  # ì²˜ë¦¬ ì‹œê°„ ê³ ë ¤í•˜ì—¬ ì œí•œ

                for i, symbol1 in enumerate(symbols):
                    for symbol2 in symbols[i+1:]:
                        symbol1_data = data[data['symbol'] == symbol1]['close']
                        symbol2_data = data[data['symbol'] == symbol2]['close']

                        # ìƒê´€ê´€ê³„ ê³„ì‚° (rolling)
                        if len(symbol1_data) > 20 and len(symbol2_data) > 20:
                            correlation = symbol1_data.rolling(20).corr(symbol2_data)
                            data.loc[data['symbol'] == symbol1, f'corr_{symbol1}_{symbol2}'] = correlation

            logger.info("âœ… êµì°¨ ìì‚° í”¼ì²˜ ìƒì„± ì™„ë£Œ")

        except Exception as e:
            logger.warning(f"êµì°¨ ìì‚° í”¼ì²˜ ìƒì„± ì‹¤íŒ¨: {e}")

        return data

    async def _generate_statistical_features(self, data: pd.DataFrame) -> pd.DataFrame:
        """ê³ ê¸‰ í†µê³„ í”¼ì²˜ ìƒì„±"""
        try:
            numeric_cols = data.select_dtypes(include=[np.number]).columns

            for col in numeric_cols[:10]:  # ì²˜ë¦¬ ì‹œê°„ ê³ ë ¤í•˜ì—¬ ì œí•œ
                if col in data.columns:
                    # ë¡¤ë§ í†µê³„
                    for window in [5, 10, 20]:
                        data[f'{col}_rolling_mean_{window}'] = data[col].rolling(window).mean()
                        data[f'{col}_rolling_std_{window}'] = data[col].rolling(window).std()
                        data[f'{col}_rolling_skew_{window}'] = data[col].rolling(window).skew()
                        data[f'{col}_rolling_kurt_{window}'] = data[col].rolling(window).kurt()

                    # Z-score
                    data[f'{col}_zscore'] = (data[col] - data[col].rolling(252).mean()) / data[col].rolling(252).std()

            logger.info("âœ… ê³ ê¸‰ í†µê³„ í”¼ì²˜ ìƒì„± ì™„ë£Œ")

        except Exception as e:
            logger.warning(f"ê³ ê¸‰ í†µê³„ í”¼ì²˜ ìƒì„± ì‹¤íŒ¨: {e}")

        return data

    async def _save_processed_data(self, data: pd.DataFrame) -> Dict[str, str]:
        """ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥"""
        if data.empty:
            return {}

        logger.info("ğŸ’¾ ì²˜ë¦¬ëœ ë°ì´í„° ì €ì¥ ì‹œì‘")

        save_paths = {}
        output_dir = Path(self.config.processed_data_path)
        output_dir.mkdir(parents=True, exist_ok=True)

        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")

        try:
            # 1. Parquet í˜•ì‹ (ì••ì¶•, ë¹ ë¥¸ ë¡œë“œ)
            parquet_path = output_dir / f"extreme_processed_data_{timestamp}.parquet"
            data.to_parquet(parquet_path, compression='snappy', index=False)
            save_paths['parquet'] = str(parquet_path)

            # 2. Feather í˜•ì‹ (ë§¤ìš° ë¹ ë¥¸ ë¡œë“œ)
            feather_path = output_dir / f"extreme_processed_data_{timestamp}.feather"
            data.to_feather(feather_path)
            save_paths['feather'] = str(feather_path)

            # 3. HDF5 í˜•ì‹ (ëŒ€ìš©ëŸ‰ ë°ì´í„°)
            hdf5_path = output_dir / f"extreme_processed_data_{timestamp}.h5"
            data.to_hdf(hdf5_path, key='data', mode='w', complevel=9, complib='blosc')
            save_paths['hdf5'] = str(hdf5_path)

            # 4. ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata = {
                'shape': data.shape,
                'columns': data.columns.tolist(),
                'dtypes': data.dtypes.astype(str).to_dict(),
                'creation_time': timestamp,
                'pipeline_stats': self.pipeline_stats
            }

            metadata_path = output_dir / f"metadata_{timestamp}.json"
            with open(metadata_path, 'w', encoding='utf-8') as f:
                import json
                json.dump(metadata, f, indent=2, ensure_ascii=False)
            save_paths['metadata'] = str(metadata_path)

            logger.info(f"âœ… ë°ì´í„° ì €ì¥ ì™„ë£Œ: {len(save_paths)} ê°œ í˜•ì‹")

        except Exception as e:
            logger.error(f"ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")

        return save_paths

# í…ŒìŠ¤íŠ¸ ë° ì‹¤í–‰
async def test_extreme_pipeline():
    """ê·¹í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    logger.info("ğŸ§ª ê·¹í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘")

    config = ExtremeDataConfig(
        chunk_size=50000,
        memory_limit_gb=16,  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ì œí•œ
        enable_technical_indicators=True,
        enable_macro_features=True,
        enable_sentiment_analysis=False,  # í…ŒìŠ¤íŠ¸ì—ì„œëŠ” ë¹„í™œì„±í™”
        use_dask=False  # í…ŒìŠ¤íŠ¸ìš©ìœ¼ë¡œ ë¹„í™œì„±í™”
    )

    pipeline = ExtremeDataPipeline(config)
    results = await pipeline.run_complete_pipeline()

    logger.info("âœ… ê·¹í•œ ë°ì´í„° íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
    logger.info(f"ê²°ê³¼: {results}")

    return results

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    asyncio.run(test_extreme_pipeline())
