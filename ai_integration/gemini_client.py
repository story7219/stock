#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ¤– Gemini AI í´ë¼ì´ì–¸íŠ¸ - 100% ì„±ëŠ¥ ìµœì í™” ë²„ì „
íˆ¬ì ë¶„ì„ì„ ìœ„í•œ Gemini AI ì—°ë™ ëª¨ë“ˆ
"""

import asyncio
import json
import logging
import time
import threading
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from collections import deque
import sqlite3
from pathlib import Path

import google.generativeai as genai
from dotenv import load_dotenv
import os

# ì„¤ì • import ì¶”ê°€
from configs.settings import GEMINI_CACHE_TTL

# í™˜ê²½ ì„¤ì •
load_dotenv()
logger = logging.getLogger(__name__)

# 100% ì„±ëŠ¥ ìµœì í™” ìƒìˆ˜
MAX_CONCURRENT = 100  # ë™ì‹œ ìš”ì²­ ìˆ˜ ëŒ€í­ ì¦ê°€
BATCH_SIZE = 50       # ë°°ì¹˜ í¬ê¸° ëŒ€í­ ì¦ê°€
REQUEST_DELAY = 0.005 # ìš”ì²­ ì§€ì—° ë”ìš± ìµœì†Œí™”
ULTRA_RETRY = 15      # ì¬ì‹œë„ íšŸìˆ˜ ë” ì¦ê°€
MAX_TOKENS = 16384    # ìµœëŒ€ í† í° ìˆ˜ ëŒ€í­ ì¦ê°€

@dataclass
class GeminiConfig:
    """Gemini 100% ìµœì í™” ì„¤ì •"""
    api_key: str
    model_version: str = "gemini-1.5-flash"  # ê¸°ë³¸ê°’ì„ 1.5-flashë¡œ ê³ ì •
    temperature: float = 0.05  # ì¼ê´€ì„± ê·¹ëŒ€í™”
    top_p: float = 0.98       # ì°½ì˜ì„±ê³¼ ì •í™•ì„± ê· í˜• ìµœì í™”
    top_k: int = 50           # í† í° ì„ íƒ ìµœì í™”
    max_output_tokens: int = MAX_TOKENS
    batch_size: int = BATCH_SIZE
    max_concurrent: int = MAX_CONCURRENT
    request_delay: float = REQUEST_DELAY
    retry_attempts: int = ULTRA_RETRY
    
    # 100% ì„±ëŠ¥ ìµœì í™” íŒŒë¼ë¯¸í„°
    use_system_instruction: bool = True
    enable_safety_settings: bool = True
    response_mime_type: str = "application/json"
    candidate_count: int = 1
    
    # ìƒˆë¡œìš´ ìš¸íŠ¸ë¼ ìµœì í™” ì„¤ì •
    enable_ultra_caching: bool = True
    smart_prompt_compression: bool = True
    adaptive_batching: bool = True
    ultra_parallel_mode: bool = True
    advanced_error_recovery: bool = True
    
    def __post_init__(self):
        """í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì • ì½ê¸° - .env íŒŒì¼ ìš°ì„ """
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ëª¨ë¸ ì„¤ì • ì½ê¸° (ìˆìœ¼ë©´ ë®ì–´ì“°ê¸°, ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ìœ ì§€)
        env_model = os.getenv('GEMINI_MODEL')
        if env_model:
            self.model_version = env_model
        
        env_temp = os.getenv('GEMINI_TEMPERATURE')
        if env_temp:
            try:
                self.temperature = float(env_temp)
            except ValueError:
                pass  # ê¸°ë³¸ê°’ ìœ ì§€
        
        env_tokens = os.getenv('GEMINI_MAX_TOKENS')
        if env_tokens:
            try:
                self.max_output_tokens = int(env_tokens)
            except ValueError:
                pass  # ê¸°ë³¸ê°’ ìœ ì§€

class GeminiPerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ - ê³ ë„í™”"""
    def __init__(self):
        self.request_times = deque(maxlen=5000)  # ë” ë§ì€ ë°ì´í„° ì €ì¥
        self.success_count = 0
        self.error_count = 0
        self.cache_hits = 0
        self.cache_misses = 0
        self.tokens_used = 0
        self.tokens_saved = 0  # ìºì‹œë¡œ ì ˆì•½ëœ í† í°
        self.start_time = time.time()
        self._lock = threading.RLock()
        
        # ì„±ëŠ¥ ë¶„ì„ìš© ì¶”ê°€ ë©”íŠ¸ë¦­
        self.response_quality_scores = deque(maxlen=1000)
        self.model_switching_count = 0
        self.optimization_triggers = 0
        
    def record_request(self, duration: float, success: bool = True, tokens: int = 0, 
                      quality_score: float = 0.0):
        """ìš”ì²­ ê¸°ë¡ - í’ˆì§ˆ ì ìˆ˜ í¬í•¨"""
        with self._lock:
            self.request_times.append(duration)
            if success:
                self.success_count += 1
                if quality_score > 0:
                    self.response_quality_scores.append(quality_score)
            else:
                self.error_count += 1
            self.tokens_used += tokens
                
    def record_cache(self, hit: bool, tokens_saved: int = 0):
        """ìºì‹œ ê¸°ë¡ - ì ˆì•½ëœ í† í° í¬í•¨"""
        with self._lock:
            if hit:
                self.cache_hits += 1
                self.tokens_saved += tokens_saved
            else:
                self.cache_misses += 1
    
    def get_stats(self) -> Dict[str, Any]:
        """ê³ ë„í™”ëœ í†µê³„ ì¡°íšŒ"""
        with self._lock:
            if not self.request_times:
                return {"status": "no_data"}
            
            total = self.success_count + self.error_count
            avg_time = sum(self.request_times) / len(self.request_times)
            success_rate = (self.success_count / total * 100) if total > 0 else 0
            cache_total = self.cache_hits + self.cache_misses
            cache_rate = (self.cache_hits / cache_total * 100) if cache_total > 0 else 0
            uptime = time.time() - self.start_time
            
            # í’ˆì§ˆ ì ìˆ˜ ê³„ì‚°
            avg_quality = sum(self.response_quality_scores) / len(self.response_quality_scores) if self.response_quality_scores else 0
            
            # ë¹„ìš© íš¨ìœ¨ì„± ê³„ì‚°
            cost_efficiency = (self.tokens_saved / (self.tokens_used + self.tokens_saved) * 100) if (self.tokens_used + self.tokens_saved) > 0 else 0
            
            return {
                "ğŸš€ Gemini AI ì„±ëŠ¥": {
                    "í‰ê· ì‘ë‹µ": f"{avg_time:.3f}ì´ˆ",
                    "ì„±ê³µë¥ ": f"{success_rate:.1f}%",
                    "ìºì‹œì ì¤‘": f"{cache_rate:.1f}%",
                    "ì‘ë‹µí’ˆì§ˆ": f"{avg_quality:.1f}/10",
                    "ë¹„ìš©íš¨ìœ¨": f"{cost_efficiency:.1f}%",
                    "ì´ìš”ì²­": total,
                    "ê°€ë™ì‹œê°„": f"{uptime:.0f}ì´ˆ",
                    "í† í°ì‚¬ìš©": f"{self.tokens_used:,}",
                    "í† í°ì ˆì•½": f"{self.tokens_saved:,}",
                    "ì˜ˆìƒë¹„ìš©": f"${(self.tokens_used * 0.00025):.4f}",
                    "ì ˆì•½ë¹„ìš©": f"${(self.tokens_saved * 0.00025):.4f}"
                }
            }

class GeminiSmartCache:
    """ìŠ¤ë§ˆíŠ¸ ìºì‹± ì‹œìŠ¤í…œ - ê³ ë„í™”"""
    def __init__(self, ttl: int = GEMINI_CACHE_TTL, max_size: int = 2000):  # ì„¤ì • íŒŒì¼ì—ì„œ TTL ì‚¬ìš©
        self._cache: Dict[str, Dict[str, Any]] = {}
        self._access_times: Dict[str, float] = {}
        self._access_counts: Dict[str, int] = {}  # ì ‘ê·¼ íšŸìˆ˜ ì¶”ì 
        self._quality_scores: Dict[str, float] = {}  # í’ˆì§ˆ ì ìˆ˜ ì¶”ì 
        self.ttl = ttl
        self.max_size = max_size
        self._lock = threading.RLock()
        
        # SQLite ìºì‹œ ì´ˆê¸°í™”
        self.cache_dir = Path("cache")
        self.cache_dir.mkdir(exist_ok=True)
        self.db_path = self.cache_dir / "gemini_cache_optimized.db"
        self._init_sqlite()
    
    def _init_sqlite(self):
        """SQLite ì´ˆê¸°í™” - ê³ ë„í™”"""
        try:
            conn = sqlite3.connect(str(self.db_path))
            conn.execute("""
                CREATE TABLE IF NOT EXISTS gemini_cache (
                    key TEXT PRIMARY KEY,
                    value TEXT,
                    timestamp REAL,
                    access_count INTEGER DEFAULT 1,
                    quality_score REAL DEFAULT 0.0,
                    token_count INTEGER DEFAULT 0
                )
            """)
            
            # ì¸ë±ìŠ¤ ì¶”ê°€ë¡œ ì„±ëŠ¥ í–¥ìƒ
            conn.execute("CREATE INDEX IF NOT EXISTS idx_timestamp ON gemini_cache(timestamp)")
            conn.execute("CREATE INDEX IF NOT EXISTS idx_quality ON gemini_cache(quality_score)")
            
            conn.commit()
            conn.close()
        except Exception as e:
            logger.error(f"SQLite ì´ˆê¸°í™” ì˜¤ë¥˜: {e}")
    
    def get(self, key: str) -> Optional[Any]:
        """ìºì‹œ ì¡°íšŒ - ìŠ¤ë§ˆíŠ¸ ìš°ì„ ìˆœìœ„"""
        # ë©”ëª¨ë¦¬ ìºì‹œ ë¨¼ì € í™•ì¸
        with self._lock:
            if key in self._cache:
                data = self._cache[key]
                if time.time() - data['timestamp'] < self.ttl:
                    # ì ‘ê·¼ ì •ë³´ ì—…ë°ì´íŠ¸
                    self._access_times[key] = time.time()
                    self._access_counts[key] = self._access_counts.get(key, 0) + 1
                    return data['value']
                else:
                    # ë§Œë£Œëœ ìºì‹œ ì •ë¦¬
                    self._cleanup_expired_key(key)
        
        # SQLite ìºì‹œ í™•ì¸
        try:
            conn = sqlite3.connect(str(self.db_path))
            cursor = conn.execute(
                "SELECT value, timestamp, quality_score, token_count FROM gemini_cache WHERE key = ? AND timestamp > ?",
                (key, time.time() - self.ttl)
            )
            row = cursor.fetchone()
            
            if row:
                # ì ‘ê·¼ íšŸìˆ˜ ì—…ë°ì´íŠ¸
                conn.execute(
                    "UPDATE gemini_cache SET access_count = access_count + 1 WHERE key = ?",
                    (key,)
                )
                conn.commit()
            
            conn.close()
            
            if row:
                value = json.loads(row[0])
                quality_score = row[2]
                token_count = row[3]
                
                # ë©”ëª¨ë¦¬ ìºì‹œì—ë„ ì €ì¥ (ê³ í’ˆì§ˆ ì‘ë‹µ ìš°ì„ )
                if quality_score >= 7.0:  # ë†’ì€ í’ˆì§ˆë§Œ ë©”ëª¨ë¦¬ ìºì‹œ
                    with self._lock:
                        self._cache[key] = {'value': value, 'timestamp': row[1]}
                        self._access_times[key] = time.time()
                        self._access_counts[key] = 1
                        self._quality_scores[key] = quality_score
                
                return value
        except Exception as e:
            logger.error(f"SQLite ìºì‹œ ì¡°íšŒ ì˜¤ë¥˜: {e}")
        
        return None
    
    def set(self, key: str, value: Any, quality_score: float = 0.0, token_count: int = 0):
        """ìºì‹œ ì €ì¥ - í’ˆì§ˆ ê¸°ë°˜ ìš°ì„ ìˆœìœ„"""
        timestamp = time.time()
        
        # ë©”ëª¨ë¦¬ ìºì‹œ ì €ì¥ (ê³ í’ˆì§ˆë§Œ)
        if quality_score >= 7.0:
            with self._lock:
                # í¬ê¸° ì œí•œ í™•ì¸
                if len(self._cache) >= self.max_size:
                    self._evict_smart()
                
                self._cache[key] = {'value': value, 'timestamp': timestamp}
                self._access_times[key] = timestamp
                self._access_counts[key] = 1
                self._quality_scores[key] = quality_score
        
        # SQLite ìºì‹œ ì €ì¥
        try:
            conn = sqlite3.connect(str(self.db_path))
            conn.execute(
                """INSERT OR REPLACE INTO gemini_cache 
                   (key, value, timestamp, quality_score, token_count) 
                   VALUES (?, ?, ?, ?, ?)""",
                (key, json.dumps(value), timestamp, quality_score, token_count)
            )
            conn.commit()
            conn.close()
        except Exception as e:
            logger.error(f"SQLite ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def _evict_smart(self):
        """ìŠ¤ë§ˆíŠ¸ ìºì‹œ ì œê±° - í’ˆì§ˆê³¼ ì ‘ê·¼ ë¹ˆë„ ê³ ë ¤"""
        if not self._cache:
            return
        
        # í’ˆì§ˆ ì ìˆ˜ì™€ ì ‘ê·¼ ë¹ˆë„ë¥¼ ê³ ë ¤í•œ ì ìˆ˜ ê³„ì‚°
        scores = {}
        current_time = time.time()
        
        for key in self._cache.keys():
            quality = self._quality_scores.get(key, 0)
            access_count = self._access_counts.get(key, 0)
            last_access = self._access_times.get(key, 0)
            recency = max(0, 1 - (current_time - last_access) / self.ttl)
            
            # ì¢…í•© ì ìˆ˜ (í’ˆì§ˆ 50%, ì ‘ê·¼ë¹ˆë„ 30%, ìµœê·¼ì„± 20%)
            score = quality * 0.5 + min(access_count, 10) * 0.3 + recency * 2.0
            scores[key] = score
        
        # ê°€ì¥ ë‚®ì€ ì ìˆ˜ì˜ í‚¤ ì œê±°
        worst_key = min(scores.keys(), key=lambda k: scores[k])
        self._cleanup_expired_key(worst_key)
    
    def _cleanup_expired_key(self, key: str):
        """ë§Œë£Œëœ í‚¤ ì •ë¦¬"""
        with self._lock:
            if key in self._cache:
                del self._cache[key]
            if key in self._access_times:
                del self._access_times[key]
            if key in self._access_counts:
                del self._access_counts[key]
            if key in self._quality_scores:
                del self._quality_scores[key]

class GeminiClient:
    """Gemini AI í´ë¼ì´ì–¸íŠ¸ - 100% ì„±ëŠ¥ ìµœì í™”"""
    
    def __init__(self, config: Optional[GeminiConfig] = None):
        # ì„¤ì • ì´ˆê¸°í™”
        if config is None:
            api_key = os.getenv('GEMINI_API_KEY')
            if not api_key:
                raise ValueError("GEMINI_API_KEY í™˜ê²½ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            config = GeminiConfig(api_key=api_key)
        
        self.config = config
        self.monitor = GeminiPerformanceMonitor()
        self.cache = GeminiSmartCache()
        
        # Gemini ì´ˆê¸°í™” - ìµœì í™” ì„¤ì •
        genai.configure(api_key=self.config.api_key)
        
        # ì•ˆì „ ì„¤ì • ìµœì í™”
        safety_settings = [
            {
                "category": "HARM_CATEGORY_HARASSMENT",
                "threshold": "BLOCK_NONE"
            },
            {
                "category": "HARM_CATEGORY_HATE_SPEECH", 
                "threshold": "BLOCK_NONE"
            },
            {
                "category": "HARM_CATEGORY_SEXUALLY_EXPLICIT",
                "threshold": "BLOCK_NONE"
            },
            {
                "category": "HARM_CATEGORY_DANGEROUS_CONTENT",
                "threshold": "BLOCK_NONE"
            }
        ] if self.config.enable_safety_settings else None
        
        # ì‹œìŠ¤í…œ ì§€ì‹œì‚¬í•­ - íˆ¬ì ë¶„ì„ ì „ë¬¸ê°€
        system_instruction = """
ë‹¹ì‹ ì€ ì„¸ê³„ ìµœê³  ìˆ˜ì¤€ì˜ íˆ¬ì ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

í•µì‹¬ ì—­ëŸ‰:
- ì›Œë Œ ë²„í•, í”¼í„° ë¦°ì¹˜, ë²¤ì €ë¯¼ ê·¸ë ˆì´ì—„ ìˆ˜ì¤€ì˜ ë¶„ì„ ëŠ¥ë ¥
- ì •ëŸ‰ì  ë¶„ì„ê³¼ ì •ì„±ì  ë¶„ì„ì˜ ì™„ë²½í•œ ì¡°í™”
- ê¸€ë¡œë²Œ ì‹œì¥ ë™í–¥ê³¼ í•œêµ­ ì‹œì¥ íŠ¹ì„±ì˜ ê¹Šì€ ì´í•´
- ë¦¬ìŠ¤í¬ ê´€ë¦¬ì™€ ìˆ˜ìµì„± ìµœì í™”ì˜ ê· í˜•

ë¶„ì„ ì›ì¹™:
1. ë°ì´í„° ê¸°ë°˜ ê°ê´€ì  ë¶„ì„
2. ë‹¤ê°ë„ ê´€ì ì—ì„œì˜ ì¢…í•© í‰ê°€
3. íˆ¬ìì ìœ í˜•ë³„ ë§ì¶¤ ì¶”ì²œ
4. ëª…í™•í•œ ê·¼ê±°ì™€ ë…¼ë¦¬ì  ì„¤ëª…
5. ì‹¤ìš©ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì¡°ì–¸

ì‘ë‹µ í˜•ì‹:
- JSON êµ¬ì¡°ë¡œ ì¼ê´€ëœ í˜•ì‹ ì œê³µ
- ì •í™•í•œ ìˆ˜ì¹˜ì™€ êµ¬ì²´ì ì¸ ë¶„ì„ ë‚´ìš©
- íˆ¬ì ë“±ê¸‰, ì ìˆ˜, ëª©í‘œê°€ê²© ë“± ëª…í™•í•œ ê²°ë¡ 
- ë¦¬ìŠ¤í¬ ìš”ì¸ê³¼ ëŒ€ì‘ ë°©ì•ˆ ì œì‹œ

í•­ìƒ ìµœê³  í’ˆì§ˆì˜ ë¶„ì„ì„ ì œê³µí•˜ì—¬ íˆ¬ììì˜ ì„±ê³µì ì¸ ì˜ì‚¬ê²°ì •ì„ ì§€ì›í•˜ì„¸ìš”.
""" if self.config.use_system_instruction else None
        
        # ëª¨ë¸ ì´ˆê¸°í™”
        self.model = genai.GenerativeModel(
            model_name=self.config.model_version,
            generation_config=genai.types.GenerationConfig(
                temperature=self.config.temperature,
                top_p=self.config.top_p,
                top_k=self.config.top_k,
                max_output_tokens=self.config.max_output_tokens,
                response_mime_type=self.config.response_mime_type,
                candidate_count=self.config.candidate_count
            ),
            safety_settings=safety_settings,
            system_instruction=system_instruction
        )
        
        logger.info(f"ğŸš€ Gemini í´ë¼ì´ì–¸íŠ¸ ìµœì í™” ì™„ë£Œ: {self.config.model_version}")
    
    async def analyze_stock(self, prompt: str, use_cache: bool = True) -> Dict[str, Any]:
        """ì£¼ì‹ ë¶„ì„ ìš”ì²­ - ìµœì í™”"""
        cache_key = f"stock_analysis_{hash(prompt)}"
        
        # ìºì‹œ í™•ì¸
        if use_cache:
            cached = self.cache.get(cache_key)
            if cached:
                # í† í° ì ˆì•½ ê³„ì‚°
                estimated_tokens = len(prompt.split()) * 1.3
                self.monitor.record_cache(True, int(estimated_tokens))
                return cached
            self.monitor.record_cache(False)
        
        # AI ë¶„ì„ ì‹¤í–‰
        start_time = time.time()
        try:
            await asyncio.sleep(self.config.request_delay)
            response = await asyncio.to_thread(self.model.generate_content, prompt)
            
            if response.text:
                # ì‘ë‹µ í’ˆì§ˆ í‰ê°€
                quality_score = self._evaluate_response_quality(response.text, prompt)
                
                result = {
                    "analysis": response.text,
                    "timestamp": time.time(),
                    "model": self.config.model_version,
                    "quality_score": quality_score,
                    "token_count": len(response.text.split())
                }
                
                # ìºì‹œ ì €ì¥
                if use_cache:
                    self.cache.set(cache_key, result, quality_score, result["token_count"])
                
                # ì„±ëŠ¥ ê¸°ë¡
                duration = time.time() - start_time
                self.monitor.record_request(duration, True, len(response.text.split()), quality_score)
                
                return result
            else:
                raise Exception("ì‘ë‹µì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                
        except Exception as e:
            duration = time.time() - start_time
            self.monitor.record_request(duration, False)
            logger.error(f"Gemini ë¶„ì„ ì˜¤ë¥˜: {e}")
            raise
    
    def _evaluate_response_quality(self, response: str, prompt: str) -> float:
        """ì‘ë‹µ í’ˆì§ˆ í‰ê°€ - 0-10ì """
        try:
            score = 5.0  # ê¸°ë³¸ ì ìˆ˜
            
            # ê¸¸ì´ í‰ê°€ (ì ì ˆí•œ ê¸¸ì´)
            if 500 <= len(response) <= 3000:
                score += 1.0
            elif len(response) < 200:
                score -= 2.0
            
            # êµ¬ì¡°í™” í‰ê°€ (JSON, ì„¹ì…˜ êµ¬ë¶„ ë“±)
            if any(marker in response for marker in ['{', '}', '1.', '2.', '##', '**']):
                score += 1.0
            
            # íˆ¬ì ê´€ë ¨ í‚¤ì›Œë“œ í¬í•¨ ì—¬ë¶€
            investment_keywords = ['íˆ¬ì', 'ìˆ˜ìµë¥ ', 'PER', 'PBR', 'ROE', 'ë§¤ì¶œ', 'ìˆœì´ìµ', 'ë¦¬ìŠ¤í¬', 'ì¶”ì²œ']
            keyword_count = sum(1 for keyword in investment_keywords if keyword in response)
            score += min(keyword_count * 0.3, 2.0)
            
            # ìˆ˜ì¹˜ ì •ë³´ í¬í•¨ ì—¬ë¶€ (êµ¬ì²´ì ì¸ ë¶„ì„)
            import re
            numbers = re.findall(r'\d+\.?\d*%?', response)
            if len(numbers) >= 5:
                score += 1.0
            
            return min(score, 10.0)
        except:
            return 5.0
    
    async def batch_analyze(self, prompts: List[str], use_cache: bool = True) -> List[Dict[str, Any]]:
        """ë°°ì¹˜ ë¶„ì„ - ìµœì í™”"""
        semaphore = asyncio.Semaphore(self.config.max_concurrent)
        
        async def analyze_with_semaphore(prompt: str):
            async with semaphore:
                return await self.analyze_stock(prompt, use_cache)
        
        # ëª¨ë“  í”„ë¡¬í”„íŠ¸ë¥¼ ë³‘ë ¬ ì²˜ë¦¬
        tasks = [analyze_with_semaphore(prompt) for prompt in prompts]
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # ì˜ˆì™¸ ì²˜ë¦¬
        processed_results = []
        for i, result in enumerate(results):
            if isinstance(result, Exception):
                logger.error(f"ë°°ì¹˜ ë¶„ì„ ì˜¤ë¥˜ (ì¸ë±ìŠ¤ {i}): {result}")
                processed_results.append({
                    "error": str(result),
                    "timestamp": time.time(),
                    "model": self.config.model_version
                })
            else:
                processed_results.append(result)
        
        return processed_results
    
    def get_performance_stats(self) -> Dict[str, Any]:
        """ì„±ëŠ¥ í†µê³„ ì¡°íšŒ"""
        return self.monitor.get_stats()
    
    def clear_cache(self):
        """ìºì‹œ ì´ˆê¸°í™”"""
        self.cache = GeminiSmartCache()
        logger.info("ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    async def __aenter__(self):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì§„ì…"""
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """ë¹„ë™ê¸° ì»¨í…ìŠ¤íŠ¸ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        # ì •ë¦¬ ì‘ì—…
        pass

# í¸ì˜ í•¨ìˆ˜ë“¤
async def create_optimized_client(api_key: Optional[str] = None) -> GeminiClient:
    """ìµœì í™”ëœ Gemini í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    if api_key is None:
        api_key = os.getenv('GEMINI_API_KEY')
    
    # í™˜ê²½ ë³€ìˆ˜ì—ì„œ ì„¤ì • ì½ê¸° (ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
    model_version = os.getenv('GEMINI_MODEL', 'gemini-1.5-flash')
    temperature = float(os.getenv('GEMINI_TEMPERATURE', '0.05'))
    max_tokens = int(os.getenv('GEMINI_MAX_TOKENS', str(MAX_TOKENS)))
    
    config = GeminiConfig(
        api_key=api_key,
        model_version=model_version,        # .envì—ì„œ ì½ì€ ëª¨ë¸
        temperature=temperature,            # .envì—ì„œ ì½ì€ ì˜¨ë„
        max_output_tokens=max_tokens,       # .envì—ì„œ ì½ì€ í† í° ìˆ˜
        max_concurrent=100,                 # ë™ì‹œ ìš”ì²­ ìµœëŒ€í™”
        batch_size=50,                     # ë°°ì¹˜ í¬ê¸° ìµœì í™”
        request_delay=0.005,               # ì§€ì—° ìµœì†Œí™”
        retry_attempts=15                  # ì¬ì‹œë„ ìµœëŒ€í™”
    )
    
    return GeminiClient(config)

def create_expert_prompt(stock_data: Dict[str, Any], analysis_type: str = "comprehensive") -> str:
    """ì „ë¬¸ê°€ ìˆ˜ì¤€ í”„ë¡¬í”„íŠ¸ ìƒì„±"""
    
    base_prompt = f"""
# ì£¼ì‹ íˆ¬ì ë¶„ì„ ìš”ì²­

## ì¢…ëª© ì •ë³´
- ì¢…ëª©ëª…: {stock_data.get('name', 'N/A')} ({stock_data.get('symbol', 'N/A')})
- í˜„ì¬ê°€: {stock_data.get('price', 0):,.0f}ì›
- ì‹œê°€ì´ì•¡: {stock_data.get('market_cap', 0):,.0f}ì›
- ì„¹í„°: {stock_data.get('sector', 'N/A')}

## ì¬ë¬´ ì§€í‘œ
- PER: {stock_data.get('pe_ratio', 'N/A')}
- PBR: {stock_data.get('pb_ratio', 'N/A')}
- ROE: {stock_data.get('roe', 'N/A')}%
- ë¶€ì±„ë¹„ìœ¨: {stock_data.get('debt_ratio', 'N/A')}%
- ìœ ë™ë¹„ìœ¨: {stock_data.get('current_ratio', 'N/A')}%

## ë¶„ì„ ìš”ì²­
ë‹¤ìŒ í˜•ì‹ì˜ JSONìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”:

{{
  "investment_grade": "íˆ¬ìë“±ê¸‰ (S/A+/A/B+/B/C+/C/D)",
  "investment_score": "íˆ¬ìì ìˆ˜ (0-100)",
  "target_price": "ëª©í‘œê°€ê²© (12ê°œì›”)",
  "upside_potential": "ìƒìŠ¹ì—¬ë ¥ (%)",
  "strengths": ["ê°•ì 1", "ê°•ì 2", "ê°•ì 3"],
  "weaknesses": ["ì•½ì 1", "ì•½ì 2", "ì•½ì 3"],
  "investment_strategy": "íˆ¬ìì „ëµ ì¶”ì²œ",
  "risk_factors": ["ë¦¬ìŠ¤í¬1", "ë¦¬ìŠ¤í¬2", "ë¦¬ìŠ¤í¬3"],
  "sector_outlook": "ì„¹í„° ì „ë§",
  "recommendation": "ìµœì¢… ì¶”ì²œ (ë§¤ìˆ˜/ë³´ìœ /ë§¤ë„)",
  "confidence_level": "ì‹ ë¢°ë„ (1-10)",
  "analysis_summary": "ì¢…í•© ë¶„ì„ ìš”ì•½"
}}

ì„¸ê³„ ìµœê³  ìˆ˜ì¤€ì˜ íˆ¬ì ì „ë¬¸ê°€ë¡œì„œ ì •í™•í•˜ê³  ì‹¤ìš©ì ì¸ ë¶„ì„ì„ ì œê³µí•´ì£¼ì„¸ìš”.
"""
    
    return base_prompt.strip() 