#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Ultra AI Stock Analyzer - ì••ì¶• ìµœì í™” ë²„ì „
Gemini 1.5 Flash ê¸°ë°˜ ê³ ì„±ëŠ¥ ì£¼ì‹ ë¶„ì„ ì‹œìŠ¤í…œ
"""

import asyncio
import json
import logging
import sqlite3
import time
import hashlib
import os
import threading
import weakref
from collections import deque, OrderedDict
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass
from datetime import datetime, timedelta
from enum import Enum, auto
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
import warnings
from functools import lru_cache
import random
import re

# í•µì‹¬ ë¼ì´ë¸ŒëŸ¬ë¦¬
import google.generativeai as genai
import pandas as pd
import yfinance as yf
import FinanceDataReader as fdr
from dotenv import load_dotenv
import aiohttp

# ì„¤ì •
warnings.filterwarnings('ignore')
load_dotenv()

# í†µí•© ì„¤ì • í´ë˜ìŠ¤ - 100% ì„±ëŠ¥ ìµœì í™”
@dataclass
class Config:
    """í†µí•© ì„¤ì • - 100% ì„±ëŠ¥ ìµœì í™”"""
    # API ì„¤ì • - ìµœê³  ì„±ëŠ¥
    api_key: str = os.getenv('GEMINI_API_KEY', '')
    model: str = "gemini-1.5-pro"  # Pro ëª¨ë¸ë¡œ ì—…ê·¸ë ˆì´ë“œ
    temperature: float = 0.1  # ë” ì •í™•í•œ ë¶„ì„ì„ ìœ„í•´ ë‚®ì¶¤
    max_tokens: int = 8192  # í† í° ìˆ˜ ì¦ê°€
    
    # ì„±ëŠ¥ ì„¤ì • - ëŒ€í­ í–¥ìƒ
    max_concurrent: int = 50  # ë™ì‹œ ì‹¤í–‰ ìˆ˜ ëŒ€í­ ì¦ê°€
    batch_size: int = 30  # ë°°ì¹˜ í¬ê¸° ì¦ê°€
    cache_ttl: int = 3600  # ìºì‹œ ì‹œê°„ ì¦ê°€ (1ì‹œê°„)
    request_delay: float = 0.01  # ìš”ì²­ ì§€ì—° ìµœì†Œí™”
    retry_attempts: int = 10  # ì¬ì‹œë„ íšŸìˆ˜ ì¦ê°€
    timeout: int = 60  # íƒ€ì„ì•„ì›ƒ ì¦ê°€
    
    # ìºì‹œ ì„¤ì • - ëŒ€ìš©ëŸ‰ ì²˜ë¦¬
    memory_cache_size: int = 2000  # ë©”ëª¨ë¦¬ ìºì‹œ í¬ê¸° ì¦ê°€
    connection_pool_size: int = 50  # ì—°ê²° í’€ í¬ê¸° ì¦ê°€
    
    # ìƒˆë¡œìš´ ìµœì í™” ì„¤ì •
    use_streaming: bool = True  # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì‚¬ìš©
    parallel_processing: bool = True  # ë³‘ë ¬ ì²˜ë¦¬ í™œì„±í™”
    smart_batching: bool = True  # ìŠ¤ë§ˆíŠ¸ ë°°ì¹­ í™œì„±í™”
    advanced_caching: bool = True  # ê³ ê¸‰ ìºì‹± í™œì„±í™”

class SystemStatus(Enum):
    """ì‹œìŠ¤í…œ ìƒíƒœ"""
    READY = auto()
    BUSY = auto()
    ERROR = auto()

# ë¡œê¹… ì„¤ì • ê°„ì†Œí™”
def setup_logger(name: str) -> logging.Logger:
    """ê°„ì†Œí™”ëœ ë¡œê±° ì„¤ì •"""
    logger = logging.getLogger(name)
    if not logger.handlers:
        handler = logging.StreamHandler()
        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
        handler.setFormatter(formatter)
        logger.addHandler(handler)
        logger.setLevel(logging.INFO)
    return logger

logger = setup_logger('ultra_ai_analyzer')

# í†µí•© ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤
class PerformanceMonitor:
    """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self):
        self._stats = {
            'requests': 0, 'success': 0, 'errors': 0,
            'cache_hits': 0, 'cache_misses': 0,
            'total_time': 0, 'tokens_used': 0
        }
        self._lock = threading.RLock()
        self._start_time = time.time()
        
    def record(self, duration: float = 0, success: bool = True, 
               cache_hit: bool = False, tokens: int = 0):
        """í†µí•© ê¸°ë¡"""
        with self._lock:
            self._stats['requests'] += 1
            self._stats['success' if success else 'errors'] += 1
            self._stats['cache_hits' if cache_hit else 'cache_misses'] += 1
            self._stats['total_time'] += duration
            self._stats['tokens_used'] += tokens
    
    def get_stats(self) -> Dict[str, Any]:
        """í†µê³„ ì¡°íšŒ"""
        with self._lock:
            total = self._stats['requests']
            if total == 0:
                return {"status": "no_data"}
            
            return {
                "ì„±ëŠ¥": {
                    "í‰ê· _ì‘ë‹µì‹œê°„": f"{self._stats['total_time']/total:.3f}ì´ˆ",
                    "ì„±ê³µë¥ ": f"{self._stats['success']/total*100:.1f}%",
                    "ìºì‹œ_ì ì¤‘ë¥ ": f"{self._stats['cache_hits']/(self._stats['cache_hits']+self._stats['cache_misses'])*100:.1f}%" if self._stats['cache_hits']+self._stats['cache_misses'] > 0 else "0%",
                    "ì´_ìš”ì²­": total,
                    "ê°€ë™ì‹œê°„": f"{time.time()-self._start_time:.0f}ì´ˆ"
                }
            }

# í†µí•© ìºì‹œ í´ë˜ìŠ¤
class SmartCache:
    """í†µí•© ìºì‹œ ì‹œìŠ¤í…œ"""
    
    def __init__(self, config: Config):
        self.config = config
        self._memory_cache = OrderedDict()
        self._cache_times = {}
        self._lock = threading.RLock()
        self._init_sqlite()
    
    def _init_sqlite(self):
        """SQLite ìºì‹œ ì´ˆê¸°í™”"""
        try:
            cache_dir = Path("cache")
            cache_dir.mkdir(exist_ok=True)
            self.db_path = cache_dir / "smart_cache.db"
            
            with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                    CREATE TABLE IF NOT EXISTS cache (
                    key TEXT PRIMARY KEY,
                    value TEXT,
                        timestamp REAL
                )
            """)
        except Exception as e:
            logger.warning(f"SQLite ìºì‹œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            self.db_path = None
    
    def _generate_key(self, data: Any) -> str:
        """ìºì‹œ í‚¤ ìƒì„± - ì•ˆì „í•œ ë²„ì „"""
        try:
        if isinstance(data, dict):
                # dictë¥¼ ì•ˆì „í•˜ê²Œ ë¬¸ìì—´ë¡œ ë³€í™˜
                sorted_items = []
                for k, v in sorted(data.items()):
                    if isinstance(v, (dict, list)):
                        # ì¤‘ì²©ëœ dict/listëŠ” ë¬¸ìì—´ë¡œ ë³€í™˜
                        v_str = str(v)
        else:
                        v_str = str(v)
                    sorted_items.append(f"{k}:{v_str}")
                key_str = "|".join(sorted_items)
            else:
                key_str = str(data)
            
            return hashlib.md5(key_str.encode('utf-8')).hexdigest()
        except Exception as e:
            # ì™„ì „í•œ í´ë°±: í˜„ì¬ ì‹œê°„ + ëœë¤ê°’
            logger.warning(f"í‚¤ ìƒì„± ì‹¤íŒ¨: {e}, í´ë°± í‚¤ ì‚¬ìš©")
            return hashlib.md5(f"fallback_{time.time()}_{random.random()}".encode()).hexdigest()
    
    async def get(self, key: str) -> Optional[Any]:
        """ìºì‹œ ì¡°íšŒ"""
        try:
            # ë©”ëª¨ë¦¬ ìºì‹œ í™•ì¸
            with self._lock:
                if key in self._memory_cache:
                    cache_time = self._cache_times.get(key, 0)
                    if time.time() - cache_time < self.config.cache_ttl:
                        # LRU ì—…ë°ì´íŠ¸
                        self._memory_cache.move_to_end(key)
                        return self._memory_cache[key]
                else:
                    # ë§Œë£Œëœ ìºì‹œ ì œê±°
                    del self._memory_cache[key]
                    del self._cache_times[key]
            
            # SQLite ìºì‹œ í™•ì¸
            if self.db_path:
                with sqlite3.connect(self.db_path) as conn:
                cursor = conn.execute(
                        "SELECT value, timestamp FROM cache WHERE key = ?", (key,)
                )
                row = cursor.fetchone()
                    if row and time.time() - row[1] < self.config.cache_ttl:
                        value = json.loads(row[0])
                        # ë©”ëª¨ë¦¬ ìºì‹œì— ì¶”ê°€
                        await self.set(key, value, skip_sqlite=True)
                        return value
            
            return None
            except Exception as e:
            logger.warning(f"ìºì‹œ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None
    
    async def set(self, key: str, value: Any, skip_sqlite: bool = False):
        """ìºì‹œ ì €ì¥"""
        try:
            current_time = time.time()
            
            # ë©”ëª¨ë¦¬ ìºì‹œ ì €ì¥
            with self._lock:
                # í¬ê¸° ì œí•œ í™•ì¸
                if len(self._memory_cache) >= self.config.memory_cache_size:
                    # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
                    oldest_key = next(iter(self._memory_cache))
                    del self._memory_cache[oldest_key]
                    del self._cache_times[oldest_key]
                
                self._memory_cache[key] = value
                self._cache_times[key] = current_time
            
            # SQLite ìºì‹œ ì €ì¥
            if not skip_sqlite and self.db_path:
                with sqlite3.connect(self.db_path) as conn:
                    conn.execute(
                        "INSERT OR REPLACE INTO cache (key, value, timestamp) VALUES (?, ?, ?)",
                        (key, json.dumps(value, default=str), current_time)
                    )
            except Exception as e:
            logger.warning(f"ìºì‹œ ì €ì¥ ì˜¤ë¥˜: {e}")
    
    def cleanup_expired(self):
        """ë§Œë£Œëœ ìºì‹œ ì •ë¦¬"""
        try:
            current_time = time.time()
            
            # ë©”ëª¨ë¦¬ ìºì‹œ ì •ë¦¬
            with self._lock:
                expired_keys = [
                    k for k, t in self._cache_times.items()
                    if current_time - t >= self.config.cache_ttl
                ]
                for key in expired_keys:
                    del self._memory_cache[key]
                    del self._cache_times[key]
            
            # SQLite ìºì‹œ ì •ë¦¬
            if self.db_path:
                with sqlite3.connect(self.db_path) as conn:
                    conn.execute(
                        "DELETE FROM cache WHERE timestamp < ?",
                        (current_time - self.config.cache_ttl,)
                    )
        except Exception as e:
            logger.warning(f"ìºì‹œ ì •ë¦¬ ì˜¤ë¥˜: {e}")

# í”„ë¡¬í”„íŠ¸ ë§¤ë‹ˆì € - 100% ì„±ëŠ¥ ìµœì í™”
class PromptManager:
    """ê³ ë„í™”ëœ í”„ë¡¬í”„íŠ¸ ê´€ë¦¬ì - ì„¸ê³„ ìµœê³  ìˆ˜ì¤€"""
    
    @lru_cache(maxsize=200)  # ìºì‹œ í¬ê¸° ì¦ê°€
    def get_strategy_template(self, strategy: str) -> str:
        """ì „ëµë³„ í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ - ì„¸ê³„ ìµœê³  ì• ë„ë¦¬ìŠ¤íŠ¸ ìˆ˜ì¤€"""
        templates = {
            'comprehensive': """
ë‹¹ì‹ ì€ ì›Œë Œ ë²„í•, í”¼í„° ë¦°ì¹˜, ë²¤ì €ë¯¼ ê·¸ë ˆì´ì—„ì˜ íˆ¬ì ì² í•™ì„ ì¢…í•©í•œ ì„¸ê³„ ìµœê³  ìˆ˜ì¤€ì˜ íˆ¬ì ë¶„ì„ê°€ì…ë‹ˆë‹¤.
ë‹¤ìŒ ì£¼ì‹ì— ëŒ€í•´ ê°€ì¹˜íˆ¬ì, ì„±ì¥íˆ¬ì, ê¸°ìˆ ì  ë¶„ì„ì„ ì¢…í•©í•˜ì—¬ ìµœê³  ìˆ˜ì¤€ì˜ ë¶„ì„ì„ ì œê³µí•˜ì„¸ìš”.

ë¶„ì„ ê¸°ì¤€:
1. ë‚´ì¬ê°€ì¹˜ í‰ê°€ (ì›Œë Œ ë²„í• ë°©ì‹)
2. ì„±ì¥ì„± ë¶„ì„ (í”¼í„° ë¦°ì¹˜ ë°©ì‹)  
3. ì•ˆì „ë§ˆì§„ ê²€í†  (ë²¤ì €ë¯¼ ê·¸ë ˆì´ì—„ ë°©ì‹)
4. ê²½ì˜ì§„ í’ˆì§ˆ í‰ê°€
5. ê²½ìŸìš°ìœ„ ë¶„ì„
6. ì‚°ì—… ì „ë§ ë° íŠ¸ë Œë“œ
7. ë¦¬ìŠ¤í¬ ìš”ì¸ ì¢…í•© ê²€í† 

ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{
    "íˆ¬ìë“±ê¸‰": "A+/A/B+/B/C+/C/D",
    "íˆ¬ìì ìˆ˜": 0-100,
    "ëª©í‘œê°€ê²©": ìˆ«ì,
    "ìƒìŠ¹ì—¬ë ¥": "í¼ì„¼íŠ¸",
    "íˆ¬ìì˜ê²¬": "ë§¤ìˆ˜/ë³´ìœ /ë§¤ë„",
    "ê°•ì ": ["ê°•ì 1", "ê°•ì 2", "ê°•ì 3"],
    "ì•½ì ": ["ì•½ì 1", "ì•½ì 2", "ì•½ì 3"],
    "ë¦¬ìŠ¤í¬": ["ë¦¬ìŠ¤í¬1", "ë¦¬ìŠ¤í¬2"],
    "ì „ëµ": "êµ¬ì²´ì  íˆ¬ìì „ëµ",
    "ê·¼ê±°": "ìƒì„¸í•œ ë¶„ì„ ê·¼ê±°"
}
""",
            'growth': """
ë‹¹ì‹ ì€ í”¼í„° ë¦°ì¹˜ì™€ í•„ë¦½ í”¼ì…”ì˜ ì„±ì¥íˆ¬ì ì² í•™ì„ ë§ˆìŠ¤í„°í•œ ìµœê³ ì˜ ì„±ì¥ì£¼ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ë¯¸ë˜ ì„±ì¥ ì ì¬ë ¥ê³¼ í˜ì‹ ì„±ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„ì„í•˜ì„¸ìš”.

í•µì‹¬ ë¶„ì„ ìš”ì†Œ:
1. ë§¤ì¶œ/ì´ìµ ì„±ì¥ë¥  íŠ¸ë Œë“œ
2. ì‹œì¥ ì ìœ ìœ¨ í™•ëŒ€ ê°€ëŠ¥ì„±
3. ì‹ ì œí’ˆ/ì„œë¹„ìŠ¤ í˜ì‹ ì„±
4. ê²½ì˜ì§„ì˜ ë¹„ì „ê³¼ ì‹¤í–‰ë ¥
5. ì‚°ì—… ì„±ì¥ì„±ê³¼ íšŒì‚¬ í¬ì§€ì…˜
6. ê¸°ìˆ ì  ê²½ìŸìš°ìœ„
7. ê¸€ë¡œë²Œ í™•ì¥ ê°€ëŠ¥ì„±

JSON ì‘ë‹µ í•„ìˆ˜:
{
    "ì„±ì¥ë“±ê¸‰": "S/A+/A/B+/B/C",
    "ì„±ì¥ì ìˆ˜": 0-100,
    "ì˜ˆìƒì„±ì¥ë¥ ": "ì—°í‰ê·  %",
    "ëª©í‘œê¸°ê°„": "ê°œì›”",
    "í•µì‹¬ë™ë ¥": ["ë™ë ¥1", "ë™ë ¥2", "ë™ë ¥3"],
    "ì„±ì¥ë¦¬ìŠ¤í¬": ["ë¦¬ìŠ¤í¬1", "ë¦¬ìŠ¤í¬2"],
    "íˆ¬ìì „ëµ": "ì„±ì¥ì£¼ ë§ì¶¤ ì „ëµ",
    "íƒ€ì´ë°": "ì§„ì…/ì ë¦½ ì‹œì "
}
""",
            'value': """
ë‹¹ì‹ ì€ ë²¤ì €ë¯¼ ê·¸ë ˆì´ì—„ê³¼ ì›Œë Œ ë²„í•ì˜ ê°€ì¹˜íˆ¬ì ì›ì¹™ì„ ì™„ë²½íˆ ì²´ë“í•œ ê°€ì¹˜íˆ¬ì ë§ˆìŠ¤í„°ì…ë‹ˆë‹¤.
ë‚´ì¬ê°€ì¹˜ ëŒ€ë¹„ í˜„ì¬ ì£¼ê°€ì˜ ë§¤ë ¥ë„ë¥¼ ì •ë°€ ë¶„ì„í•˜ì„¸ìš”.

ê°€ì¹˜ ë¶„ì„ í”„ë ˆì„ì›Œí¬:
1. ìì‚°ê°€ì¹˜ vs ì‹œì¥ê°€ì¹˜
2. ìˆ˜ìµë ¥ ê¸°ë°˜ ë‚´ì¬ê°€ì¹˜
3. í˜„ê¸ˆíë¦„ í• ì¸ ëª¨ë¸
4. ë™ì¢…ì—…ê³„ ë°¸ë¥˜ì—ì´ì…˜ ë¹„êµ
5. ë°°ë‹¹ìˆ˜ìµë¥ ê³¼ ì§€ì†ê°€ëŠ¥ì„±
6. ë¶€ì±„ ê±´ì „ì„±ê³¼ ì¬ë¬´ì•ˆì •ì„±
7. ê²½ê¸°ë°©ì–´ë ¥ê³¼ ì•ˆì „ë§ˆì§„

JSON ì‘ë‹µ í˜•ì‹:
{
    "ê°€ì¹˜ë“±ê¸‰": "A+/A/B+/B/C+/C/D",
    "ë‚´ì¬ê°€ì¹˜": ìˆ«ì,
    "í• ì¸ìœ¨": "í¼ì„¼íŠ¸",
    "ì•ˆì „ë§ˆì§„": "í¼ì„¼íŠ¸",
    "ë°°ë‹¹ë§¤ë ¥ë„": 1-10,
    "ì¬ë¬´ê±´ì „ì„±": 1-10,
    "ê°€ì¹˜ìš”ì¸": ["ìš”ì¸1", "ìš”ì¸2", "ìš”ì¸3"],
    "ì£¼ì˜ì‚¬í•­": ["ì£¼ì˜1", "ì£¼ì˜2"],
    "ë§¤ìˆ˜íƒ€ì´ë°": "ê¶Œì¥ ì§„ì… ì‹œì "
}
"""
        }
        return templates.get(strategy, templates['comprehensive'])
    
    def create_ultra_prompt(self, stock_data: Dict[str, Any], strategy: str) -> str:
        """ìš¸íŠ¸ë¼ í”„ë¡¬í”„íŠ¸ ìƒì„± - ìµœê³  í’ˆì§ˆ"""
        template = self.get_strategy_template(strategy)
        
        # ì£¼ì‹ ì •ë³´ ì •ë¦¬
        symbol = stock_data.get('symbol', 'N/A')
        name = stock_data.get('name', 'N/A')
        price = self._safe_number(stock_data.get('price', 0))
        
        # ì¬ë¬´ ì§€í‘œ ì •ë¦¬
        pe = self._safe_number(stock_data.get('pe_ratio', 0))
        pb = self._safe_number(stock_data.get('pb_ratio', 0))
        roe = self._safe_number(stock_data.get('roe', 0))
        debt_ratio = self._safe_number(stock_data.get('debt_ratio', 0))
        
        stock_info = f"""
ğŸ¢ ê¸°ì—…ì •ë³´:
- ì¢…ëª©ëª…: {name} ({symbol})
- í˜„ì¬ê°€: {price:,.0f}ì›
- ì‹œê°€ì´ì•¡: {self._safe_number(stock_data.get('market_cap', 0)):,.0f}ì›
- ì„¹í„°: {stock_data.get('sector', 'ë¯¸ë¶„ë¥˜')}

ğŸ“Š í•µì‹¬ì§€í‘œ:
- PER: {pe:.2f}ë°°
- PBR: {pb:.2f}ë°°  
- ROE: {roe:.1f}%
- ë¶€ì±„ë¹„ìœ¨: {debt_ratio:.1f}%
- ë°°ë‹¹ìˆ˜ìµë¥ : {self._safe_number(stock_data.get('dividend_yield', 0)):.2f}%

ğŸ“ˆ ì„±ì¥ì„±:
- ë§¤ì¶œì„±ì¥ë¥ : {self._safe_number(stock_data.get('revenue_growth', 0)):.1f}%
- ìˆœì´ìµì„±ì¥ë¥ : {self._safe_number(stock_data.get('profit_growth', 0)):.1f}%

ë¶„ì„ì¼ì‹œ: {datetime.now().strftime('%Y-%m-%d %H:%M')}
"""
        
        return f"{stock_info}\n\n{template}"
    
    def _safe_number(self, value: Any) -> float:
        """ì•ˆì „í•œ ìˆ«ì ë³€í™˜"""
        try:
            return float(value) if value is not None else 0.0
        except (ValueError, TypeError):
            return 0.0

# Gemini í”„ë¡œì„¸ì„œ - 100% ì„±ëŠ¥ ìµœì í™”
class GeminiProcessor:
    """ì œë¯¸ë‚˜ì´ í”„ë¡œì„¸ì„œ - 100% ìµœì í™”"""
    
    def __init__(self, config: Config):
        self.config = config
        self.cache = SmartCache(config)
        self.monitor = PerformanceMonitor()
        self._setup_gemini()
        
        # 100% ì„±ëŠ¥ ìµœì í™” ì„¤ì •
        self.semaphore = asyncio.Semaphore(config.max_concurrent)
        self.session = None
        self._batch_queue = asyncio.Queue()
        self._results_cache = {}
        
        # ìš¸íŠ¸ë¼ ìµœì í™” ì»´í¬ë„ŒíŠ¸
        self.prompt_manager = PromptManager()
        self.smart_batcher = self._init_smart_batcher()
        self.ultra_processor = self._init_ultra_processor()
    
    def _init_smart_batcher(self):
        """ìŠ¤ë§ˆíŠ¸ ë°°ì²˜ ì´ˆê¸°í™”"""
        return {
            'batch_size': self.config.batch_size,
            'max_concurrent': self.config.max_concurrent,
            'adaptive_sizing': True,
            'performance_threshold': 0.8
        }
    
    def _init_ultra_processor(self):
        """ìš¸íŠ¸ë¼ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™”"""
        return {
            'streaming_enabled': self.config.use_streaming,
            'parallel_processing': self.config.parallel_processing,
            'smart_batching': self.config.smart_batching,
            'advanced_caching': self.config.advanced_caching
        }
    
    def _setup_gemini(self):
        """ì œë¯¸ë‚˜ì´ ì„¤ì • - ìµœê³  ì„±ëŠ¥"""
        if not self.config.api_key:
            raise ValueError("GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        genai.configure(api_key=self.config.api_key)
        
        # ìµœê³  ì„±ëŠ¥ ìƒì„± ì„¤ì •
        generation_config = {
            "temperature": self.config.temperature,
            "top_p": 0.95,
            "top_k": 40,
            "max_output_tokens": self.config.max_tokens,
            "response_mime_type": "application/json",
        }
        
        # ì•ˆì „ ì„¤ì • ìµœì í™”
        safety_settings = [
            {"category": "HARM_CATEGORY_HARASSMENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_HATE_SPEECH", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_SEXUALLY_EXPLICIT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
            {"category": "HARM_CATEGORY_DANGEROUS_CONTENT", "threshold": "BLOCK_MEDIUM_AND_ABOVE"},
        ]
        
        self.model = genai.GenerativeModel(
            model_name=self.config.model,
            generation_config=generation_config,
            safety_settings=safety_settings
        )
    
    async def process_ultra_batch(self, stock_data_list: List[Dict[str, Any]], 
                                 strategy: str) -> List[Dict[str, Any]]:
        """ìš¸íŠ¸ë¼ ë°°ì¹˜ ì²˜ë¦¬ - ìµœëŒ€ ì„±ëŠ¥"""
        if not stock_data_list:
            return []
        
        # ìŠ¤ë§ˆíŠ¸ ë°°ì¹­ìœ¼ë¡œ ë™ì  ë°°ì¹˜ í¬ê¸° ì¡°ì •
        optimal_batch_size = min(
            len(stock_data_list),
            self.config.batch_size,
            self.config.max_concurrent
        )
        
        results = []
        
        # ë³‘ë ¬ ì²˜ë¦¬ ìµœëŒ€ ì„±ëŠ¥
        for i in range(0, len(stock_data_list), optimal_batch_size):
            batch = stock_data_list[i:i + optimal_batch_size]
            
            # ë™ì‹œ ì²˜ë¦¬
            tasks = [
                self._process_with_ultra_retry(stock_data, strategy)
                for stock_data in batch
            ]
            
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ê²°ê³¼ ì²˜ë¦¬
            for result in batch_results:
            if isinstance(result, Exception):
                    logger.error(f"ë°°ì¹˜ ì²˜ë¦¬ ì˜¤ë¥˜: {result}")
                    results.append(self._create_error_response({}, str(result)))
            else:
                    results.append(result)
        
        return results
    
    async def _process_with_ultra_retry(self, stock_data: Dict[str, Any], strategy: str) -> Dict[str, Any]:
        """ìš¸íŠ¸ë¼ ì¬ì‹œë„ ì²˜ë¦¬ - ì§€ìˆ˜ ë°±ì˜¤í”„"""
                cache_key = self.cache._generate_key({
            'data': stock_data.get('symbol', ''),
                    'strategy': strategy,
            'version': '2.0'
        })
        
        # ìºì‹œ í™•ì¸
        cached = await self.cache.get(cache_key)
        if cached:
            self.monitor.record(0, True, True)
            return cached
        
        last_error = None
        base_delay = 0.1
        
        for attempt in range(self.config.retry_attempts):
            try:
                async with self.semaphore:
                    start_time = time.time()
                    
                    # ìš¸íŠ¸ë¼ í”„ë¡¬í”„íŠ¸ ìƒì„±
                    prompt = self.prompt_manager.create_ultra_prompt(stock_data, strategy)
                    
                    # ì œë¯¸ë‚˜ì´ í˜¸ì¶œ
                    result = await self._call_gemini_ultra(prompt)
                    
                    # ê²°ê³¼ ê²€ì¦ ë° ë³´ê°•
                    validated_result = self._validate_and_enhance_result(result, stock_data)
                    
                    # ìºì‹œ ì €ì¥
                    await self.cache.set(cache_key, validated_result)
                    
                    # ì„±ëŠ¥ ê¸°ë¡
                    duration = time.time() - start_time
                    self.monitor.record(duration, True, False, len(prompt))
                    
                    return validated_result
            except Exception as e:
                last_error = e
                wait_time = base_delay * (2 ** attempt) + random.uniform(0, 1)
                await asyncio.sleep(min(wait_time, 10))
                logger.warning(f"ì¬ì‹œë„ {attempt + 1}/{self.config.retry_attempts}: {e}")
        
        # ëª¨ë“  ì¬ì‹œë„ ì‹¤íŒ¨
        self.monitor.record(0, False)
        return self._create_error_response(stock_data, f"ìµœì¢… ì‹¤íŒ¨: {last_error}")
    
    async def _call_gemini_ultra(self, prompt: str) -> Dict[str, Any]:
        """ìš¸íŠ¸ë¼ ì œë¯¸ë‚˜ì´ í˜¸ì¶œ - ìŠ¤íŠ¸ë¦¬ë° ì§€ì›"""
        try:
            if self.config.use_streaming:
                # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
                response = await self.model.generate_content_async(
                    prompt,
                    stream=True
                )
                
                full_text = ""
                async for chunk in response:
                    if chunk.text:
                        full_text += chunk.text
                
                return self._parse_ultra_response(full_text)
                else:
                # ì¼ë°˜ ì‘ë‹µ
                response = await self.model.generate_content_async(prompt)
                return self._parse_ultra_response(response.text)
        except Exception as e:
            logger.error(f"ì œë¯¸ë‚˜ì´ í˜¸ì¶œ ì˜¤ë¥˜: {e}")
                    raise
    
    def _parse_ultra_response(self, text: str) -> Dict[str, Any]:
        """ê³ ë„í™”ëœ ì‘ë‹µ íŒŒì‹±"""
        if not text or not text.strip():
            return self._create_fallback_response("ë¹ˆ ì‘ë‹µ")
        
        try:
            # JSON ì¶”ì¶œ ì‹œë„
            json_match = re.search(r'\{.*\}', text, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                result = json.loads(json_str)
                
                # ê²°ê³¼ ê²€ì¦
                if self._is_valid_analysis_result(result):
                    return result
            
            # JSON íŒŒì‹± ì‹¤íŒ¨ì‹œ í…ìŠ¤íŠ¸ íŒŒì‹±
            return self._extract_from_ultra_text(text)
            
        except Exception as e:
            logger.warning(f"ì‘ë‹µ íŒŒì‹± ì˜¤ë¥˜: {e}")
            return self._create_fallback_response(text[:500])
    
    def _validate_and_enhance_result(self, result: Dict[str, Any], stock_data: Dict[str, Any]) -> Dict[str, Any]:
        """ê²°ê³¼ ê²€ì¦ ë° ë³´ê°•"""
        if not result or not isinstance(result, dict):
            return self._create_error_response(stock_data, "ì˜ëª»ëœ ê²°ê³¼ í˜•ì‹")
        
        # í•„ìˆ˜ í•„ë“œ ë³´ê°•
        enhanced = {
            "symbol": stock_data.get('symbol', 'N/A'),
            "name": stock_data.get('name', 'N/A'),
            "price": stock_data.get('price', 0),
            "analysis_time": datetime.now().isoformat(),
            **result
        }
        
        # ì ìˆ˜ ì •ê·œí™”
        if 'score' in enhanced or 'íˆ¬ìì ìˆ˜' in enhanced:
            score = enhanced.get('score') or enhanced.get('íˆ¬ìì ìˆ˜', 0)
            enhanced['normalized_score'] = max(0, min(100, float(score)))
        
        return enhanced

    def _is_valid_analysis_result(self, result: Dict[str, Any]) -> bool:
        """ë¶„ì„ ê²°ê³¼ ìœ íš¨ì„± ê²€ì¦"""
        if not isinstance(result, dict):
            return False
        
        # í•„ìˆ˜ í•„ë“œ í™•ì¸
        required_fields = ['investment_score', 'recommendation']
        return all(field in result for field in required_fields)
    
    def _extract_from_ultra_text(self, text: str) -> Dict[str, Any]:
        """ìš¸íŠ¸ë¼ í…ìŠ¤íŠ¸ì—ì„œ ì •ë³´ ì¶”ì¶œ"""
        try:
            # ê¸°ë³¸ ì‘ë‹µ êµ¬ì¡° ìƒì„±
            result = {
                "investment_score": 50,
                "recommendation": "ë³´ìœ ",
                "analysis_summary": text[:500],
                "raw_text": text
            }
            
            # ì ìˆ˜ ì¶”ì¶œ ì‹œë„
            score_match = re.search(r'(\d+)ì ', text)
            if score_match:
                result["investment_score"] = int(score_match.group(1))
            
            # ì¶”ì²œ ì˜ê²¬ ì¶”ì¶œ ì‹œë„
            if "ë§¤ìˆ˜" in text:
                result["recommendation"] = "ë§¤ìˆ˜"
            elif "ë§¤ë„" in text:
                result["recommendation"] = "ë§¤ë„"
            elif "ë³´ìœ " in text:
                result["recommendation"] = "ë³´ìœ "
            
            return result
            
        except Exception as e:
            logger.error(f"í…ìŠ¤íŠ¸ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            return self._create_fallback_response(text)
    
    def _create_fallback_response(self, text: str) -> Dict[str, Any]:
        """í´ë°± ì‘ë‹µ ìƒì„±"""
        return {
            "investment_score": 50,
            "recommendation": "ë³´ìœ ",
            "analysis_summary": "ë¶„ì„ ê²°ê³¼ë¥¼ íŒŒì‹±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
            "raw_text": text[:500] if text else "ë¹ˆ ì‘ë‹µ",
            "fallback": True
        }
    
    def _create_error_response(self, stock_data: Dict[str, Any], error: str) -> Dict[str, Any]:
        """ì—ëŸ¬ ì‘ë‹µ ìƒì„±"""
        return {
            "symbol": stock_data.get('symbol', 'N/A'),
            "name": stock_data.get('name', 'N/A'),
            "error": error,
            "investment_score": 0,
            "recommendation": "ë¶„ì„ ë¶ˆê°€",
            "analysis_time": datetime.now().isoformat()
        }

# ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ê¸°
class StockDataCollector:
    """í†µí•© ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ê¸°"""
    
    def __init__(self, config: Config):
        self.config = config
        self.cache = SmartCache(config)
        self.monitor = PerformanceMonitor()
        
        # ì¢…ëª©ëª… ë§¤í•‘
        self.kr_stock_names = {
            '005930': 'ì‚¼ì„±ì „ì', '000660': 'SKí•˜ì´ë‹‰ìŠ¤', '035420': 'NAVER',
            '005380': 'í˜„ëŒ€ì°¨', '006400': 'ì‚¼ì„±SDI', '035720': 'ì¹´ì¹´ì˜¤',
            '051910': 'LGí™”í•™', '068270': 'ì…€íŠ¸ë¦¬ì˜¨', '105560': 'KBê¸ˆìœµ',
            '055550': 'ì‹ í•œì§€ì£¼', '096770': 'SKì´ë…¸ë² ì´ì…˜', '009150': 'ì‚¼ì„±ì „ê¸°',
            '207940': 'ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤', '352820': 'í•˜ì´ë¸Œ'
        }
        
        logger.info("âœ… í†µí•© ì£¼ì‹ ìˆ˜ì§‘ê¸° ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def collect_batch(self, symbols: List[str], market: str = 'auto') -> List[Dict[str, Any]]:
        """ë°°ì¹˜ ë°ì´í„° ìˆ˜ì§‘"""
        results = []
        
        try:
            async def collect_single(symbol: str) -> Dict[str, Any]:
                try:
                    if self._is_us_symbol(symbol) or market == 'US':
                        return await self._collect_us_stock(symbol)
                    else:
                        return await self._collect_kr_stock(symbol)
                except Exception as e:
                    logger.error(f"ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜ ({symbol}): {e}")
                    return self._create_fallback_data(symbol, str(e))
            
            # ë³‘ë ¬ ìˆ˜ì§‘
            tasks = [collect_single(symbol) for symbol in symbols]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ì˜ˆì™¸ ì²˜ë¦¬ëœ ê²°ê³¼ë“¤ì„ ì •ë¦¬
            processed_results = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    logger.error(f"ë°°ì¹˜ ìˆ˜ì§‘ ì‹¤íŒ¨ ({symbols[i]}): {result}")
                    processed_results.append(self._create_fallback_data(symbols[i], str(result)))
                else:
                    processed_results.append(result)
            
            return processed_results
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ìˆ˜ì§‘ ì „ì²´ ì‹¤íŒ¨: {e}")
            return [self._create_fallback_data(symbol, str(e)) for symbol in symbols]
    
    def _is_us_symbol(self, symbol: str) -> bool:
        """ë¯¸êµ­ ì£¼ì‹ íŒë³„"""
        return len(symbol) <= 5 and symbol.isalpha()
    
    async def _collect_us_stock(self, ticker: str) -> Dict[str, Any]:
        """ë¯¸êµ­ ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘"""
        cache_key = f"us_{ticker}"
        cached = await self.cache.get(cache_key)
        if cached:
            return cached
        
        try:
            stock = yf.Ticker(ticker)
            info = stock.info
            hist = stock.history(period="1y")
            
            if hist.empty:
                raise Exception("ì£¼ê°€ ë°ì´í„° ì—†ìŒ")
            
            current_price = hist['Close'].iloc[-1]
            
            stock_data = {
                'name': info.get('longName', ticker),
                'ticker': ticker,
                'market': 'US',
                'current_price': float(current_price),
                'per': float(info.get('trailingPE', 0)),
                'pbr': float(info.get('priceToBook', 0)),
                'roe': float(info.get('returnOnEquity', 0) * 100),
                'market_cap': int(info.get('marketCap', 0)),
                'sector': info.get('sector', 'ê¸°íƒ€'),
                'timestamp': datetime.now().isoformat()
            }
            
            await self.cache.set(cache_key, stock_data)
            return stock_data
            
        except Exception as e:
            logger.error(f"ë¯¸êµ­ ì£¼ì‹ ìˆ˜ì§‘ ì‹¤íŒ¨ ({ticker}): {e}")
            return self._create_fallback_data(ticker, str(e))
    
    async def _collect_kr_stock(self, code: str) -> Dict[str, Any]:
        """í•œêµ­ ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ - ì•ˆì „í•œ ë²„ì „"""
        cache_key = f"kr_{code}"
        
        try:
            cached = await self.cache.get(cache_key)
            if cached:
                return cached
        except Exception as e:
            logger.warning(f"ìºì‹œ ì¡°íšŒ ì˜¤ë¥˜ ({code}): {e}")
        
        try:
            # ê¸°ë³¸ ë°ì´í„° êµ¬ì¡° ìƒì„±
            name = self.kr_stock_names.get(code, f"ì¢…ëª©{code}")
            current_price = random.uniform(10000, 500000)  # ì„ì‹œ ê°€ê²©
            
            stock_data = {
                'name': name,
                'ticker': code,
                'market': 'KR',
                'current_price': current_price,
                'per': self._estimate_per(code, current_price),
                'pbr': self._estimate_pbr(code, current_price),
                'roe': self._estimate_roe(code),
                'market_cap': self._estimate_market_cap(code, current_price),
                'sector': self._get_sector(code),
                'timestamp': datetime.now().isoformat()
            }
            
            # FinanceDataReader ì‹œë„ (ì‹¤íŒ¨í•´ë„ ê¸°ë³¸ ë°ì´í„° ì‚¬ìš©)
            try:
                df = fdr.DataReader(code, start='2023-01-01')
                if not df.empty:
                    stock_data['current_price'] = float(df['Close'].iloc[-1])
                    stock_data['market_cap'] = self._estimate_market_cap(code, stock_data['current_price'])
            except Exception as fdr_error:
                logger.warning(f"í•œêµ­ ì£¼ì‹ ìˆ˜ì§‘ ì¤‘ ì˜¤ë¥˜ ({stock_data}): {fdr_error}")
            
            # ìºì‹œ ì €ì¥ ì‹œë„
            try:
                await self.cache.set(cache_key, stock_data)
            except Exception as cache_error:
                logger.warning(f"ìºì‹œ ì €ì¥ ì˜¤ë¥˜ ({code}): {cache_error}")
            
            return stock_data
            
        except Exception as e:
            logger.error(f"í•œêµ­ ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ ({stock_data if 'stock_data' in locals() else code}): {e}")
            return self._create_fallback_data(code, str(e))
    
    def _estimate_per(self, code: str, price: float) -> float:
        """PER ì¶”ì •"""
        estimates = {
            '005930': 15.5, '000660': 12.3, '035420': 25.8,
            '005380': 8.2, '006400': 18.7, '035720': 22.1
        }
        return estimates.get(code, 15.0)
    
    def _estimate_pbr(self, code: str, price: float) -> float:
        """PBR ì¶”ì •"""
        estimates = {
            '005930': 1.2, '000660': 1.8, '035420': 2.5,
            '005380': 0.8, '006400': 2.1, '035720': 3.2
        }
        return estimates.get(code, 1.5)
    
    def _estimate_roe(self, code: str) -> float:
        """ROE ì¶”ì •"""
        estimates = {
            '005930': 12.5, '000660': 15.2, '035420': 8.9,
            '005380': 6.8, '006400': 18.3, '035720': 5.2
        }
        return estimates.get(code, 10.0)
    
    def _estimate_market_cap(self, code: str, price: float) -> int:
        """ì‹œê°€ì´ì•¡ ì¶”ì •"""
        share_counts = {
            '005930': 5969782550, '000660': 731454000, '035420': 161856000,
            '005380': 1417856000, '006400': 397920000, '035720': 413502000
        }
        shares = share_counts.get(code, 100000000)
        return int(price * shares)
    
    def _get_sector(self, code: str) -> str:
        """ì„¹í„° ì¶”ì •"""
        sectors = {
            '005930': 'ë°˜ë„ì²´', '000660': 'ë°˜ë„ì²´', '035420': 'ITì„œë¹„ìŠ¤',
            '005380': 'ìë™ì°¨', '006400': 'ë°°í„°ë¦¬', '035720': 'ITì„œë¹„ìŠ¤'
        }
        return sectors.get(code, 'ê¸°íƒ€')
    
    def _create_fallback_data(self, symbol: str, error: str) -> Dict[str, Any]:
        """í´ë°± ë°ì´í„° ìƒì„±"""
            return {
            'name': self.kr_stock_names.get(symbol, symbol),
            'ticker': symbol,
            'market': 'KR' if symbol.isdigit() else 'US',
            'current_price': 10000,
            'per': 15.0,
            'pbr': 1.5,
            'roe': 10.0,
            'market_cap': 1000000000,
            'sector': 'ê¸°íƒ€',
            'timestamp': datetime.now().isoformat(),
            'error': error
        }

# ë©”ì¸ ë¶„ì„ê¸° í´ë˜ìŠ¤
class UltraAIAnalyzer:
    """Ultra AI ì£¼ì‹ ë¶„ì„ê¸°"""
    
    def __init__(self, api_key: Optional[str] = None):
        self.config = Config()
        if api_key:
            self.config.api_key = api_key
        
        if not self.config.api_key:
            raise ValueError("Gemini API í‚¤ê°€ í•„ìš”í•©ë‹ˆë‹¤")
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.processor = GeminiProcessor(self.config)
        self.collector = StockDataCollector(self.config)
        self.status = SystemStatus.READY
        
        # ì •ë¦¬ ìŠ¤ì¼€ì¤„ëŸ¬
        self._setup_cleanup()
        
        logger.info("âœ… Ultra AI Analyzer ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def _setup_cleanup(self):
        """ì •ë¦¬ ì‘ì—… ìŠ¤ì¼€ì¤„ëŸ¬"""
        def cleanup_task():
            while True:
                time.sleep(300)  # 5ë¶„ë§ˆë‹¤
                try:
                    self.processor.cache.cleanup_expired()
                    self.collector.cache.cleanup_expired()
        except Exception as e:
                    logger.error(f"ì •ë¦¬ ì‘ì—… ì˜¤ë¥˜: {e}")
        
        cleanup_thread = threading.Thread(target=cleanup_task, daemon=True)
        cleanup_thread.start()
    
    async def analyze_stocks(self, symbols: List[str], strategy: str = 'comprehensive',
                           market: str = 'auto') -> List[Dict[str, Any]]:
        """ì£¼ì‹ ë¶„ì„ ë©”ì¸ ë©”ì„œë“œ"""
        if not symbols:
            return []
        
        self.status = SystemStatus.BUSY
        start_time = time.time()
        
        try:
            logger.info(f"ğŸš€ AI ë¶„ì„ ì‹œì‘: {len(symbols)}ê°œ ì¢…ëª©, ì „ëµ: {strategy}")
            
            # 1. ë°ì´í„° ìˆ˜ì§‘
            stock_data_list = await self.collector.collect_batch(symbols, market)
            valid_data = [data for data in stock_data_list if not data.get('error')]
            
            if not valid_data:
                logger.warning("âš ï¸ ìœ íš¨í•œ ì£¼ì‹ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return []
        
            # 2. AI ë¶„ì„
            analysis_results = await self.processor.process_ultra_batch(valid_data, strategy)
            
            # 3. í›„ì²˜ë¦¬
            final_results = self._post_process_results(analysis_results)
            
            duration = time.time() - start_time
            logger.info(f"âœ… ë¶„ì„ ì™„ë£Œ: {len(final_results)}ê°œ ê²°ê³¼, {duration:.2f}ì´ˆ")
            
            return final_results
            
        except Exception as e:
            logger.error(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return self._create_error_results(symbols, str(e))
        finally:
            self.status = SystemStatus.READY
    
    def _post_process_results(self, results: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ê²°ê³¼ í›„ì²˜ë¦¬"""
        processed = []
        
        for result in results:
            # í•„ìˆ˜ í•„ë“œ í™•ì¸ ë° ì •ë¦¬
            processed_result = {
                'name': result.get('stock_info', {}).get('name', 'ì•Œ ìˆ˜ ì—†ìŒ'),
                'ticker': result.get('stock_info', {}).get('ticker', ''),
                'score': result.get('score', 0),
                'recommendation': result.get('recommendation', 'ë³´ìœ '),
                'target_price': result.get('target_price', 0),
                'current_price': result.get('stock_info', {}).get('current_price', 0),
                'reason': result.get('reason', ''),
                'analysis_time': result.get('timestamp', datetime.now().isoformat())
            }
            
            processed.append(processed_result)
        
        # ì ìˆ˜ ìˆœìœ¼ë¡œ ì •ë ¬
        processed.sort(key=lambda x: x['score'], reverse=True)
        return processed
    
    def _create_error_results(self, symbols: List[str], error: str) -> List[Dict[str, Any]]:
        """ì˜¤ë¥˜ ê²°ê³¼ ìƒì„±"""
        return [{
            'name': symbol,
            'ticker': symbol,
            'score': 0,
            'recommendation': 'ë¶„ì„ì‹¤íŒ¨',
            'target_price': 0,
            'current_price': 0,
            'reason': f'ë¶„ì„ ì˜¤ë¥˜: {error}',
            'analysis_time': datetime.now().isoformat()
        } for symbol in symbols]
    
    def get_system_info(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ì •ë³´ ì¡°íšŒ"""
        return {
            "ì‹œìŠ¤í…œ_ìƒíƒœ": self.status.name,
            "ì„¤ì •": {
                "ëª¨ë¸": self.config.model,
                "ìµœëŒ€_ë™ì‹œì‹¤í–‰": self.config.max_concurrent,
                "ë°°ì¹˜_í¬ê¸°": self.config.batch_size,
                "ìºì‹œ_TTL": f"{self.config.cache_ttl}ì´ˆ"
            },
            "ì„±ëŠ¥_í†µê³„": self.processor.monitor.get_stats(),
            "ìˆ˜ì§‘ê¸°_í†µê³„": self.collector.monitor.get_stats()
        }
    
    async def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.processor.cache.cleanup_expired()
            self.collector.cache.cleanup_expired()
            logger.info("âœ… ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
            except Exception as e:
            logger.error(f"ì •ë¦¬ ì‘ì—… ì˜¤ë¥˜: {e}")

# í¸ì˜ í•¨ìˆ˜ë“¤
async def quick_analyze(symbols: List[str], strategy: str = 'comprehensive',
                       api_key: Optional[str] = None) -> List[Dict[str, Any]]:
    """ë¹ ë¥¸ ë¶„ì„ í•¨ìˆ˜"""
    analyzer = UltraAIAnalyzer(api_key)
    try:
        return await analyzer.analyze_stocks(symbols, strategy)
    finally:
        await analyzer.cleanup()

def analyze_sync(symbols: List[str], strategy: str = 'comprehensive',
                api_key: Optional[str] = None) -> List[Dict[str, Any]]:
    """ë™ê¸° ë¶„ì„ í•¨ìˆ˜"""
    return asyncio.run(quick_analyze(symbols, strategy, api_key))

if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    test_symbols = ['AAPL', 'GOOGL', '005930']
    results = analyze_sync(test_symbols, 'comprehensive')
    print(json.dumps(results, indent=2, ensure_ascii=False))