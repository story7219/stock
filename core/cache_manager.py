"""
ğŸš€ Ultra ë©€í‹°ë ˆë²¨ ìºì‹± ì‹œìŠ¤í…œ v5.0
L1(ë©”ëª¨ë¦¬) + L2(Redis) + L3(ë””ìŠ¤í¬) ìºì‹œ í†µí•© ê´€ë¦¬
ê³ ì„±ëŠ¥ ë¹„ë™ê¸° ì²˜ë¦¬, ë©”ëª¨ë¦¬ ìµœì í™”, ìë™ ìºì‹œ ì›Œë°
"""
import asyncio
import json
import pickle
import hashlib
import time
import gc
import weakref
import threading
from typing import Any, Optional, Dict, List, Union, Callable, TypeVar, Generic
from datetime import datetime, timedelta
from functools import wraps
from dataclasses import dataclass, field
from concurrent.futures import ThreadPoolExecutor
from collections import OrderedDict
import gzip
import lz4.frame

import redis.asyncio as redis
import diskcache as dc
import structlog
from config.settings import settings

logger = structlog.get_logger(__name__)

T = TypeVar('T')


@dataclass
class CacheStats:
    """ìºì‹œ í†µê³„ ì •ë³´"""
    hits: int = 0
    misses: int = 0
    sets: int = 0
    deletes: int = 0
    evictions: int = 0
    memory_usage: int = 0
    
    @property
    def hit_rate(self) -> float:
        """ìºì‹œ íˆíŠ¸ìœ¨"""
        total = self.hits + self.misses
        return self.hits / total if total > 0 else 0.0
    
    def reset(self) -> None:
        """í†µê³„ ì´ˆê¸°í™”"""
        self.hits = self.misses = self.sets = self.deletes = self.evictions = 0
        self.memory_usage = 0


@dataclass
class CacheEntry(Generic[T]):
    """ê³ ì„±ëŠ¥ ìºì‹œ ì—”íŠ¸ë¦¬"""
    value: T
    created_at: datetime
    ttl: int
    access_count: int = 0
    last_accessed: Optional[datetime] = None
    size: int = 0
    compressed: bool = False
    
    def __post_init__(self):
        """ì—”íŠ¸ë¦¬ ìƒì„± í›„ ì´ˆê¸°í™”"""
        if self.last_accessed is None:
            self.last_accessed = self.created_at
        if self.size == 0:
            self.size = self._calculate_size()
    
    def _calculate_size(self) -> int:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ê³„ì‚°"""
        try:
            return len(pickle.dumps(self.value))
        except Exception:
            return 1024  # ê¸°ë³¸ê°’
    
    def is_expired(self) -> bool:
        """ìºì‹œ ì—”íŠ¸ë¦¬ ë§Œë£Œ í™•ì¸"""
        if self.ttl <= 0:
            return False
        return datetime.now() > self.created_at + timedelta(seconds=self.ttl)
    
    def touch(self) -> None:
        """ì—”íŠ¸ë¦¬ ì ‘ê·¼ ì‹œê°„ ì—…ë°ì´íŠ¸"""
        self.access_count += 1
        self.last_accessed = datetime.now()
    
    def get_age(self) -> float:
        """ì—”íŠ¸ë¦¬ ìƒì„± í›„ ê²½ê³¼ ì‹œê°„(ì´ˆ)"""
        return (datetime.now() - self.created_at).total_seconds()


class LRUCache(Generic[T]):
    """ê³ ì„±ëŠ¥ LRU ìºì‹œ (L1 ìºì‹œ)"""
    
    def __init__(self, max_size: int = 50000, max_memory_mb: int = 512):
        self.max_size = max_size
        self.max_memory_bytes = max_memory_mb * 1024 * 1024
        self._cache: OrderedDict[str, CacheEntry[T]] = OrderedDict()
        self._lock = threading.RLock()
        self._stats = CacheStats()
        self._memory_usage = 0
        
        # ë©”ëª¨ë¦¬ ì •ë¦¬ ìŠ¤ë ˆë“œ
        self._cleanup_thread = threading.Thread(target=self._periodic_cleanup, daemon=True)
        self._cleanup_thread.start()
    
    async def get(self, key: str) -> Optional[T]:
        """L1 ìºì‹œì—ì„œ ê°’ ì¡°íšŒ"""
        with self._lock:
            entry = self._cache.get(key)
            if entry is None:
                self._stats.misses += 1
                return None
            
            if entry.is_expired():
                self._remove_entry(key)
                self._stats.misses += 1
                return None
            
            # LRU ì—…ë°ì´íŠ¸
            self._cache.move_to_end(key)
            entry.touch()
            self._stats.hits += 1
            
            return entry.value
    
    async def set(self, key: str, value: T, ttl: int = 300) -> None:
        """L1 ìºì‹œì— ê°’ ì €ì¥"""
        with self._lock:
            # ê¸°ì¡´ ì—”íŠ¸ë¦¬ ì œê±°
            if key in self._cache:
                self._remove_entry(key)
            
            # ìƒˆ ì—”íŠ¸ë¦¬ ìƒì„±
            entry = CacheEntry(
                value=value,
                created_at=datetime.now(),
                ttl=ttl
            )
            
            # ë©”ëª¨ë¦¬ ë¶€ì¡± ì‹œ ì •ë¦¬
            while (len(self._cache) >= self.max_size or 
                   self._memory_usage + entry.size > self.max_memory_bytes):
                if not self._evict_lru():
                    break
            
            self._cache[key] = entry
            self._memory_usage += entry.size
            self._stats.sets += 1
    
    async def delete(self, key: str) -> bool:
        """L1 ìºì‹œì—ì„œ í‚¤ ì‚­ì œ"""
        with self._lock:
            if key in self._cache:
                self._remove_entry(key)
                self._stats.deletes += 1
                return True
            return False
    
    def _remove_entry(self, key: str) -> None:
        """ì—”íŠ¸ë¦¬ ì œê±° (ë½ í•„ìš”)"""
        entry = self._cache.pop(key, None)
        if entry:
            self._memory_usage -= entry.size
    
    def _evict_lru(self) -> bool:
        """LRU ì •ì±…ìœ¼ë¡œ ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°"""
        if not self._cache:
            return False
        
        # ê°€ì¥ ì˜¤ë˜ëœ í•­ëª© ì œê±°
        oldest_key = next(iter(self._cache))
        self._remove_entry(oldest_key)
        self._stats.evictions += 1
        return True
    
    def _periodic_cleanup(self) -> None:
        """ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬"""
        while True:
            try:
                time.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì‹¤í–‰
                self._cleanup_expired()
                if self._memory_usage > self.max_memory_bytes * 0.8:
                    self._force_gc()
            except Exception as e:
                logger.error(f"ìºì‹œ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
    
    def _cleanup_expired(self) -> None:
        """ë§Œë£Œëœ ì—”íŠ¸ë¦¬ ì •ë¦¬"""
        with self._lock:
            expired_keys = [
                key for key, entry in self._cache.items()
                if entry.is_expired()
            ]
            for key in expired_keys:
                self._remove_entry(key)
    
    def _force_gc(self) -> None:
        """ê°•ì œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜"""
        gc.collect()
        logger.debug("L1 ìºì‹œ ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ ì‹¤í–‰")
    
    def stats(self) -> CacheStats:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        with self._lock:
            self._stats.memory_usage = self._memory_usage
            return self._stats
    
    def clear(self) -> None:
        """ëª¨ë“  ìºì‹œ í•­ëª© ì‚­ì œ"""
        with self._lock:
            self._cache.clear()
            self._memory_usage = 0
            self._stats.reset()


class RedisCache:
    """ê³ ì„±ëŠ¥ Redis ìºì‹œ (L2 ìºì‹œ)"""
    
    def __init__(self):
        self.redis_client: Optional[redis.Redis] = None
        self._connection_pool: Optional[redis.ConnectionPool] = None
        self._stats = CacheStats()
        self._pipeline_buffer: List[tuple] = []
        self._pipeline_lock = asyncio.Lock()
        self._compression_enabled = settings.cache.l2_compression
    
    async def initialize(self) -> None:
        """Redis ì—°ê²° ì´ˆê¸°í™”"""
        try:
            # ê³ ì„±ëŠ¥ ì—°ê²° í’€ ìƒì„±
            self._connection_pool = redis.ConnectionPool.from_url(
                settings.redis.url,
                max_connections=settings.redis.max_connections,
                **settings.redis.connection_pool_kwargs
            )
            
            self.redis_client = redis.Redis(
                connection_pool=self._connection_pool,
                decode_responses=False  # ë°”ì´ë„ˆë¦¬ ë°ì´í„° ì²˜ë¦¬
            )
            
            # ì—°ê²° í…ŒìŠ¤íŠ¸
            await self.redis_client.ping()
            logger.info("Redis L2 ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")
            
            # íŒŒì´í”„ë¼ì¸ í”ŒëŸ¬ì‹œ íƒœìŠ¤í¬ ì‹œì‘
            asyncio.create_task(self._pipeline_flusher())
            
        except Exception as e:
            logger.warning(f"Redis ì„œë²„ ì—°ê²° ì‹¤íŒ¨ (L2 ìºì‹œ ë¹„í™œì„±í™”): {e}")
            self.redis_client = None
    
    async def get(self, key: str) -> Optional[Any]:
        """Redisì—ì„œ ê°’ ì¡°íšŒ"""
        if not self.redis_client:
            self._stats.misses += 1
            return None
        
        try:
            data = await self.redis_client.get(key)
            if data is None:
                self._stats.misses += 1
                return None
            
            # ì••ì¶• í•´ì œ
            if self._compression_enabled and data.startswith(b'LZ4:'):
                data = lz4.frame.decompress(data[4:])
            
            value = pickle.loads(data)
            self._stats.hits += 1
            return value
            
        except Exception as e:
            logger.error(f"Redis ì¡°íšŒ ì‹¤íŒ¨ {key}: {e}")
            self._stats.misses += 1
            return None
    
    async def set(self, key: str, value: Any, ttl: int = 3600) -> None:
        """Redisì— ê°’ ì €ì¥"""
        if not self.redis_client:
            return
        
        try:
            data = pickle.dumps(value)
            
            # ì••ì¶• ì ìš©
            if self._compression_enabled and len(data) > 1024:
                compressed = lz4.frame.compress(data)
                if len(compressed) < len(data) * 0.8:  # 20% ì´ìƒ ì••ì¶•ëœ ê²½ìš°ë§Œ
                    data = b'LZ4:' + compressed
            
            # íŒŒì´í”„ë¼ì¸ ë²„í¼ì— ì¶”ê°€
            async with self._pipeline_lock:
                self._pipeline_buffer.append(('setex', key, ttl, data))
                
                # ë²„í¼ê°€ ê°€ë“ ì°¬ ê²½ìš° ì¦‰ì‹œ í”ŒëŸ¬ì‹œ
                if len(self._pipeline_buffer) >= settings.redis.pipeline_size:
                    await self._flush_pipeline()
            
            self._stats.sets += 1
            
        except Exception as e:
            logger.error(f"Redis ì €ì¥ ì‹¤íŒ¨ {key}: {e}")
    
    async def delete(self, key: str) -> bool:
        """Redisì—ì„œ í‚¤ ì‚­ì œ"""
        if not self.redis_client:
            return False
        
        try:
            result = await self.redis_client.delete(key)
            self._stats.deletes += 1
            return result > 0
        except Exception as e:
            logger.error(f"Redis ì‚­ì œ ì‹¤íŒ¨ {key}: {e}")
            return False
    
    async def _pipeline_flusher(self) -> None:
        """íŒŒì´í”„ë¼ì¸ ë²„í¼ ì£¼ê¸°ì  í”ŒëŸ¬ì‹œ"""
        while True:
            try:
                await asyncio.sleep(0.1)  # 100msë§ˆë‹¤ í™•ì¸
                async with self._pipeline_lock:
                    if self._pipeline_buffer:
                        await self._flush_pipeline()
            except Exception as e:
                logger.error(f"íŒŒì´í”„ë¼ì¸ í”ŒëŸ¬ì‹œ ì˜¤ë¥˜: {e}")
    
    async def _flush_pipeline(self) -> None:
        """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰"""
        if not self._pipeline_buffer or not self.redis_client:
            return
        
        try:
            pipe = self.redis_client.pipeline()
            for cmd, *args in self._pipeline_buffer:
                getattr(pipe, cmd)(*args)
            
            await pipe.execute()
            self._pipeline_buffer.clear()
            
        except Exception as e:
            logger.error(f"íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            self._pipeline_buffer.clear()
    
    def stats(self) -> CacheStats:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        return self._stats
    
    async def close(self) -> None:
        """Redis ì—°ê²° ì¢…ë£Œ"""
        try:
            # ë‚¨ì€ íŒŒì´í”„ë¼ì¸ í”ŒëŸ¬ì‹œ
            async with self._pipeline_lock:
                await self._flush_pipeline()
            
            if self.redis_client:
                await self.redis_client.close()
            if self._connection_pool:
                await self._connection_pool.disconnect()
                
        except Exception as e:
            logger.error(f"Redis ì—°ê²° ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")


class DiskCache:
    """ê³ ì„±ëŠ¥ ë””ìŠ¤í¬ ìºì‹œ (L3 ìºì‹œ)"""
    
    def __init__(self, cache_dir: str = "cache"):
        self.cache_dir = cache_dir
        self._stats = CacheStats()
        
        # DiskCache ì„¤ì •
        self.cache = dc.Cache(
            cache_dir,
            size_limit=settings.cache.l3_max_size_gb * 1024**3,  # GB to bytes
            disk_min_file_size=1024,  # 1KB ì´ìƒë§Œ ë””ìŠ¤í¬ì— ì €ì¥
            disk_pickle_protocol=pickle.HIGHEST_PROTOCOL
        )
    
    async def get(self, key: str) -> Optional[Any]:
        """ë””ìŠ¤í¬ ìºì‹œì—ì„œ ê°’ ì¡°íšŒ"""
        try:
            value = self.cache.get(key)
            if value is not None:
                self._stats.hits += 1
            else:
                self._stats.misses += 1
            return value
        except Exception as e:
            logger.error(f"ë””ìŠ¤í¬ ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨ {key}: {e}")
            self._stats.misses += 1
            return None
    
    async def set(self, key: str, value: Any, ttl: int = 86400) -> None:
        """ë””ìŠ¤í¬ ìºì‹œì— ê°’ ì €ì¥"""
        try:
            expire_time = time.time() + ttl if ttl > 0 else None
            self.cache.set(key, value, expire=expire_time)
            self._stats.sets += 1
        except Exception as e:
            logger.error(f"ë””ìŠ¤í¬ ìºì‹œ ì €ì¥ ì‹¤íŒ¨ {key}: {e}")
    
    async def delete(self, key: str) -> bool:
        """ë””ìŠ¤í¬ ìºì‹œì—ì„œ í‚¤ ì‚­ì œ"""
        try:
            result = self.cache.delete(key)
            if result:
                self._stats.deletes += 1
            return result
        except Exception as e:
            logger.error(f"ë””ìŠ¤í¬ ìºì‹œ ì‚­ì œ ì‹¤íŒ¨ {key}: {e}")
            return False
    
    def stats(self) -> CacheStats:
        """ìºì‹œ í†µê³„ ë°˜í™˜"""
        try:
            cache_stats = self.cache.stats()
            self._stats.memory_usage = cache_stats.get('size', 0)
        except Exception:
            pass
        return self._stats
    
    def clear(self) -> None:
        """ëª¨ë“  ìºì‹œ í•­ëª© ì‚­ì œ"""
        try:
            self.cache.clear()
            self._stats.reset()
        except Exception as e:
            logger.error(f"ë””ìŠ¤í¬ ìºì‹œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")


class UltraCacheManager:
    """ğŸš€ Ultra ë©€í‹°ë ˆë²¨ ìºì‹œ ë§¤ë‹ˆì €"""
    
    def __init__(self):
        # ìºì‹œ ê³„ì¸µ ì´ˆê¸°í™”
        self.l1_cache: Optional[LRUCache] = None
        self.l2_cache: Optional[RedisCache] = None
        self.l3_cache: Optional[DiskCache] = None
        
        # ì„±ëŠ¥ ìµœì í™”
        self._executor = ThreadPoolExecutor(max_workers=4)
        self._prefetch_queue: asyncio.Queue = asyncio.Queue(maxsize=1000)
        self._write_queue: asyncio.Queue = asyncio.Queue(maxsize=1000)
        
        # í†µê³„ ë° ëª¨ë‹ˆí„°ë§
        self._global_stats = CacheStats()
        self._last_stats_update = time.time()
        
        logger.info("Ultra ìºì‹œ ë§¤ë‹ˆì € ì´ˆê¸°í™”")
    
    async def initialize(self) -> None:
        """ìºì‹œ ë§¤ë‹ˆì € ì´ˆê¸°í™”"""
        try:
            # L1 ìºì‹œ (ë©”ëª¨ë¦¬) ì´ˆê¸°í™”
            if settings.cache.l1_enabled:
                self.l1_cache = LRUCache(
                    max_size=settings.cache.l1_max_size,
                    max_memory_mb=512
                )
                logger.info("L1 ë©”ëª¨ë¦¬ ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")
            
            # L2 ìºì‹œ (Redis) ì´ˆê¸°í™”
            if settings.cache.l2_enabled and settings.redis.enable_redis:
                self.l2_cache = RedisCache()
                await self.l2_cache.initialize()
                logger.info("L2 Redis ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")
            
            # L3 ìºì‹œ (ë””ìŠ¤í¬) ì´ˆê¸°í™”
            if settings.cache.l3_enabled:
                self.l3_cache = DiskCache(settings.cache.l3_directory)
                logger.info("L3 ë””ìŠ¤í¬ ìºì‹œ ì´ˆê¸°í™” ì™„ë£Œ")
            
            # ë°±ê·¸ë¼ìš´ë“œ íƒœìŠ¤í¬ ì‹œì‘
            asyncio.create_task(self._prefetch_worker())
            asyncio.create_task(self._write_worker())
            asyncio.create_task(self._stats_updater())
            
            logger.info("Ultra ìºì‹œ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ìºì‹œ ë§¤ë‹ˆì € ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    async def get(self, key: str) -> Optional[Any]:
        """ë©€í‹°ë ˆë²¨ ìºì‹œì—ì„œ ê°’ ì¡°íšŒ"""
        start_time = time.time()
        
        try:
            # L1 ìºì‹œ í™•ì¸
            if self.l1_cache:
                value = await self.l1_cache.get(key)
                if value is not None:
                    self._global_stats.hits += 1
                    logger.debug(f"L1 ìºì‹œ íˆíŠ¸: {key}")
                    return value
            
            # L2 ìºì‹œ í™•ì¸
            if self.l2_cache:
                value = await self.l2_cache.get(key)
                if value is not None:
                    # L1 ìºì‹œì— ë°±í•„
                    if self.l1_cache:
                        await self.l1_cache.set(key, value, settings.cache.l1_ttl)
                    self._global_stats.hits += 1
                    logger.debug(f"L2 ìºì‹œ íˆíŠ¸: {key}")
                    return value
            
            # L3 ìºì‹œ í™•ì¸
            if self.l3_cache:
                value = await self.l3_cache.get(key)
                if value is not None:
                    # ìƒìœ„ ìºì‹œì— ë°±í•„
                    if self.l2_cache:
                        await self.l2_cache.set(key, value, settings.cache.l2_ttl)
                    if self.l1_cache:
                        await self.l1_cache.set(key, value, settings.cache.l1_ttl)
                    self._global_stats.hits += 1
                    logger.debug(f"L3 ìºì‹œ íˆíŠ¸: {key}")
                    return value
            
            # ëª¨ë“  ìºì‹œì—ì„œ ë¯¸ìŠ¤
            self._global_stats.misses += 1
            return None
            
        except Exception as e:
            logger.error(f"ìºì‹œ ì¡°íšŒ ì‹¤íŒ¨ {key}: {e}")
            self._global_stats.misses += 1
            return None
        finally:
            # ì„±ëŠ¥ ë¡œê¹…
            elapsed = time.time() - start_time
            if elapsed > 0.1:  # 100ms ì´ìƒ
                logger.warning(f"ìºì‹œ ì¡°íšŒ ì§€ì—°: {key} ({elapsed:.3f}ì´ˆ)")
    
    async def set(self, key: str, value: Any, ttl: int = 3600) -> None:
        """ë©€í‹°ë ˆë²¨ ìºì‹œì— ê°’ ì €ì¥"""
        try:
            # Write-through ì „ëµ
            if settings.cache.write_through:
                await self._write_through(key, value, ttl)
            else:
                # ë¹„ë™ê¸° ì“°ê¸° íì— ì¶”ê°€
                await self._write_queue.put((key, value, ttl))
            
            self._global_stats.sets += 1
            
        except Exception as e:
            logger.error(f"ìºì‹œ ì €ì¥ ì‹¤íŒ¨ {key}: {e}")
    
    async def _write_through(self, key: str, value: Any, ttl: int) -> None:
        """Write-through ìºì‹œ ì“°ê¸°"""
        tasks = []
        
        # ëª¨ë“  ìºì‹œ ë ˆë²¨ì— ë™ì‹œ ì“°ê¸°
        if self.l1_cache:
            tasks.append(self.l1_cache.set(key, value, min(ttl, settings.cache.l1_ttl)))
        
        if self.l2_cache:
            tasks.append(self.l2_cache.set(key, value, min(ttl, settings.cache.l2_ttl)))
        
        if self.l3_cache:
            tasks.append(self.l3_cache.set(key, value, min(ttl, settings.cache.l3_ttl)))
        
        if tasks:
            await asyncio.gather(*tasks, return_exceptions=True)
    
    async def delete(self, key: str) -> bool:
        """ëª¨ë“  ìºì‹œ ë ˆë²¨ì—ì„œ í‚¤ ì‚­ì œ"""
        results = []
        
        if self.l1_cache:
            results.append(await self.l1_cache.delete(key))
        
        if self.l2_cache:
            results.append(await self.l2_cache.delete(key))
        
        if self.l3_cache:
            results.append(await self.l3_cache.delete(key))
        
        self._global_stats.deletes += 1
        return any(results)
    
    def generate_key(self, *args, **kwargs) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        key_data = f"{args}:{sorted(kwargs.items())}"
        return hashlib.md5(key_data.encode()).hexdigest()
    
    async def _prefetch_worker(self) -> None:
        """í”„ë¦¬í˜ì¹˜ ì›Œì»¤"""
        while True:
            try:
                # í”„ë¦¬í˜ì¹˜ ë¡œì§ êµ¬í˜„
                await asyncio.sleep(1)
            except Exception as e:
                logger.error(f"í”„ë¦¬í˜ì¹˜ ì›Œì»¤ ì˜¤ë¥˜: {e}")
    
    async def _write_worker(self) -> None:
        """ë¹„ë™ê¸° ì“°ê¸° ì›Œì»¤"""
        while True:
            try:
                key, value, ttl = await self._write_queue.get()
                await self._write_through(key, value, ttl)
                self._write_queue.task_done()
            except Exception as e:
                logger.error(f"ì“°ê¸° ì›Œì»¤ ì˜¤ë¥˜: {e}")
    
    async def _stats_updater(self) -> None:
        """í†µê³„ ì—…ë°ì´íŠ¸ ì›Œì»¤"""
        while True:
            try:
                await asyncio.sleep(60)  # 1ë¶„ë§ˆë‹¤
                self._update_global_stats()
            except Exception as e:
                logger.error(f"í†µê³„ ì—…ë°ì´íŠ¸ ì˜¤ë¥˜: {e}")
    
    def _update_global_stats(self) -> None:
        """ì „ì—­ í†µê³„ ì—…ë°ì´íŠ¸"""
        try:
            # ê° ìºì‹œ ë ˆë²¨ í†µê³„ ìˆ˜ì§‘
            stats_data = {
                "timestamp": datetime.now().isoformat(),
                "global": {
                    "hits": self._global_stats.hits,
                    "misses": self._global_stats.misses,
                    "hit_rate": self._global_stats.hit_rate,
                }
            }
            
            if self.l1_cache:
                l1_stats = self.l1_cache.stats()
                stats_data["l1"] = {
                    "hits": l1_stats.hits,
                    "misses": l1_stats.misses,
                    "hit_rate": l1_stats.hit_rate,
                    "memory_usage": l1_stats.memory_usage,
                }
            
            if self.l2_cache:
                l2_stats = self.l2_cache.stats()
                stats_data["l2"] = {
                    "hits": l2_stats.hits,
                    "misses": l2_stats.misses,
                    "hit_rate": l2_stats.hit_rate,
                }
            
            if self.l3_cache:
                l3_stats = self.l3_cache.stats()
                stats_data["l3"] = {
                    "hits": l3_stats.hits,
                    "misses": l3_stats.misses,
                    "hit_rate": l3_stats.hit_rate,
                    "memory_usage": l3_stats.memory_usage,
                }
            
            logger.info("ìºì‹œ í†µê³„ ì—…ë°ì´íŠ¸", extra=stats_data)
            
        except Exception as e:
            logger.error(f"í†µê³„ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}")
    
    def stats(self) -> Dict[str, Any]:
        """ì „ì²´ ìºì‹œ í†µê³„"""
        return {
            "global": {
                "hits": self._global_stats.hits,
                "misses": self._global_stats.misses,
                "sets": self._global_stats.sets,
                "deletes": self._global_stats.deletes,
                "hit_rate": self._global_stats.hit_rate,
            },
            "l1": self.l1_cache.stats().__dict__ if self.l1_cache else None,
            "l2": self.l2_cache.stats().__dict__ if self.l2_cache else None,
            "l3": self.l3_cache.stats().__dict__ if self.l3_cache else None,
        }
    
    async def close(self) -> None:
        """ìºì‹œ ë§¤ë‹ˆì € ì¢…ë£Œ"""
        try:
            # í ëŒ€ê¸° ì‘ì—… ì™„ë£Œ
            await self._write_queue.join()
            
            # ê° ìºì‹œ ë ˆë²¨ ì¢…ë£Œ
            if self.l2_cache:
                await self.l2_cache.close()
            
            if self.l3_cache:
                self.l3_cache.clear()
            
            # ìŠ¤ë ˆë“œ í’€ ì¢…ë£Œ
            self._executor.shutdown(wait=True)
            
            logger.info("Ultra ìºì‹œ ë§¤ë‹ˆì € ì¢…ë£Œ ì™„ë£Œ")
            
        except Exception as e:
            logger.error(f"ìºì‹œ ë§¤ë‹ˆì € ì¢…ë£Œ ì¤‘ ì˜¤ë¥˜: {e}")


# ì „ì—­ ìºì‹œ ë§¤ë‹ˆì € ì¸ìŠ¤í„´ìŠ¤
_cache_manager: Optional[UltraCacheManager] = None


def get_cache_manager() -> UltraCacheManager:
    """ì „ì—­ ìºì‹œ ë§¤ë‹ˆì € ë°˜í™˜"""
    global _cache_manager
    if _cache_manager is None:
        _cache_manager = UltraCacheManager()
    return _cache_manager


def cached(ttl: int = 3600, key_prefix: str = "", use_compression: bool = True):
    """ê³ ì„±ëŠ¥ ìºì‹œ ë°ì½”ë ˆì´í„°"""
    def decorator(func: Callable) -> Callable:
        cache_manager = get_cache_manager()
        
        if asyncio.iscoroutinefunction(func):
            @wraps(func)
            async def async_wrapper(*args, **kwargs):
                # ìºì‹œ í‚¤ ìƒì„±
                cache_key = f"{key_prefix}:{func.__name__}:{cache_manager.generate_key(*args, **kwargs)}"
                
                # ìºì‹œì—ì„œ ì¡°íšŒ
                cached_result = await cache_manager.get(cache_key)
                if cached_result is not None:
                    return cached_result
                
                # í•¨ìˆ˜ ì‹¤í–‰
                result = await func(*args, **kwargs)
                
                # ìºì‹œì— ì €ì¥
                if result is not None:
                    await cache_manager.set(cache_key, result, ttl)
                
                return result
            
            return async_wrapper
        else:
            @wraps(func)
            def sync_wrapper(*args, **kwargs):
                # ë™ê¸° í•¨ìˆ˜ëŠ” ìºì‹œ ì—†ì´ ì‹¤í–‰ (ì„±ëŠ¥ìƒ ì´ìœ )
                return func(*args, **kwargs)
            
            return sync_wrapper
    
    return decorator


async def initialize_cache() -> None:
    """ìºì‹œ ì‹œìŠ¤í…œ ì´ˆê¸°í™”"""
    cache_manager = get_cache_manager()
    await cache_manager.initialize()


async def cleanup_cache() -> None:
    """ìºì‹œ ì‹œìŠ¤í…œ ì •ë¦¬"""
    global _cache_manager
    if _cache_manager:
        await _cache_manager.close()
        _cache_manager = None


# í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
CacheManager = UltraCacheManager 