#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íŒŒì¼ëª…: krx_smart_main_collector.py
ëª©ì : ëª¨ë“  KRX(ì£¼ì‹/ETF/ì„ ë¬¼/ì˜µì…˜) ë°ì´í„° ê³¼ê±°~í˜„ì¬ê¹Œì§€ ì•ˆì „Â·ìµœì í™” ìˆ˜ì§‘
Author: World-Class AI Quant Team
Created: 2025-07-14
Version: 4.0.0

- ìµœì‹  Python(3.11+) ë¬¸ë²•, ìµœì†Œ ì½”ë“œ, ìµœëŒ€ ìœ ì§€ë³´ìˆ˜ì„±
- pykrx, pandas, tqdm, numpy, logging, typing ì ê·¹ í™œìš©
- ì „ëµ(404ë°©ì§€, ê·¹ê°’, ë³‘ë ¬, pykrx ë“±) 100% ìœ ì§€
"""

from __future__ import annotations
from dataclasses import dataclass, field
from typing import Any, Dict, List, Callable, Optional
import pandas as pd
import numpy as np
from pykrx import stock
from tqdm import tqdm
import logging
import asyncio
import aiohttp
import sqlite3
import json
import time
import hashlib
from datetime import datetime
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor
import requests
from collections import defaultdict

logging.basicConfig(level=logging.INFO)

@dataclass
class KRXCollector:
    """KRX ë°ì´í„° ìˆ˜ì§‘ê¸° (ìµœì†Œ ì½”ë“œ, ìµœëŒ€ ìœ ì§€ë³´ìˆ˜, ìµœì‹  ë¬¸ë²•)"""
    logger: logging.Logger = logging.getLogger("KRXCollector")
    db_path: str = "krx_data.db"
    cache_dir: Path = field(default_factory=lambda: Path("cache"))
    retry_count: int = 3
    delay: float = 0.1
    
    def __post_init__(self):
        self.cache_dir.mkdir(exist_ok=True)
        self._init_db()
        self.endpoint_health = defaultdict(dict)
        self.request_count = 0
        
    def _init_db(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS extremes (
                    ticker TEXT PRIMARY KEY,
                    max_price REAL,
                    min_price REAL,
                    max_price_date TEXT,
                    min_price_date TEXT,
                    max_volume REAL,
                    max_value REAL,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)
            conn.execute("""
                CREATE TABLE IF NOT EXISTS collection_log (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    category TEXT,
                    ticker_count INTEGER,
                    success_count INTEGER,
                    error_count INTEGER,
                    execution_time REAL,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
                )
            """)

    def safe_request(self, func: Callable, *args, **kwargs) -> Any:
        """ì•ˆì „í•œ ìš”ì²­ (404 ë°©ì§€, ì¬ì‹œë„)"""
        for attempt in range(self.retry_count):
            try:
                self.request_count += 1
                result = func(*args, **kwargs)
                time.sleep(self.delay)
                return result
            except Exception as e:
                self.logger.warning(f"ìš”ì²­ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{self.retry_count}): {e}")
                if attempt < self.retry_count - 1:
                    time.sleep(self.delay * (2 ** attempt))  # ì§€ìˆ˜ ë°±ì˜¤í”„
                else:
                    self.logger.error(f"ìµœì¢… ì‹¤íŒ¨: {e}")
                    return None

    def collect_all(
        self,
        tickers: List[str],
        start: str,
        end: str,
        fetch_func: Callable[[str, str, str], pd.DataFrame],
        desc: str = "ìˆ˜ì§‘"
    ) -> Dict[str, pd.DataFrame]:
        """ë²”ìš© ë°ì´í„° ìˆ˜ì§‘ (ì•ˆì „í•œ ìš”ì²­)"""
        results = {}
        for t in tqdm(tickers, desc=desc):
            df = self.safe_request(fetch_func, start, end, t)
            if df is not None and not df.empty:
                results[t] = df
        return results

    def get_extremes(self, df: pd.DataFrame) -> dict:
        """ê·¹ê°’ ì¶”ì  (ìµœì‹  ë¬¸ë²•)"""
        if df.empty:
            return {}
        return {
            "max_price": float(np.float64(df["ì¢…ê°€"].max())) if not pd.isnull(df["ì¢…ê°€"].max()) else None,
            "min_price": float(np.float64(df["ì¢…ê°€"].min())) if not pd.isnull(df["ì¢…ê°€"].min()) else None,
            "max_price_date": str(df["ì¢…ê°€"].idxmax().date()) if hasattr(df["ì¢…ê°€"].idxmax(), "date") else str(df["ì¢…ê°€"].idxmax()),
            "min_price_date": str(df["ì¢…ê°€"].idxmin().date()) if hasattr(df["ì¢…ê°€"].idxmin(), "date") else str(df["ì¢…ê°€"].idxmin()),
            "max_volume": float(np.float64(df["ê±°ë˜ëŸ‰"].max())) if "ê±°ë˜ëŸ‰" in df and not pd.isnull(df["ê±°ë˜ëŸ‰"].max()) else None,
            "max_value": float(np.float64(df["ê±°ë˜ëŒ€ê¸ˆ"].max())) if "ê±°ë˜ëŒ€ê¸ˆ" in df and not pd.isnull(df["ê±°ë˜ëŒ€ê¸ˆ"].max()) else None,
        }

    def save_extremes(self, ticker: str, extremes: dict):
        """ê·¹ê°’ DB ì €ì¥"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                INSERT OR REPLACE INTO extremes 
                (ticker, max_price, min_price, max_price_date, min_price_date, max_volume, max_value)
                VALUES (?, ?, ?, ?, ?, ?, ?)
            """, (
                ticker,
                extremes.get("max_price"),
                extremes.get("min_price"),
                extremes.get("max_price_date"),
                extremes.get("min_price_date"),
                extremes.get("max_volume"),
                extremes.get("max_value")
            ))

    def collect_kospi200(self, start: str, end: str) -> Dict[str, pd.DataFrame]:
        tickers = self.safe_request(stock.get_index_portfolio_deposit_file, "1028")
        return self.collect_all(tickers, start, end, stock.get_market_ohlcv_by_date, desc="KOSPI200")

    def collect_kosdaq150(self, start: str, end: str) -> Dict[str, pd.DataFrame]:
        tickers = self.safe_request(stock.get_index_portfolio_deposit_file, "1538")
        return self.collect_all(tickers, start, end, stock.get_market_ohlcv_by_date, desc="KOSDAQ150")

    def collect_top_etf(self, n: int, start: str, end: str) -> Dict[str, pd.DataFrame]:
        today = datetime.now().strftime("%Y%m%d")
        etf_tickers = self.safe_request(stock.get_etf_ticker_list)
        if not etf_tickers:
            return {}
        
        values = []
        for t in etf_tickers:
            df = self.safe_request(stock.get_etf_ohlcv_by_date, today, today, t)
            if df is not None and not df.empty:
                values.append((t, df.iloc[0]["ê±°ë˜ëŒ€ê¸ˆ"]))
        
        values.sort(key=lambda x: x[1], reverse=True)
        top_tickers = [t for t, _ in values[:n]]
        return self.collect_all(top_tickers, start, end, stock.get_etf_ohlcv_by_date, desc="ETF_TOP")

    def collect_derivatives(self, start: str, end: str) -> Dict[str, Dict[str, pd.DataFrame]]:
        """ì„ ë¬¼/ì˜µì…˜ ë°ì´í„° ìˆ˜ì§‘ (stock ëª¨ë“ˆ ì‚¬ìš©)"""
        categories = {
            "FUTURES": self.safe_request(stock.get_future_ticker_list),
        }
        return {
            cat: self.collect_all(tickers, start, end, stock.get_future_ohlcv_by_ticker, desc=cat)
            for cat, tickers in categories.items()
            if tickers
        }

    def collect_all_data(self, start: str, end: str, etf_n: int = 8) -> Dict[str, Any]:
        """ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘ (ëª¨ë‹ˆí„°ë§ í¬í•¨)"""
        start_time = time.time()
        self.logger.info("ğŸš€ ëª¨ë“  ë°ì´í„° ë™ì‹œ ìˆ˜ì§‘ ì‹œì‘")
        
        data = {
            "kospi200": self.collect_kospi200(start, end),
            "kosdaq150": self.collect_kosdaq150(start, end),
            "etf_top": self.collect_top_etf(etf_n, start, end),
            "derivatives": self.collect_derivatives(start, end),
        }
        
        # ê·¹ê°’ ì €ì¥ ë° í†µê³„
        total_tickers = sum(len(tickers) for tickers in data.values() if isinstance(tickers, dict))
        success_count = sum(len(tickers) for tickers in data.values() if isinstance(tickers, dict))
        
        # DBì— ìˆ˜ì§‘ ë¡œê·¸ ì €ì¥
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                INSERT INTO collection_log 
                (category, ticker_count, success_count, error_count, execution_time)
                VALUES (?, ?, ?, ?, ?)
            """, ("ALL", total_tickers, success_count, total_tickers - success_count, time.time() - start_time))
        
        self.logger.info(f"âœ… ìˆ˜ì§‘ ì™„ë£Œ: {success_count}/{total_tickers} ì„±ê³µ, {time.time() - start_time:.2f}ì´ˆ")
        return data

    def save_data_by_type(
        self, 
        data: Dict[str, Any], 
        output_dir: str = "data",
        storage_config: Dict[str, str] = None
    ):
        """ë°ì´í„° ì„±ê²©ì— ë”°ë¥¸ ì„ íƒì  ì €ì¥ (Parquet, SQLite, SQL, HDF5)"""
        if storage_config is None:
            storage_config = {
                "kospi200": "parquet",      # ëŒ€ìš©ëŸ‰ ì£¼ì‹ ë°ì´í„° â†’ Parquet (ì••ì¶•ë¥  ë†’ìŒ)
                "kosdaq150": "parquet",     # ëŒ€ìš©ëŸ‰ ì£¼ì‹ ë°ì´í„° â†’ Parquet
                "etf_top": "sqlite",        # ìƒìœ„ ETF â†’ SQLite (ë¹ ë¥¸ ì¡°íšŒ)
                "derivatives": "hdf5",      # ì„ ë¬¼/ì˜µì…˜ â†’ HDF5 (ë³µì¡í•œ êµ¬ì¡°)
            }
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        Path(output_dir).mkdir(exist_ok=True)
        
        for category, tickers in data.items():
            if not isinstance(tickers, dict):
                continue
                
            storage_type = storage_config.get(category, "parquet")
            
            if storage_type == "parquet":
                self._save_as_parquet(tickers, output_dir, category, timestamp)
            elif storage_type == "sqlite":
                self._save_as_sqlite(tickers, category, timestamp)
            elif storage_type == "sql":
                self._save_as_sql(tickers, output_dir, category, timestamp)
            elif storage_type == "hdf5":
                self._save_as_hdf5(tickers, output_dir, category, timestamp)
            else:
                self.logger.warning(f"ì•Œ ìˆ˜ ì—†ëŠ” ì €ì¥ íƒ€ì…: {storage_type}")

    def _save_as_parquet(self, tickers: Dict[str, pd.DataFrame], output_dir: str, category: str, timestamp: str):
        """Parquet í˜•ì‹ ì €ì¥ (ëŒ€ìš©ëŸ‰ ë°ì´í„° ìµœì í™”)"""
        for ticker, df in tickers.items():
            filepath = Path(output_dir) / f"{category}_{ticker}_{timestamp}.parquet"
            df.to_parquet(filepath, compression="gzip", index=True)
            self.logger.info(f"Parquet ì €ì¥ ì™„ë£Œ: {filepath}")

    def _save_as_sqlite(self, tickers: Dict[str, pd.DataFrame], category: str, timestamp: str):
        """SQLite DB ì €ì¥ (ë¹ ë¥¸ ì¡°íšŒìš©)"""
        table_name = f"{category}_{timestamp.replace('_', '')}"
        
        with sqlite3.connect(self.db_path) as conn:
            for ticker, df in tickers.items():
                df.to_sql(f"{table_name}_{ticker}", conn, if_exists="replace", index=True)
                self.logger.info(f"SQLite ì €ì¥ ì™„ë£Œ: {table_name}_{ticker}")

    def _save_as_sql(self, tickers: Dict[str, pd.DataFrame], output_dir: str, category: str, timestamp: str):
        """SQL íŒŒì¼ ì €ì¥ (ë°±ì—…/ì´ë™ìš©)"""
        sql_file = Path(output_dir) / f"{category}_{timestamp}.sql"
        
        with open(sql_file, 'w', encoding='utf-8') as f:
            f.write(f"-- {category} ë°ì´í„° SQL ë°±ì—…\n")
            f.write(f"-- ìƒì„±ì¼: {datetime.now()}\n\n")
            
            for ticker, df in tickers.items():
                table_name = f"{category}_{ticker}_{timestamp.replace('_', '')}"
                
                # CREATE TABLE ë¬¸
                columns = []
                for col, dtype in df.dtypes.items():
                    if 'int' in str(dtype):
                        sql_type = "INTEGER"
                    elif 'float' in str(dtype):
                        sql_type = "REAL"
                    else:
                        sql_type = "TEXT"
                    columns.append(f"{col} {sql_type}")
                
                f.write(f"CREATE TABLE IF NOT EXISTS {table_name} (\n")
                f.write("    " + ",\n    ".join(columns) + "\n);\n\n")
                
                # INSERT ë¬¸
                for _, row in df.iterrows():
                    values = []
                    for val in row.values:
                        if pd.isna(val):
                            values.append("NULL")
                        elif isinstance(val, (int, float)):
                            values.append(str(val))
                        else:
                            values.append(f"'{str(val)}'")
                    
                    f.write(f"INSERT INTO {table_name} VALUES ({', '.join(values)});\n")
                f.write("\n")
        
        self.logger.info(f"SQL ì €ì¥ ì™„ë£Œ: {sql_file}")

    def _save_as_hdf5(self, tickers: Dict[str, pd.DataFrame], output_dir: str, category: str, timestamp: str):
        """HDF5 í˜•ì‹ ì €ì¥ (ë³µì¡í•œ êµ¬ì¡° ë°ì´í„°)"""
        try:
            import h5py
            hdf5_file = Path(output_dir) / f"{category}_{timestamp}.h5"
            
            with h5py.File(hdf5_file, 'w') as f:
                for ticker, df in tickers.items():
                    # ë°ì´í„°í”„ë ˆì„ì„ HDF5ì— ì €ì¥
                    df.to_hdf(hdf5_file, key=f"{category}/{ticker}", mode='a')
                    
                    # ë©”íƒ€ë°ì´í„° ì €ì¥
                    f[f"{category}/{ticker}"].attrs['ticker'] = ticker
                    f[f"{category}/{ticker}"].attrs['category'] = category
                    f[f"{category}/{ticker}"].attrs['created_at'] = timestamp
                    f[f"{category}/{ticker}"].attrs['row_count'] = len(df)
                    f[f"{category}/{ticker}"].attrs['column_count'] = len(df.columns)
            
            self.logger.info(f"HDF5 ì €ì¥ ì™„ë£Œ: {hdf5_file}")
            
        except ImportError:
            self.logger.warning("h5pyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ HDF5 ì €ì¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Parquetìœ¼ë¡œ ëŒ€ì²´í•©ë‹ˆë‹¤.")
            self._save_as_parquet(tickers, output_dir, category, timestamp)

    def save_to_parquet(self, data: Dict[str, Any], output_dir: str = "data"):
        """ê¸°ì¡´ Parquet ì €ì¥ (í•˜ìœ„ í˜¸í™˜ì„±)"""
        self.save_data_by_type(data, output_dir, {"kospi200": "parquet", "kosdaq150": "parquet", "etf_top": "parquet", "derivatives": "parquet"})

    def get_storage_recommendations(self) -> Dict[str, str]:
        """ë°ì´í„° ì„±ê²©ë³„ ì €ì¥ ë°©ì‹ ì¶”ì²œ"""
        return {
            "kospi200": "parquet",      # ëŒ€ìš©ëŸ‰ ì£¼ì‹ ë°ì´í„° â†’ Parquet (ì••ì¶•ë¥  ë†’ìŒ, ë¹ ë¥¸ ì½ê¸°)
            "kosdaq150": "parquet",     # ëŒ€ìš©ëŸ‰ ì£¼ì‹ ë°ì´í„° â†’ Parquet
            "etf_top": "sqlite",        # ìƒìœ„ ETF â†’ SQLite (ë¹ ë¥¸ ì¡°íšŒ, ì¸ë±ì‹±)
            "derivatives": "hdf5",      # ì„ ë¬¼/ì˜µì…˜ â†’ HDF5 (ë³µì¡í•œ êµ¬ì¡°, ë©”íƒ€ë°ì´í„°)
            "extremes": "sqlite",       # ê·¹ê°’ ë°ì´í„° â†’ SQLite (ë¹ ë¥¸ ì¡°íšŒ)
            "logs": "sqlite",           # ë¡œê·¸ ë°ì´í„° â†’ SQLite (êµ¬ì¡°í™”ëœ ì €ì¥)
        }

    def load_from_storage(self, category: str, storage_type: str = None, **kwargs) -> Dict[str, pd.DataFrame]:
        """ì €ì¥ëœ ë°ì´í„° ë¡œë“œ"""
        if storage_type is None:
            storage_type = self.get_storage_recommendations().get(category, "parquet")
        
        if storage_type == "parquet":
            return self._load_from_parquet(category, **kwargs)
        elif storage_type == "sqlite":
            return self._load_from_sqlite(category, **kwargs)
        elif storage_type == "hdf5":
            return self._load_from_hdf5(category, **kwargs)
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì €ì¥ íƒ€ì…: {storage_type}")

    def _load_from_parquet(self, category: str, data_dir: str = "data") -> Dict[str, pd.DataFrame]:
        """Parquet íŒŒì¼ì—ì„œ ë¡œë“œ"""
        data = {}
        data_path = Path(data_dir)
        
        for file in data_path.glob(f"{category}_*.parquet"):
            ticker = file.stem.split("_")[1]  # íŒŒì¼ëª…ì—ì„œ í‹°ì»¤ ì¶”ì¶œ
            data[ticker] = pd.read_parquet(file)
        
        return data

    def _load_from_sqlite(self, category: str, **kwargs) -> Dict[str, pd.DataFrame]:
        """SQLite DBì—ì„œ ë¡œë“œ"""
        data = {}
        with sqlite3.connect(self.db_path) as conn:
            tables = conn.execute("SELECT name FROM sqlite_master WHERE type='table' AND name LIKE ?", (f"{category}%",)).fetchall()
            
            for (table_name,) in tables:
                ticker = table_name.split("_")[1]  # í…Œì´ë¸”ëª…ì—ì„œ í‹°ì»¤ ì¶”ì¶œ
                data[ticker] = pd.read_sql(f"SELECT * FROM {table_name}", conn)
        
        return data

    def _load_from_hdf5(self, category: str, data_dir: str = "data") -> Dict[str, pd.DataFrame]:
        """HDF5 íŒŒì¼ì—ì„œ ë¡œë“œ"""
        try:
            import h5py
            data = {}
            data_path = Path(data_dir)
            
            for file in data_path.glob(f"{category}_*.h5"):
                with h5py.File(file, 'r') as f:
                    if category in f:
                        for ticker in f[category].keys():
                            data[ticker] = pd.read_hdf(file, key=f"{category}/{ticker}")
            
            return data
            
        except ImportError:
            self.logger.warning("h5pyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ HDF5 ë¡œë“œë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {}

    def send_telegram_alert(self, message: str):
        """í…”ë ˆê·¸ë¨ ì•Œë¦¼ (ì„ íƒì )"""
        try:
            bot_token = "YOUR_BOT_TOKEN"  # í™˜ê²½ë³€ìˆ˜ì—ì„œ ê°€ì ¸ì˜¤ê¸°
            chat_id = "YOUR_CHAT_ID"
            if bot_token != "YOUR_BOT_TOKEN":
                url = f"https://api.telegram.org/bot{bot_token}/sendMessage"
                requests.post(url, data={"chat_id": chat_id, "text": message})
        except Exception as e:
            self.logger.warning(f"í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    collector = KRXCollector()
    START = "19960503"
    END = datetime.now().strftime("%Y%m%d")
    
    # ë°ì´í„° ìˆ˜ì§‘
    data = collector.collect_all_data(START, END, etf_n=8)
    
    # ê·¹ê°’ ì¶”ì  ë° ì €ì¥
    for category, tickers in data.items():
        if isinstance(tickers, dict):
            for ticker, df in tickers.items():
                extremes = collector.get_extremes(df)
                collector.save_extremes(ticker, extremes)
                print(f"[{category}] {ticker}: {extremes}")
    
    # ë°ì´í„° ì„±ê²©ì— ë”°ë¥¸ ì„ íƒì  ì €ì¥
    print("\nğŸ“Š ì €ì¥ ë°©ì‹ ì¶”ì²œ:")
    recommendations = collector.get_storage_recommendations()
    for category, storage_type in recommendations.items():
        print(f"  - {category}: {storage_type}")
    
    # ê¸°ë³¸ ì €ì¥ (ì¶”ì²œ ë°©ì‹)
    collector.save_data_by_type(data)
    
    # ì‚¬ìš©ì ì •ì˜ ì €ì¥ ì„¤ì • ì˜ˆì‹œ
    custom_storage = {
        "kospi200": "parquet",      # ëŒ€ìš©ëŸ‰ â†’ Parquet
        "kosdaq150": "sqlite",      # ë¹ ë¥¸ ì¡°íšŒ â†’ SQLite
        "etf_top": "sql",           # ë°±ì—…ìš© â†’ SQL
        "derivatives": "hdf5",      # ë³µì¡í•œ êµ¬ì¡° â†’ HDF5
    }
    collector.save_data_by_type(data, storage_config=custom_storage)
    
    # ì €ì¥ëœ ë°ì´í„° ë¡œë“œ ì˜ˆì‹œ
    print("\nğŸ“‚ ì €ì¥ëœ ë°ì´í„° ë¡œë“œ ì˜ˆì‹œ:")
    for category in data.keys():
        loaded_data = collector.load_from_storage(category)
        print(f"  - {category}: {len(loaded_data)}ê°œ í‹°ì»¤ ë¡œë“œë¨")
    
    # ì•Œë¦¼ ë°œì†¡
    collector.send_telegram_alert("ğŸ‰ KRX ë°ì´í„° ìˆ˜ì§‘ ë° ì„ íƒì  ì €ì¥ ì™„ë£Œ!")
    
    print("\nâœ… ëª¨ë“  ë°ì´í„° ìˆ˜ì§‘, ê·¹ê°’ ì¶”ì , ì„ íƒì  ì €ì¥ ì™„ë£Œ")
    print("ğŸ’¾ ì €ì¥ëœ íŒŒì¼ë“¤:")
    print("  - Parquet: ëŒ€ìš©ëŸ‰ ë°ì´í„° (ì••ì¶•ë¥  ë†’ìŒ)")
    print("  - SQLite: ë¹ ë¥¸ ì¡°íšŒìš© (ì¸ë±ì‹±)")
    print("  - SQL: ë°±ì—…/ì´ë™ìš© (í‘œì¤€ SQL)")
    print("  - HDF5: ë³µì¡í•œ êµ¬ì¡° (ë©”íƒ€ë°ì´í„° í¬í•¨)")