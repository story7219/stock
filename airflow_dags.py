#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íŒŒì¼ëª…: airflow_dags.py
ëª¨ë“ˆ: Apache Airflow DAG ì •ì˜
ëª©ì : ë°ì´í„° íŒŒì´í”„ë¼ì¸ ìë™í™” ë° ìŠ¤ì¼€ì¤„ë§

Author: AI Assistant
Created: 2025-01-27
Modified: 2025-01-27
Version: 1.0.0

Dependencies:
    - Python 3.11+
    - apache-airflow
    - pandas, numpy
    - requests, aiohttp
    - sqlalchemy
"""

from __future__ import annotations

import json
import logging
import os
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict, List, Optional

try:
    from airflow import DAG
    from airflow.decorators import dag, task
    from airflow.operators.python import PythonOperator
    from airflow.operators.bash import BashOperator
    from airflow.operators.email import EmailOperator
    from airflow.operators.trigger_dagrun import TriggerDagRunOperator
    from airflow.providers.postgres.operators.postgres import PostgresOperator
    from airflow.providers.redis.operators.redis import RedisOperator
    from airflow.providers.http.operators.http import SimpleHttpOperator
    from airflow.providers.telegram.operators.telegram import TelegramOperator
    AIRFLOW_AVAILABLE = True
except ImportError:
    AIRFLOW_AVAILABLE = False

try:
    import pandas as pd
    import numpy as np
    PD_AVAILABLE = True
    NP_AVAILABLE = True
except ImportError:
    PD_AVAILABLE = False
    NP_AVAILABLE = False

try:
    import requests
    import aiohttp
    REQUESTS_AVAILABLE = True
    AIOHTTP_AVAILABLE = True
except ImportError:
    REQUESTS_AVAILABLE = False
    AIOHTTP_AVAILABLE = False

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# ê¸°ë³¸ DAG ì„¤ì •
default_args = {
    'owner': 'trading_system',
    'depends_on_past': False,
    'start_date': datetime(2025, 1, 1),
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 3,
    'retry_delay': timedelta(minutes=5),
    'catchup': False
}


@dag(
    dag_id='data_collection_pipeline',
    default_args=default_args,
    description='ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸',
    schedule_interval='0 */4 * * *',  # 4ì‹œê°„ë§ˆë‹¤
    max_active_runs=1,
    tags=['data', 'collection', 'trading']
)
def data_collection_pipeline():
    """ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸ DAG"""

    @task(task_id='collect_stock_data')
    def collect_stock_data(**context) -> Dict[str, Any]:
        """ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            logger.info("ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
            
            # KIS APIë¥¼ í†µí•œ ë°ì´í„° ìˆ˜ì§‘ (ì˜ˆì‹œ)
            symbols = ["005930", "000660", "035420", "051910", "006400"]
            collected_data = []
            
            for symbol in symbols:
                # ì‹¤ì œë¡œëŠ” KIS API í˜¸ì¶œ
                data = {
                    'symbol': symbol,
                    'timestamp': datetime.now(),
                    'price': 50000 + (hash(symbol) % 10000),
                    'volume': 1000000 + (hash(symbol) % 500000)
                }
                collected_data.append(data)
            
            # ë°ì´í„° ì €ì¥
            if PD_AVAILABLE:
                df = pd.DataFrame(collected_data)
                output_path = f"/tmp/stock_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
                df.to_csv(output_path, index=False)
                logger.info(f"ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_path}")
            
            return {
                'status': 'success',
                'data_count': len(collected_data),
                'output_path': output_path if PD_AVAILABLE else None
            }
            
        except Exception as e:
            logger.error(f"ì£¼ì‹ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            raise

    @task(task_id='collect_news_data')
    def collect_news_data(**context) -> Dict[str, Any]:
        """ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            logger.info("ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
            
            # ë‰´ìŠ¤ API í˜¸ì¶œ (ì˜ˆì‹œ)
            news_data = [
                {
                    'title': 'ì‚¼ì„±ì „ì ì‹¤ì  ë°œí‘œ',
                    'content': 'ì‚¼ì„±ì „ìê°€ ì˜ˆìƒì¹˜ë¥¼ ìƒíšŒí•˜ëŠ” ì‹¤ì ì„ ë°œí‘œí–ˆìŠµë‹ˆë‹¤.',
                    'timestamp': datetime.now(),
                    'sentiment': 'positive'
                },
                {
                    'title': 'ë°˜ë„ì²´ ì‹œì¥ ë™í–¥',
                    'content': 'ê¸€ë¡œë²Œ ë°˜ë„ì²´ ì‹œì¥ì´ íšŒë³µì„¸ë¥¼ ë³´ì´ê³  ìˆìŠµë‹ˆë‹¤.',
                    'timestamp': datetime.now(),
                    'sentiment': 'positive'
                }
            ]
            
            # ë°ì´í„° ì €ì¥
            if PD_AVAILABLE:
                df = pd.DataFrame(news_data)
                output_path = f"/tmp/news_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
                df.to_csv(output_path, index=False)
                logger.info(f"ë‰´ìŠ¤ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {output_path}")
            
            return {
                'status': 'success',
                'data_count': len(news_data),
                'output_path': output_path if PD_AVAILABLE else None
            }
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            raise

    @task(task_id='process_data')
    def process_data(**context) -> Dict[str, Any]:
        """ë°ì´í„° ì „ì²˜ë¦¬"""
        try:
            logger.info("ë°ì´í„° ì „ì²˜ë¦¬ ì‹œì‘")
            
            # ì´ì „ íƒœìŠ¤í¬ ê²°ê³¼ ê°€ì ¸ì˜¤ê¸°
            ti = context['ti']
            stock_result = ti.xcom_pull(task_ids='collect_stock_data')
            news_result = ti.xcom_pull(task_ids='collect_news_data')
            
            if PD_AVAILABLE and stock_result and news_result:
                # ë°ì´í„° ë¡œë“œ ë° ì „ì²˜ë¦¬
                stock_df = pd.read_csv(stock_result['output_path'])
                news_df = pd.read_csv(news_result['output_path'])
                
                # ë°ì´í„° ì •ì œ
                stock_df = stock_df.dropna()
                news_df = news_df.dropna()
                
                # íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§
                stock_df['price_change'] = stock_df['price'].diff()
                stock_df['volume_ma'] = stock_df['volume'].rolling(window=5).mean()
                
                # ê²°ê³¼ ì €ì¥
                processed_path = f"/tmp/processed_data_{datetime.now().strftime('%Y%m%d_%H%M%S')}.parquet"
                stock_df.to_parquet(processed_path, index=False)
                
                logger.info(f"ë°ì´í„° ì „ì²˜ë¦¬ ì™„ë£Œ: {processed_path}")
                
                return {
                    'status': 'success',
                    'processed_path': processed_path,
                    'stock_count': len(stock_df),
                    'news_count': len(news_df)
                }
            else:
                logger.warning("ì´ì „ íƒœìŠ¤í¬ ê²°ê³¼ê°€ ì—†ê±°ë‚˜ pandasê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
                return {'status': 'skipped'}
                
        except Exception as e:
            logger.error(f"ë°ì´í„° ì „ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            raise

    @task(task_id='load_to_database')
    def load_to_database(**context) -> Dict[str, Any]:
        """ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ"""
        try:
            logger.info("ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì‹œì‘")
            
            ti = context['ti']
            process_result = ti.xcom_pull(task_ids='process_data')
            
            if process_result and process_result.get('processed_path'):
                # ë°ì´í„°ë² ì´ìŠ¤ì— ë¡œë“œ (ì˜ˆì‹œ)
                logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ: {process_result['processed_path']}")
                
                return {
                    'status': 'success',
                    'loaded_records': process_result.get('stock_count', 0)
                }
            else:
                logger.warning("ì „ì²˜ë¦¬ëœ ë°ì´í„°ê°€ ì—†ìŒ")
                return {'status': 'skipped'}
                
        except Exception as e:
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise

    @task(task_id='send_notification')
    def send_notification(**context) -> Dict[str, Any]:
        """ì•Œë¦¼ ì „ì†¡"""
        try:
            logger.info("ì•Œë¦¼ ì „ì†¡ ì‹œì‘")
            
            ti = context['ti']
            load_result = ti.xcom_pull(task_ids='load_to_database')
            
            if load_result and load_result.get('status') == 'success':
                message = f"ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {load_result.get('loaded_records', 0)}ê°œ ë ˆì½”ë“œ"
                logger.info(message)
                
                # ì‹¤ì œë¡œëŠ” Telegramì´ë‚˜ ì´ë©”ì¼ë¡œ ì „ì†¡
                return {
                    'status': 'success',
                    'message': message
                }
            else:
                logger.warning("ì•Œë¦¼ ì „ì†¡ ê±´ë„ˆëœ€")
                return {'status': 'skipped'}
                
        except Exception as e:
            logger.error(f"ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {e}")
            raise

    # íƒœìŠ¤í¬ ì‹¤í–‰ ìˆœì„œ ì •ì˜
    stock_data = collect_stock_data()
    news_data = collect_news_data()
    processed_data = process_data()
    loaded_data = load_to_database()
    notification = send_notification()
    
    # ì˜ì¡´ì„± ì„¤ì •
    [stock_data, news_data] >> processed_data >> loaded_data >> notification


@dag(
    dag_id='model_training_pipeline',
    default_args=default_args,
    description='ëª¨ë¸ í›ˆë ¨ íŒŒì´í”„ë¼ì¸',
    schedule_interval='0 2 * * 0',  # ë§¤ì£¼ ì¼ìš”ì¼ ìƒˆë²½ 2ì‹œ
    max_active_runs=1,
    tags=['model', 'training', 'ml']
)
def model_training_pipeline():
    """ëª¨ë¸ í›ˆë ¨ íŒŒì´í”„ë¼ì¸ DAG"""

    @task(task_id='prepare_training_data')
    def prepare_training_data(**context) -> Dict[str, Any]:
        """í›ˆë ¨ ë°ì´í„° ì¤€ë¹„"""
        try:
            logger.info("í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì‹œì‘")
            
            # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ í›ˆë ¨ ë°ì´í„° ë¡œë“œ
            if PD_AVAILABLE:
                # ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ ì¿¼ë¦¬
                data = pd.DataFrame({
                    'feature1': np.random.randn(1000),
                    'feature2': np.random.randn(1000),
                    'target': np.random.randn(1000)
                })
                
                # ë°ì´í„° ë¶„í• 
                train_size = int(len(data) * 0.8)
                train_data = data[:train_size]
                test_data = data[train_size:]
                
                # ì €ì¥
                train_path = "/tmp/train_data.parquet"
                test_path = "/tmp/test_data.parquet"
                train_data.to_parquet(train_path, index=False)
                test_data.to_parquet(test_path, index=False)
                
                logger.info(f"í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(train_data)}ê°œ í›ˆë ¨, {len(test_data)}ê°œ í…ŒìŠ¤íŠ¸")
                
                return {
                    'status': 'success',
                    'train_path': train_path,
                    'test_path': test_path,
                    'train_size': len(train_data),
                    'test_size': len(test_data)
                }
            else:
                logger.warning("pandasê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
                return {'status': 'skipped'}
                
        except Exception as e:
            logger.error(f"í›ˆë ¨ ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            raise

    @task(task_id='train_model')
    def train_model(**context) -> Dict[str, Any]:
        """ëª¨ë¸ í›ˆë ¨"""
        try:
            logger.info("ëª¨ë¸ í›ˆë ¨ ì‹œì‘")
            
            ti = context['ti']
            data_result = ti.xcom_pull(task_ids='prepare_training_data')
            
            if data_result and data_result.get('status') == 'success':
                # ì‹¤ì œë¡œëŠ” ML ëª¨ë¸ í›ˆë ¨
                logger.info("ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ (ì‹œë®¬ë ˆì´ì…˜)")
                
                model_path = f"/tmp/model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.pkl"
                
                return {
                    'status': 'success',
                    'model_path': model_path,
                    'train_size': data_result.get('train_size', 0)
                }
            else:
                logger.warning("í›ˆë ¨ ë°ì´í„°ê°€ ì—†ìŒ")
                return {'status': 'skipped'}
                
        except Exception as e:
            logger.error(f"ëª¨ë¸ í›ˆë ¨ ì‹¤íŒ¨: {e}")
            raise

    @task(task_id='evaluate_model')
    def evaluate_model(**context) -> Dict[str, Any]:
        """ëª¨ë¸ í‰ê°€"""
        try:
            logger.info("ëª¨ë¸ í‰ê°€ ì‹œì‘")
            
            ti = context['ti']
            train_result = ti.xcom_pull(task_ids='train_model')
            data_result = ti.xcom_pull(task_ids='prepare_training_data')
            
            if train_result and data_result:
                # ëª¨ë¸ í‰ê°€ (ì‹œë®¬ë ˆì´ì…˜)
                metrics = {
                    'accuracy': 0.85,
                    'precision': 0.82,
                    'recall': 0.88,
                    'f1_score': 0.85
                }
                
                logger.info(f"ëª¨ë¸ í‰ê°€ ì™„ë£Œ: {metrics}")
                
                return {
                    'status': 'success',
                    'metrics': metrics,
                    'model_path': train_result.get('model_path')
                }
            else:
                logger.warning("í›ˆë ¨ëœ ëª¨ë¸ì´ ì—†ìŒ")
                return {'status': 'skipped'}
                
        except Exception as e:
            logger.error(f"ëª¨ë¸ í‰ê°€ ì‹¤íŒ¨: {e}")
            raise

    @task(task_id='deploy_model')
    def deploy_model(**context) -> Dict[str, Any]:
        """ëª¨ë¸ ë°°í¬"""
        try:
            logger.info("ëª¨ë¸ ë°°í¬ ì‹œì‘")
            
            ti = context['ti']
            eval_result = ti.xcom_pull(task_ids='evaluate_model')
            
            if eval_result and eval_result.get('status') == 'success':
                metrics = eval_result.get('metrics', {})
                
                # ì„±ëŠ¥ ê¸°ì¤€ í™•ì¸
                if metrics.get('accuracy', 0) > 0.8:
                    logger.info("ëª¨ë¸ ë°°í¬ ì™„ë£Œ")
                    return {
                        'status': 'success',
                        'deployed_model': eval_result.get('model_path'),
                        'performance': metrics
                    }
                else:
                    logger.warning("ëª¨ë¸ ì„±ëŠ¥ì´ ê¸°ì¤€ì— ë¯¸ë‹¬")
                    return {'status': 'performance_insufficient'}
            else:
                logger.warning("í‰ê°€ëœ ëª¨ë¸ì´ ì—†ìŒ")
                return {'status': 'skipped'}
                
        except Exception as e:
            logger.error(f"ëª¨ë¸ ë°°í¬ ì‹¤íŒ¨: {e}")
            raise

    # íƒœìŠ¤í¬ ì‹¤í–‰ ìˆœì„œ ì •ì˜
    training_data = prepare_training_data()
    trained_model = train_model()
    evaluated_model = evaluate_model()
    deployed_model = deploy_model()
    
    # ì˜ì¡´ì„± ì„¤ì •
    training_data >> trained_model >> evaluated_model >> deployed_model


@dag(
    dag_id='backtest_pipeline',
    default_args=default_args,
    description='ë°±í…ŒìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸',
    schedule_interval='0 3 * * 1',  # ë§¤ì£¼ ì›”ìš”ì¼ ìƒˆë²½ 3ì‹œ
    max_active_runs=1,
    tags=['backtest', 'strategy', 'analysis']
)
def backtest_pipeline():
    """ë°±í…ŒìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸ DAG"""

    @task(task_id='prepare_backtest_data')
    def prepare_backtest_data(**context) -> Dict[str, Any]:
        """ë°±í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„"""
        try:
            logger.info("ë°±í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì‹œì‘")
            
            # ë°±í…ŒìŠ¤íŠ¸ ê¸°ê°„ ì„¤ì •
            start_date = datetime(2023, 1, 1)
            end_date = datetime(2024, 12, 31)
            
            if PD_AVAILABLE:
                # ì‹¤ì œë¡œëŠ” ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ë°ì´í„° ë¡œë“œ
                dates = pd.date_range(start_date, end_date, freq='D')
                data = pd.DataFrame({
                    'date': dates,
                    'price': np.random.randn(len(dates)).cumsum() + 100,
                    'volume': np.random.randint(1000, 10000, len(dates))
                })
                
                data_path = "/tmp/backtest_data.parquet"
                data.to_parquet(data_path, index=False)
                
                logger.info(f"ë°±í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì™„ë£Œ: {len(data)}ê°œ ë°ì´í„°")
                
                return {
                    'status': 'success',
                    'data_path': data_path,
                    'start_date': start_date.isoformat(),
                    'end_date': end_date.isoformat(),
                    'data_count': len(data)
                }
            else:
                logger.warning("pandasê°€ ì„¤ì¹˜ë˜ì§€ ì•ŠìŒ")
                return {'status': 'skipped'}
                
        except Exception as e:
            logger.error(f"ë°±í…ŒìŠ¤íŠ¸ ë°ì´í„° ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            raise

    @task(task_id='run_backtest')
    def run_backtest(**context) -> Dict[str, Any]:
        """ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        try:
            logger.info("ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹œì‘")
            
            ti = context['ti']
            data_result = ti.xcom_pull(task_ids='prepare_backtest_data')
            
            if data_result and data_result.get('status') == 'success':
                # ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ì‹œë®¬ë ˆì´ì…˜)
                results = {
                    'total_return': 0.15,
                    'annual_return': 0.12,
                    'max_drawdown': -0.08,
                    'sharpe_ratio': 1.2,
                    'win_rate': 0.65,
                    'total_trades': 150
                }
                
                results_path = f"/tmp/backtest_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
                with open(results_path, 'w') as f:
                    json.dump(results, f, indent=2)
                
                logger.info(f"ë°±í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {results}")
                
                return {
                    'status': 'success',
                    'results_path': results_path,
                    'results': results
                }
            else:
                logger.warning("ë°±í…ŒìŠ¤íŠ¸ ë°ì´í„°ê°€ ì—†ìŒ")
                return {'status': 'skipped'}
                
        except Exception as e:
            logger.error(f"ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
            raise

    @task(task_id='generate_report')
    def generate_report(**context) -> Dict[str, Any]:
        """ë°±í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„±"""
        try:
            logger.info("ë°±í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„± ì‹œì‘")
            
            ti = context['ti']
            backtest_result = ti.xcom_pull(task_ids='run_backtest')
            
            if backtest_result and backtest_result.get('status') == 'success':
                results = backtest_result.get('results', {})
                
                # ë¦¬í¬íŠ¸ ìƒì„± (ì‹œë®¬ë ˆì´ì…˜)
                report = f"""
                ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ ë¦¬í¬íŠ¸
                ===================
                ì´ ìˆ˜ìµë¥ : {results.get('total_return', 0):.2%}
                ì—°í‰ê·  ìˆ˜ìµë¥ : {results.get('annual_return', 0):.2%}
                ìµœëŒ€ ë‚™í­: {results.get('max_drawdown', 0):.2%}
                ìƒ¤í”„ ë¹„ìœ¨: {results.get('sharpe_ratio', 0):.2f}
                ìŠ¹ë¥ : {results.get('win_rate', 0):.2%}
                ì´ ê±°ë˜ íšŸìˆ˜: {results.get('total_trades', 0)}íšŒ
                """
                
                report_path = f"/tmp/backtest_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
                with open(report_path, 'w', encoding='utf-8') as f:
                    f.write(report)
                
                logger.info("ë°±í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ")
                
                return {
                    'status': 'success',
                    'report_path': report_path,
                    'summary': results
                }
            else:
                logger.warning("ë°±í…ŒìŠ¤íŠ¸ ê²°ê³¼ê°€ ì—†ìŒ")
                return {'status': 'skipped'}
                
        except Exception as e:
            logger.error(f"ë°±í…ŒìŠ¤íŠ¸ ë¦¬í¬íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            raise

    # íƒœìŠ¤í¬ ì‹¤í–‰ ìˆœì„œ ì •ì˜
    backtest_data = prepare_backtest_data()
    backtest_results = run_backtest()
    backtest_report = generate_report()
    
    # ì˜ì¡´ì„± ì„¤ì •
    backtest_data >> backtest_results >> backtest_report


@dag(
    dag_id='monitoring_pipeline',
    default_args=default_args,
    description='ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ íŒŒì´í”„ë¼ì¸',
    schedule_interval='*/30 * * * *',  # 30ë¶„ë§ˆë‹¤
    max_active_runs=1,
    tags=['monitoring', 'health', 'alert']
)
def monitoring_pipeline():
    """ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ íŒŒì´í”„ë¼ì¸ DAG"""

    @task(task_id='check_system_health')
    def check_system_health(**context) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"""
        try:
            logger.info("ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì‹œì‘")
            
            # ì‹œìŠ¤í…œ ìƒíƒœ ì²´í¬ (ì‹œë®¬ë ˆì´ì…˜)
            health_status = {
                'database': 'healthy',
                'api': 'healthy',
                'storage': 'healthy',
                'memory_usage': 0.65,
                'cpu_usage': 0.45,
                'disk_usage': 0.30
            }
            
            # ì„ê³„ê°’ í™•ì¸
            alerts = []
            if health_status['memory_usage'] > 0.8:
                alerts.append('ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ')
            if health_status['cpu_usage'] > 0.8:
                alerts.append('CPU ì‚¬ìš©ëŸ‰ ë†’ìŒ')
            if health_status['disk_usage'] > 0.8:
                alerts.append('ë””ìŠ¤í¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ')
            
            logger.info(f"ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì™„ë£Œ: {health_status}")
            
            return {
                'status': 'success',
                'health': health_status,
                'alerts': alerts,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            raise

    @task(task_id='check_data_quality')
    def check_data_quality(**context) -> Dict[str, Any]:
        """ë°ì´í„° í’ˆì§ˆ í™•ì¸"""
        try:
            logger.info("ë°ì´í„° í’ˆì§ˆ í™•ì¸ ì‹œì‘")
            
            # ë°ì´í„° í’ˆì§ˆ ì²´í¬ (ì‹œë®¬ë ˆì´ì…˜)
            quality_metrics = {
                'completeness': 0.98,
                'accuracy': 0.95,
                'consistency': 0.92,
                'timeliness': 0.99,
                'validity': 0.94
            }
            
            # í’ˆì§ˆ ì´ìŠˆ í™•ì¸
            issues = []
            for metric, value in quality_metrics.items():
                if value < 0.9:
                    issues.append(f"{metric}: {value:.2%}")
            
            logger.info(f"ë°ì´í„° í’ˆì§ˆ í™•ì¸ ì™„ë£Œ: {quality_metrics}")
            
            return {
                'status': 'success',
                'quality': quality_metrics,
                'issues': issues,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"ë°ì´í„° í’ˆì§ˆ í™•ì¸ ì‹¤íŒ¨: {e}")
            raise

    @task(task_id='send_alerts')
    def send_alerts(**context) -> Dict[str, Any]:
        """ì•Œë¦¼ ì „ì†¡"""
        try:
            logger.info("ì•Œë¦¼ ì „ì†¡ ì‹œì‘")
            
            ti = context['ti']
            health_result = ti.xcom_pull(task_ids='check_system_health')
            quality_result = ti.xcom_pull(task_ids='check_data_quality')
            
            alerts = []
            
            if health_result:
                alerts.extend(health_result.get('alerts', []))
            
            if quality_result:
                alerts.extend(quality_result.get('issues', []))
            
            if alerts:
                message = f"ì‹œìŠ¤í…œ ì•Œë¦¼:\n" + "\n".join(alerts)
                logger.warning(f"ì•Œë¦¼ ì „ì†¡: {message}")
                
                return {
                    'status': 'success',
                    'alerts_sent': len(alerts),
                    'message': message
                }
            else:
                logger.info("ì•Œë¦¼ ì—†ìŒ")
                return {
                    'status': 'success',
                    'alerts_sent': 0,
                    'message': 'ëª¨ë“  ì‹œìŠ¤í…œ ì •ìƒ'
                }
                
        except Exception as e:
            logger.error(f"ì•Œë¦¼ ì „ì†¡ ì‹¤íŒ¨: {e}")
            raise

    # íƒœìŠ¤í¬ ì‹¤í–‰ ìˆœì„œ ì •ì˜
    system_health = check_system_health()
    data_quality = check_data_quality()
    alerts = send_alerts()
    
    # ì˜ì¡´ì„± ì„¤ì •
    [system_health, data_quality] >> alerts


# DAG ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
if AIRFLOW_AVAILABLE:
    data_collection_dag = data_collection_pipeline()
    model_training_dag = model_training_pipeline()
    backtest_dag = backtest_pipeline()
    monitoring_dag = monitoring_pipeline()
else:
    logger.warning("Apache Airflowê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ DAGë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")


def create_custom_dag(dag_id: str, schedule: str, tasks: List[Dict[str, Any]]) -> Optional[DAG]:
    """ì»¤ìŠ¤í…€ DAG ìƒì„±"""
    if not AIRFLOW_AVAILABLE:
        logger.warning("Apache Airflowê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ DAGë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return None

    dag = DAG(
        dag_id=dag_id,
        default_args=default_args,
        description=f'ì»¤ìŠ¤í…€ DAG: {dag_id}',
        schedule_interval=schedule,
        max_active_runs=1,
        tags=['custom']
    )

    with dag:
        for i, task_config in enumerate(tasks):
            task_id = task_config.get('task_id', f'task_{i}')
            task_type = task_config.get('type', 'python')
            
            if task_type == 'python':
                def task_function(**context):
                    logger.info(f"ì»¤ìŠ¤í…€ íƒœìŠ¤í¬ ì‹¤í–‰: {task_id}")
                    return {'status': 'success', 'task_id': task_id}
                
                PythonOperator(
                    task_id=task_id,
                    python_callable=task_function
                )
            elif task_type == 'bash':
                BashOperator(
                    task_id=task_id,
                    bash_command=task_config.get('command', 'echo "Hello World"')
                )

    return dag


async def main() -> None:
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ Apache Airflow DAG ì‹œìŠ¤í…œ ì‹œì‘")
    print("=" * 60)

    if not AIRFLOW_AVAILABLE:
        print("âŒ Apache Airflowê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•˜ì„¸ìš”:")
        print("pip install apache-airflow")
        return

    print("âœ… Apache Airflow DAG ì‹œìŠ¤í…œ ì¤€ë¹„ ì™„ë£Œ")
    print("\nğŸ“‹ ìƒì„±ëœ DAG ëª©ë¡:")
    print("  - data_collection_pipeline: ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸")
    print("  - model_training_pipeline: ëª¨ë¸ í›ˆë ¨ íŒŒì´í”„ë¼ì¸")
    print("  - backtest_pipeline: ë°±í…ŒìŠ¤íŠ¸ íŒŒì´í”„ë¼ì¸")
    print("  - monitoring_pipeline: ì‹œìŠ¤í…œ ëª¨ë‹ˆí„°ë§ íŒŒì´í”„ë¼ì¸")

    # ì»¤ìŠ¤í…€ DAG ì˜ˆì‹œ
    custom_tasks = [
        {'task_id': 'custom_task_1', 'type': 'python'},
        {'task_id': 'custom_task_2', 'type': 'bash', 'command': 'echo "Custom task"'},
        {'task_id': 'custom_task_3', 'type': 'python'}
    ]
    
    custom_dag = create_custom_dag('custom_pipeline', '0 1 * * *', custom_tasks)
    if custom_dag:
        print("  - custom_pipeline: ì»¤ìŠ¤í…€ íŒŒì´í”„ë¼ì¸")

    print("\nâœ… DAG ìƒì„± ì™„ë£Œ")


if __name__ == "__main__":
    import asyncio
    asyncio.run(main())

