#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ÌååÏùºÎ™Ö: ml_training_example.py
Î™®Îìà: Î®∏Ïã†Îü¨Îãù/Îî•Îü¨Îãù ÌïôÏäµ ÏòàÏ†ú
Î™©Ï†Å: Îã§ÏñëÌïú ML/DL Î™®Îç∏ ÌïôÏäµ Î∞è ÏòàÏ∏° ÏòàÏ†ú

Author: AI Trading System
Created: 2025-01-27
Version: 1.0.0

Dependencies:
    - Python 3.11+
    - tensorflow==2.19.0
    - torch==2.1.2
    - scikit-learn==1.7.0
    - xgboost==3.0.2
    - lightgbm==4.6.0
    - pandas==2.1.4
    - numpy==1.24.0

License: MIT
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any
import warnings
warnings.filterwarnings('ignore')

# Î®∏Ïã†Îü¨Îãù ÎùºÏù¥Î∏åÎü¨Î¶¨
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.linear_model import LinearRegression, Ridge, Lasso
from sklearn.svm import SVR
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split, TimeSeriesSplit
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Îî•Îü¨Îãù ÎùºÏù¥Î∏åÎü¨Î¶¨
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

# Î∂ÄÏä§ÌåÖ ÎùºÏù¥Î∏åÎü¨Î¶¨
import xgboost as xgb
import lightgbm as lgb

# PyTorch (ÏÑ†ÌÉùÏÇ¨Ìï≠)
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    from torch.utils.data import DataLoader, TensorDataset
    PYTORCH_AVAILABLE = True
except ImportError:
    PYTORCH_AVAILABLE = False
    print("‚ö†Ô∏è PyTorchÎ•º ÏÇ¨Ïö©Ìï† Ïàò ÏóÜÏäµÎãàÎã§.")


class MLTrainingExample:
    """Î®∏Ïã†Îü¨Îãù/Îî•Îü¨Îãù ÌïôÏäµ ÏòàÏ†ú"""
    
    def __init__(self):
        """Ï¥àÍ∏∞Ìôî"""
        self.scaler = StandardScaler()
        self.models = {}
        self.results = {}
        
    def generate_sample_data(self, n_samples: int = 1000) -> Tuple[pd.DataFrame, pd.Series]:
        """ÏÉòÌîå Ï£ºÏãù Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±"""
        np.random.seed(42)
        
        # ÏãúÍ∞Ñ Ïù∏Îç±Ïä§ ÏÉùÏÑ±
        dates = pd.date_range(start='2020-01-01', periods=n_samples, freq='D')
        
        # Í∏∞Î≥∏ Í∞ÄÍ≤© Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±
        base_price = 100
        returns = np.random.normal(0.001, 0.02, n_samples)  # ÏùºÍ∞Ñ ÏàòÏùµÎ•†
        prices = [base_price]
        
        for ret in returns[1:]:
            prices.append(prices[-1] * (1 + ret))
        
        # Í∏∞Ïà†Ï†Å ÏßÄÌëú ÏÉùÏÑ±
        df = pd.DataFrame({
            'Date': dates,
            'Close': prices,
            'Volume': np.random.lognormal(10, 1, n_samples),
            'High': [p * (1 + abs(np.random.normal(0, 0.01))) for p in prices],
            'Low': [p * (1 - abs(np.random.normal(0, 0.01))) for p in prices],
            'Open': [p * (1 + np.random.normal(0, 0.005)) for p in prices]
        })
        
        # Í∏∞Ïà†Ï†Å ÏßÄÌëú Ï∂îÍ∞Ä
        df['MA5'] = df['Close'].rolling(5).mean()
        df['MA20'] = df['Close'].rolling(20).mean()
        df['RSI'] = self._calculate_rsi(df['Close'])
        df['Volatility'] = df['Close'].rolling(20).std()
        df['Price_Change'] = df['Close'].pct_change()
        df['Volume_Change'] = df['Volume'].pct_change()
        
        # ÌÉÄÍ≤ü Î≥ÄÏàò (Îã§ÏùåÎÇ† ÏàòÏùµÎ•†)
        df['Target'] = df['Close'].shift(-1) / df['Close'] - 1
        
        # Í≤∞Ï∏°Ïπò Ï†úÍ±∞
        df = df.dropna()
        
        return df
    
    def _calculate_rsi(self, prices: pd.Series, period: int = 14) -> pd.Series:
        """RSI Í≥ÑÏÇ∞"""
        delta = prices.diff()
        gain = (delta.where(delta > 0, 0)).rolling(window=period).mean()
        loss = (-delta.where(delta < 0, 0)).rolling(window=period).mean()
        rs = gain / loss
        rsi = 100 - (100 / (1 + rs))
        return rsi
    
    def prepare_data(self, df: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
        """Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ"""
        # ÌîºÏ≤ò ÏÑ†ÌÉù
        feature_cols = ['Close', 'Volume', 'MA5', 'MA20', 'RSI', 'Volatility', 
                       'Price_Change', 'Volume_Change']
        
        X = df[feature_cols].values
        y = df['Target'].values
        
        # Ïä§ÏºÄÏùºÎßÅ
        X_scaled = self.scaler.fit_transform(X)
        
        return X_scaled, y
    
    def train_traditional_ml_models(self, X: np.ndarray, y: np.ndarray) -> Dict[str, Any]:
        """Ï†ÑÌÜµÏ†ÅÏù∏ Î®∏Ïã†Îü¨Îãù Î™®Îç∏ ÌõàÎ†®"""
        print("ü§ñ Ï†ÑÌÜµÏ†ÅÏù∏ Î®∏Ïã†Îü¨Îãù Î™®Îç∏ ÌõàÎ†® Ï§ë...")
        
        # Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        models = {
            'Linear Regression': LinearRegression(),
            'Ridge Regression': Ridge(alpha=1.0),
            'Lasso Regression': Lasso(alpha=0.1),
            'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),
            'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),
            'SVR': SVR(kernel='rbf', C=1.0, gamma='scale'),
            'XGBoost': xgb.XGBRegressor(n_estimators=100, random_state=42),
            'LightGBM': lgb.LGBMRegressor(n_estimators=100, random_state=42)
        }
        
        results = {}
        
        for name, model in models.items():
            try:
                print(f"  üìä {name} ÌõàÎ†® Ï§ë...")
                
                # Î™®Îç∏ ÌõàÎ†®
                model.fit(X_train, y_train)
                
                # ÏòàÏ∏°
                y_pred = model.predict(X_test)
                
                # ÌèâÍ∞Ä
                mse = mean_squared_error(y_test, y_pred)
                mae = mean_absolute_error(y_test, y_pred)
                r2 = r2_score(y_test, y_pred)
                
                results[name] = {
                    'model': model,
                    'mse': mse,
                    'mae': mae,
                    'r2': r2,
                    'predictions': y_pred
                }
                
                print(f"    ‚úÖ {name}: MSE={mse:.6f}, MAE={mae:.6f}, R¬≤={r2:.4f}")
                
            except Exception as e:
                print(f"    ‚ùå {name} ÌõàÎ†® Ïã§Ìå®: {e}")
                continue
        
        self.models.update(models)
        self.results['traditional_ml'] = results
        
        return results
    
    def train_deep_learning_models(self, X: np.ndarray, y: np.ndarray) -> Dict[str, Any]:
        """Îî•Îü¨Îãù Î™®Îç∏ ÌõàÎ†®"""
        print("üß† Îî•Îü¨Îãù Î™®Îç∏ ÌõàÎ†® Ï§ë...")
        
        # Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # ÏãúÍ≥ÑÏó¥ Îç∞Ïù¥ÌÑ∞Î°ú Î≥ÄÌôò (LSTMÏö©)
        sequence_length = 10
        X_train_seq, y_train_seq = self._create_sequences(X_train, y_train, sequence_length)
        X_test_seq, y_test_seq = self._create_sequences(X_test, y_test, sequence_length)
        
        models = {}
        results = {}
        
        # 1. Dense Neural Network
        print("  üìä Dense Neural Network ÌõàÎ†® Ï§ë...")
        dense_model = self._build_dense_model(X.shape[1])
        dense_history = dense_model.fit(
            X_train, y_train,
            validation_data=(X_test, y_test),
            epochs=50,
            batch_size=32,
            callbacks=[
                EarlyStopping(patience=10, restore_best_weights=True),
                ModelCheckpoint('best_dense_model.h5', save_best_only=True)
            ],
            verbose=0
        )
        
        y_pred_dense = dense_model.predict(X_test)
        results['Dense Neural Network'] = {
            'model': dense_model,
            'history': dense_history,
            'mse': mean_squared_error(y_test, y_pred_dense),
            'mae': mean_absolute_error(y_test, y_pred_dense),
            'r2': r2_score(y_test, y_pred_dense),
            'predictions': y_pred_dense
        }
        
        # 2. LSTM Model
        print("  üìä LSTM Model ÌõàÎ†® Ï§ë...")
        lstm_model = self._build_lstm_model(sequence_length, X.shape[1])
        lstm_history = lstm_model.fit(
            X_train_seq, y_train_seq,
            validation_data=(X_test_seq, y_test_seq),
            epochs=50,
            batch_size=32,
            callbacks=[
                EarlyStopping(patience=10, restore_best_weights=True),
                ModelCheckpoint('best_lstm_model.h5', save_best_only=True)
            ],
            verbose=0
        )
        
        y_pred_lstm = lstm_model.predict(X_test_seq)
        results['LSTM Model'] = {
            'model': lstm_model,
            'history': lstm_history,
            'mse': mean_squared_error(y_test_seq, y_pred_lstm),
            'mae': mean_absolute_error(y_test_seq, y_pred_lstm),
            'r2': r2_score(y_test_seq, y_pred_lstm),
            'predictions': y_pred_lstm
        }
        
        self.results['deep_learning'] = results
        
        return results
    
    def _create_sequences(self, X: np.ndarray, y: np.ndarray, sequence_length: int) -> Tuple[np.ndarray, np.ndarray]:
        """ÏãúÍ≥ÑÏó¥ ÏãúÌÄÄÏä§ ÏÉùÏÑ±"""
        X_seq, y_seq = [], []
        
        for i in range(len(X) - sequence_length):
            X_seq.append(X[i:i+sequence_length])
            y_seq.append(y[i+sequence_length])
        
        return np.array(X_seq), np.array(y_seq)
    
    def _build_dense_model(self, input_dim: int) -> keras.Model:
        """Dense Neural Network Î™®Îç∏ Íµ¨Ï∂ï"""
        model = Sequential([
            Dense(128, activation='relu', input_shape=(input_dim,)),
            BatchNormalization(),
            Dropout(0.3),
            Dense(64, activation='relu'),
            BatchNormalization(),
            Dropout(0.2),
            Dense(32, activation='relu'),
            Dense(1, activation='linear')
        ])
        
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae']
        )
        
        return model
    
    def _build_lstm_model(self, sequence_length: int, input_dim: int) -> keras.Model:
        """LSTM Î™®Îç∏ Íµ¨Ï∂ï"""
        model = Sequential([
            LSTM(128, return_sequences=True, input_shape=(sequence_length, input_dim)),
            Dropout(0.3),
            LSTM(64, return_sequences=False),
            Dropout(0.2),
            Dense(32, activation='relu'),
            Dense(1, activation='linear')
        ])
        
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='mse',
            metrics=['mae']
        )
        
        return model
    
    def train_pytorch_models(self, X: np.ndarray, y: np.ndarray) -> Dict[str, Any]:
        """PyTorch Î™®Îç∏ ÌõàÎ†® (ÏÑ†ÌÉùÏÇ¨Ìï≠)"""
        if not PYTORCH_AVAILABLE:
            print("‚ö†Ô∏è PyTorchÎ•º ÏÇ¨Ïö©Ìï† Ïàò ÏóÜÏäµÎãàÎã§.")
            return {}
        
        print("üî• PyTorch Î™®Îç∏ ÌõàÎ†® Ï§ë...")
        
        # Îç∞Ïù¥ÌÑ∞ Î∂ÑÌï†
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )
        
        # ÌÖêÏÑú Î≥ÄÌôò
        X_train_tensor = torch.FloatTensor(X_train)
        y_train_tensor = torch.FloatTensor(y_train)
        X_test_tensor = torch.FloatTensor(X_test)
        y_test_tensor = torch.FloatTensor(y_test)
        
        # Îç∞Ïù¥ÌÑ∞Î°úÎçî ÏÉùÏÑ±
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
        
        # Î™®Îç∏ Ï†ïÏùò
        class PyTorchNN(nn.Module):
            def __init__(self, input_dim):
                super(PyTorchNN, self).__init__()
                self.fc1 = nn.Linear(input_dim, 128)
                self.fc2 = nn.Linear(128, 64)
                self.fc3 = nn.Linear(64, 32)
                self.fc4 = nn.Linear(32, 1)
                self.dropout = nn.Dropout(0.3)
                self.batch_norm1 = nn.BatchNorm1d(128)
                self.batch_norm2 = nn.BatchNorm1d(64)
                self.batch_norm3 = nn.BatchNorm1d(32)
                
            def forward(self, x):
                x = torch.relu(self.batch_norm1(self.fc1(x)))
                x = self.dropout(x)
                x = torch.relu(self.batch_norm2(self.fc2(x)))
                x = self.dropout(x)
                x = torch.relu(self.batch_norm3(self.fc3(x)))
                x = self.fc4(x)
                return x
        
        # Î™®Îç∏ ÌõàÎ†®
        model = PyTorchNN(X.shape[1])
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=0.001)
        
        model.train()
        for epoch in range(50):
            for batch_X, batch_y in train_loader:
                optimizer.zero_grad()
                outputs = model(batch_X)
                loss = criterion(outputs.squeeze(), batch_y)
                loss.backward()
                optimizer.step()
        
        # ÏòàÏ∏°
        model.eval()
        with torch.no_grad():
            y_pred = model(X_test_tensor).squeeze().numpy()
        
        results = {
            'PyTorch Neural Network': {
                'model': model,
                'mse': mean_squared_error(y_test, y_pred),
                'mae': mean_absolute_error(y_test, y_pred),
                'r2': r2_score(y_test, y_pred),
                'predictions': y_pred
            }
        }
        
        self.results['pytorch'] = results
        
        return results
    
    def compare_models(self) -> pd.DataFrame:
        """Î™®Îç∏ ÏÑ±Îä• ÎπÑÍµê"""
        print("üìä Î™®Îç∏ ÏÑ±Îä• ÎπÑÍµê Ï§ë...")
        
        comparison_data = []
        
        for category, results in self.results.items():
            for model_name, result in results.items():
                comparison_data.append({
                    'Category': category,
                    'Model': model_name,
                    'MSE': result['mse'],
                    'MAE': result['mae'],
                    'R¬≤': result['r2']
                })
        
        df_comparison = pd.DataFrame(comparison_data)
        
        # ÏÑ±Îä•Î≥Ñ Ï†ïÎ†¨
        df_comparison = df_comparison.sort_values('R¬≤', ascending=False)
        
        print("\nüèÜ Î™®Îç∏ ÏÑ±Îä• ÏàúÏúÑ:")
        print(df_comparison.to_string(index=False))
        
        return df_comparison
    
    def plot_results(self):
        """Í≤∞Í≥º ÏãúÍ∞ÅÌôî"""
        print("üìà Í≤∞Í≥º ÏãúÍ∞ÅÌôî Ï§ë...")
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        
        # 1. R¬≤ Ï†êÏàò ÎπÑÍµê
        comparison_df = self.compare_models()
        axes[0, 0].barh(comparison_df['Model'], comparison_df['R¬≤'])
        axes[0, 0].set_title('Model R¬≤ Scores')
        axes[0, 0].set_xlabel('R¬≤ Score')
        
        # 2. MSE ÎπÑÍµê
        axes[0, 1].barh(comparison_df['Model'], comparison_df['MSE'])
        axes[0, 1].set_title('Model MSE Scores')
        axes[0, 1].set_xlabel('MSE')
        
        # 3. Ïã§Ï†úÍ∞í vs ÏòàÏ∏°Í∞í (ÏµúÍ≥† ÏÑ±Îä• Î™®Îç∏)
        best_model_name = comparison_df.iloc[0]['Model']
        best_category = comparison_df.iloc[0]['Category']
        best_predictions = self.results[best_category][best_model_name]['predictions']
        
        # Ïã§Ï†úÍ∞í Í∞ÄÏ†∏Ïò§Í∏∞ (ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞)
        _, _, _, y_test = train_test_split(
            self.X, self.y, test_size=0.2, random_state=42
        )
        
        axes[1, 0].scatter(y_test, best_predictions, alpha=0.6)
        axes[1, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)
        axes[1, 0].set_xlabel('Actual Values')
        axes[1, 0].set_ylabel('Predicted Values')
        axes[1, 0].set_title(f'Actual vs Predicted ({best_model_name})')
        
        # 4. ÏòàÏ∏°Í∞í Î∂ÑÌè¨
        axes[1, 1].hist(best_predictions, bins=30, alpha=0.7, label='Predictions')
        axes[1, 1].hist(y_test, bins=30, alpha=0.7, label='Actual')
        axes[1, 1].set_xlabel('Values')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].set_title('Distribution Comparison')
        axes[1, 1].legend()
        
        plt.tight_layout()
        plt.savefig('ml_training_results.png', dpi=300, bbox_inches='tight')
        plt.show()
    
    def run_complete_example(self):
        """ÏôÑÏ†ÑÌïú ÌïôÏäµ ÏòàÏ†ú Ïã§Ìñâ"""
        print("üöÄ Î®∏Ïã†Îü¨Îãù/Îî•Îü¨Îãù ÌïôÏäµ ÏòàÏ†ú ÏãúÏûë")
        print("=" * 50)
        
        # 1. Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±
        print("üìä ÏÉòÌîå Îç∞Ïù¥ÌÑ∞ ÏÉùÏÑ± Ï§ë...")
        df = self.generate_sample_data(1000)
        print(f"  ‚úÖ {len(df)} Í∞úÏùò Îç∞Ïù¥ÌÑ∞ Ìè¨Ïù∏Ìä∏ ÏÉùÏÑ± ÏôÑÎ£å")
        
        # 2. Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ
        print("üîß Îç∞Ïù¥ÌÑ∞ Ï§ÄÎπÑ Ï§ë...")
        X, y = self.prepare_data(df)
        self.X, self.y = X, y
        print(f"  ‚úÖ ÌîºÏ≤ò: {X.shape[1]}Í∞ú, ÏÉòÌîå: {X.shape[0]}Í∞ú")
        
        # 3. Ï†ÑÌÜµÏ†ÅÏù∏ Î®∏Ïã†Îü¨Îãù Î™®Îç∏ ÌõàÎ†®
        print("\nü§ñ Ï†ÑÌÜµÏ†ÅÏù∏ Î®∏Ïã†Îü¨Îãù Î™®Îç∏ ÌõàÎ†®")
        self.train_traditional_ml_models(X, y)
        
        # 4. Îî•Îü¨Îãù Î™®Îç∏ ÌõàÎ†®
        print("\nüß† Îî•Îü¨Îãù Î™®Îç∏ ÌõàÎ†®")
        self.train_deep_learning_models(X, y)
        
        # 5. PyTorch Î™®Îç∏ ÌõàÎ†® (ÏÑ†ÌÉùÏÇ¨Ìï≠)
        print("\nüî• PyTorch Î™®Îç∏ ÌõàÎ†®")
        self.train_pytorch_models(X, y)
        
        # 6. Î™®Îç∏ ÎπÑÍµê
        print("\nüìä Î™®Îç∏ ÏÑ±Îä• ÎπÑÍµê")
        comparison_df = self.compare_models()
        
        # 7. Í≤∞Í≥º ÏãúÍ∞ÅÌôî
        print("\nüìà Í≤∞Í≥º ÏãúÍ∞ÅÌôî")
        self.plot_results()
        
        print("\n‚úÖ ÌïôÏäµ ÏòàÏ†ú ÏôÑÎ£å!")
        print(f"üìÅ Í≤∞Í≥º Ïù¥ÎØ∏ÏßÄ: ml_training_results.png")
        
        return comparison_df


def main():
    """Î©îÏù∏ Ìï®Ïàò"""
    print("üéØ Î®∏Ïã†Îü¨Îãù/Îî•Îü¨Îãù ÌïôÏäµ ÏòàÏ†ú")
    print("=" * 50)
    
    # ÏòàÏ†ú Ïã§Ìñâ
    example = MLTrainingExample()
    results = example.run_complete_example()
    
    print("\nüéâ Î™®Îì† ÌïôÏäµÏù¥ ÏôÑÎ£åÎêòÏóàÏäµÎãàÎã§!")
    print("üìä ÏµúÍ≥† ÏÑ±Îä• Î™®Îç∏:", results.iloc[0]['Model'])
    print("üìà R¬≤ Ï†êÏàò:", results.iloc[0]['R¬≤'])


if __name__ == "__main__":
    main() 