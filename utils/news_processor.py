#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ“° ì‹¤ì‹œê°„ ë‰´ìŠ¤ ë°ì´í„° ì²˜ë¦¬ ì‹œìŠ¤í…œ
Gemini 1.5 Flash ëª¨ë¸ ì „ìš© ë‰´ìŠ¤ ë¶„ì„ ë° ê°€ê³µ

Features:
- ì¸ë² ìŠ¤íŒ…ë‹·ì»´ ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘
- Gemini 1.5 Flash ìµœì í™” ë°ì´í„° ê°€ê³µ
- ì£¼ì‹ ê´€ë ¨ ë‰´ìŠ¤ í•„í„°ë§ ë° ë¶„ë¥˜
- ê°ì • ë¶„ì„ ë° ì˜í–¥ë„ í‰ê°€
- í•œêµ­ì–´ ë²ˆì—­ ë° ìš”ì•½
"""

import asyncio
import json
import logging
import time
import re
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from enum import Enum
import requests
from bs4 import BeautifulSoup
import feedparser
import google.generativeai as genai
from dotenv import load_dotenv
import os

# í™˜ê²½ ì„¤ì •
load_dotenv()
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class NewsCategory(Enum):
    """ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬"""
    MARKET = "ì‹œì¥"
    COMPANY = "ê¸°ì—…"
    ECONOMIC = "ê²½ì œ"
    TECHNOLOGY = "ê¸°ìˆ "
    POLICY = "ì •ì±…"
    GLOBAL = "í•´ì™¸"
    OTHER = "ê¸°íƒ€"

class SentimentType(Enum):
    """ê°ì • ë¶„ì„ íƒ€ì…"""
    VERY_POSITIVE = "ë§¤ìš°ê¸ì •"
    POSITIVE = "ê¸ì •"
    NEUTRAL = "ì¤‘ë¦½"
    NEGATIVE = "ë¶€ì •"
    VERY_NEGATIVE = "ë§¤ìš°ë¶€ì •"

@dataclass
class NewsData:
    """ë‰´ìŠ¤ ë°ì´í„° êµ¬ì¡°"""
    title: str
    content: str
    url: str
    published_time: datetime
    source: str
    category: NewsCategory
    sentiment: SentimentType
    impact_score: float  # 0-100
    related_stocks: List[str]
    keywords: List[str]
    summary: str
    translated_title: str = ""
    translated_content: str = ""

class InvestingNewsCollector:
    """ì¸ë² ìŠ¤íŒ…ë‹·ì»´ ë‰´ìŠ¤ ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        self.base_url = "https://www.investing.com"
        self.rss_feeds = {
            "general": "https://www.investing.com/rss/news.rss",
            "stock_news": "https://www.investing.com/rss/news_285.rss",
            "economic": "https://www.investing.com/rss/news_95.rss",
            "forex": "https://www.investing.com/rss/news_1.rss"
        }
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        self.session = requests.Session()
        self.session.headers.update(self.headers)
    
    async def collect_latest_news(self, hours_back: int = 24, max_articles: int = 50) -> List[Dict[str, Any]]:
        """ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘"""
        logger.info(f"ğŸ“° ì¸ë² ìŠ¤íŒ…ë‹·ì»´ì—ì„œ ìµœê·¼ {hours_back}ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘")
        
        all_news = []
        cutoff_time = datetime.now() - timedelta(hours=hours_back)
        
        for feed_name, rss_url in self.rss_feeds.items():
            try:
                logger.info(f"ğŸ“¡ {feed_name} RSS í”¼ë“œ ìˆ˜ì§‘ ì¤‘...")
                feed_news = await self._parse_rss_feed(rss_url, cutoff_time)
                all_news.extend(feed_news)
                
                # API í˜¸ì¶œ ì œí•œ ë°©ì§€
                await asyncio.sleep(1)
                
            except Exception as e:
                logger.error(f"RSS í”¼ë“œ ìˆ˜ì§‘ ì‹¤íŒ¨ ({feed_name}): {e}")
        
        # ì¤‘ë³µ ì œê±° ë° ì‹œê°„ìˆœ ì •ë ¬
        unique_news = self._remove_duplicates(all_news)
        sorted_news = sorted(unique_news, key=lambda x: x['published_time'], reverse=True)
        
        # ìµœëŒ€ ê¸°ì‚¬ ìˆ˜ ì œí•œ
        final_news = sorted_news[:max_articles]
        
        logger.info(f"âœ… ì´ {len(final_news)}ê°œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ")
        return final_news
    
    async def _parse_rss_feed(self, rss_url: str, cutoff_time: datetime) -> List[Dict[str, Any]]:
        """RSS í”¼ë“œ íŒŒì‹±"""
        try:
            # RSS í”¼ë“œ íŒŒì‹±
            feed = await asyncio.to_thread(feedparser.parse, rss_url)
            
            if not feed.entries:
                return []
            
            news_list = []
            
            for entry in feed.entries:
                try:
                    # ë°œí–‰ ì‹œê°„ íŒŒì‹±
                    pub_time = self._parse_publish_time(entry)
                    
                    # ì‹œê°„ í•„í„°ë§
                    if pub_time < cutoff_time:
                        continue
                    
                    # ë‰´ìŠ¤ ë°ì´í„° êµ¬ì„±
                    news_item = {
                        'title': entry.title,
                        'url': entry.link,
                        'published_time': pub_time,
                        'source': 'Investing.com',
                        'description': getattr(entry, 'description', ''),
                        'content': await self._extract_full_content(entry.link)
                    }
                    
                    news_list.append(news_item)
                    
                except Exception as e:
                    logger.warning(f"ë‰´ìŠ¤ í•­ëª© ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                    continue
            
            return news_list
            
        except Exception as e:
            logger.error(f"RSS í”¼ë“œ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return []
    
    async def _extract_full_content(self, url: str) -> str:
        """ì „ì²´ ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ"""
        try:
            response = await asyncio.to_thread(
                self.session.get, 
                url, 
                timeout=10
            )
            
            if response.status_code != 200:
                return ""
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ì¸ë² ìŠ¤íŒ…ë‹·ì»´ ê¸°ì‚¬ ë³¸ë¬¸ ì„ íƒì
            content_selectors = [
                '.articlePage p',
                '.WYSIWYG p',
                '.InstrumentPageContentContainer p',
                'article p'
            ]
            
            content_text = ""
            for selector in content_selectors:
                paragraphs = soup.select(selector)
                if paragraphs:
                    content_text = ' '.join([p.get_text().strip() for p in paragraphs])
                    break
            
            return content_text[:2000]  # ë‚´ìš© ê¸¸ì´ ì œí•œ
            
        except Exception as e:
            logger.warning(f"ê¸°ì‚¬ ë‚´ìš© ì¶”ì¶œ ì‹¤íŒ¨ ({url}): {e}")
            return ""
    
    def _parse_publish_time(self, entry) -> datetime:
        """ë°œí–‰ ì‹œê°„ íŒŒì‹±"""
        try:
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                return datetime(*entry.published_parsed[:6])
            elif hasattr(entry, 'published'):
                # ë‹¤ì–‘í•œ ì‹œê°„ í˜•ì‹ ì²˜ë¦¬
                time_str = entry.published
                return self._parse_time_string(time_str)
            else:
                return datetime.now()
        except:
            return datetime.now()
    
    def _parse_time_string(self, time_str: str) -> datetime:
        """ì‹œê°„ ë¬¸ìì—´ íŒŒì‹±"""
        formats = [
            '%a, %d %b %Y %H:%M:%S %z',
            '%a, %d %b %Y %H:%M:%S GMT',
            '%Y-%m-%d %H:%M:%S',
            '%Y-%m-%dT%H:%M:%SZ'
        ]
        
        for fmt in formats:
            try:
                return datetime.strptime(time_str, fmt)
            except:
                continue
        
        return datetime.now()
    
    def _remove_duplicates(self, news_list: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¤‘ë³µ ë‰´ìŠ¤ ì œê±°"""
        seen_titles = set()
        unique_news = []
        
        for news in news_list:
            title_key = news['title'].lower().strip()
            if title_key not in seen_titles:
                seen_titles.add(title_key)
                unique_news.append(news)
        
        return unique_news

class GeminiNewsProcessor:
    """Gemini 1.5 Flash ë‰´ìŠ¤ ì²˜ë¦¬ê¸°"""
    
    def __init__(self):
        # Gemini 1.5 Flash ëª¨ë¸ë¡œ ê³ ì •
        self.model_name = "gemini-1.5-flash"
        self.api_key = os.getenv('GEMINI_API_KEY', '')
        
        if not self.api_key:
            raise ValueError("GEMINI_API_KEY í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
        
        # Gemini ì„¤ì •
        genai.configure(api_key=self.api_key)
        self.model = genai.GenerativeModel(
            model_name=self.model_name,
            generation_config=genai.types.GenerationConfig(
                temperature=0.3,  # ì¼ê´€ëœ ë¶„ì„ì„ ìœ„í•´ ë‚®ì€ ì˜¨ë„
                top_p=0.8,
                top_k=40,
                max_output_tokens=2048,
                candidate_count=1
            )
        )
        
        logger.info(f"ğŸ¤– Gemini {self.model_name} ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    async def process_news_batch(self, news_list: List[Dict[str, Any]]) -> List[NewsData]:
        """ë‰´ìŠ¤ ë°°ì¹˜ ì²˜ë¦¬"""
        logger.info(f"ğŸ”„ {len(news_list)}ê°œ ë‰´ìŠ¤ Gemini 1.5 Flashë¡œ ì²˜ë¦¬ ì‹œì‘")
        
        processed_news = []
        
        for i, news_item in enumerate(news_list, 1):
            try:
                logger.info(f"ğŸ“° ë‰´ìŠ¤ ì²˜ë¦¬ ì¤‘... ({i}/{len(news_list)})")
                
                processed = await self._process_single_news(news_item)
                if processed:
                    processed_news.append(processed)
                
                # API í˜¸ì¶œ ì œí•œ ë°©ì§€
                await asyncio.sleep(0.5)
                
            except Exception as e:
                logger.error(f"ë‰´ìŠ¤ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                continue
        
        logger.info(f"âœ… {len(processed_news)}ê°œ ë‰´ìŠ¤ ì²˜ë¦¬ ì™„ë£Œ")
        return processed_news
    
    async def _process_single_news(self, news_item: Dict[str, Any]) -> Optional[NewsData]:
        """ë‹¨ì¼ ë‰´ìŠ¤ ì²˜ë¦¬"""
        try:
            # Geminiì—ê²Œ ìµœì í™”ëœ í”„ë¡¬í”„íŠ¸ ìƒì„±
            prompt = self._create_analysis_prompt(news_item)
            
            # Gemini 1.5 Flash ë¶„ì„ ì‹¤í–‰
            response = await asyncio.to_thread(
                self.model.generate_content,
                prompt
            )
            
            if not response or not response.text:
                return None
            
            # ì‘ë‹µ íŒŒì‹±
            analysis_result = self._parse_gemini_response(response.text)
            
            # NewsData ê°ì²´ ìƒì„±
            return self._create_news_data(news_item, analysis_result)
            
        except Exception as e:
            logger.error(f"ë‹¨ì¼ ë‰´ìŠ¤ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            return None
    
    def _create_analysis_prompt(self, news_item: Dict[str, Any]) -> str:
        """Gemini 1.5 Flash ìµœì í™” ë¶„ì„ í”„ë¡¬í”„íŠ¸"""
        title = news_item.get('title', '')
        content = news_item.get('content', news_item.get('description', ''))
        
        return f"""
ğŸ¤– **Gemini 1.5 Flash ì „ë¬¸ ë‰´ìŠ¤ ë¶„ì„**

ë‹¤ìŒ ê¸ˆìœµ ë‰´ìŠ¤ë¥¼ ì „ë¬¸ ì• ë„ë¦¬ìŠ¤íŠ¸ ê´€ì ì—ì„œ ë¶„ì„í•´ì£¼ì„¸ìš”:

**ì œëª©:** {title}
**ë‚´ìš©:** {content}

**ë¶„ì„ ìš”êµ¬ì‚¬í•­:**
1. ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜ (ì‹œì¥/ê¸°ì—…/ê²½ì œ/ê¸°ìˆ /ì •ì±…/í•´ì™¸/ê¸°íƒ€)
2. ê°ì • ë¶„ì„ (ë§¤ìš°ê¸ì •/ê¸ì •/ì¤‘ë¦½/ë¶€ì •/ë§¤ìš°ë¶€ì •)
3. ì‹œì¥ ì˜í–¥ë„ ì ìˆ˜ (0-100ì )
4. ê´€ë ¨ ì£¼ì‹ ì¢…ëª© ì¶”ì¶œ (í‹°ì»¤ ì‹¬ë³¼)
5. í•µì‹¬ í‚¤ì›Œë“œ 5ê°œ
6. í•œêµ­ì–´ ìš”ì•½ (100ì ì´ë‚´)
7. ì œëª© í•œêµ­ì–´ ë²ˆì—­
8. íˆ¬ì ì‹œì‚¬ì 

**ì‘ë‹µ í˜•ì‹ (JSON):**
```json
{{
    "category": "ì¹´í…Œê³ ë¦¬",
    "sentiment": "ê°ì •ë¶„ì„ê²°ê³¼",
    "impact_score": 85,
    "related_stocks": ["AAPL", "MSFT"],
    "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3", "í‚¤ì›Œë“œ4", "í‚¤ì›Œë“œ5"],
    "korean_summary": "í•œêµ­ì–´ ìš”ì•½",
    "korean_title": "í•œêµ­ì–´ ì œëª©",
    "investment_insight": "íˆ¬ì ì‹œì‚¬ì ",
    "confidence": 0.9
}}
```

ë°˜ë“œì‹œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”.
"""
    
    def _parse_gemini_response(self, response_text: str) -> Dict[str, Any]:
        """Gemini ì‘ë‹µ íŒŒì‹±"""
        try:
            # JSON ì¶”ì¶œ
            import re
            
            # JSON íŒ¨í„´ ì°¾ê¸°
            json_patterns = [
                r'```json\s*(\{.*?\})\s*```',
                r'```\s*(\{.*?\})\s*```',
                r'(\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\})'
            ]
            
            for pattern in json_patterns:
                matches = re.findall(pattern, response_text, re.DOTALL | re.IGNORECASE)
                if matches:
                    try:
                        result = json.loads(matches[0])
                        return self._validate_analysis_result(result)
                    except json.JSONDecodeError:
                        continue
            
            # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜
            return self._create_fallback_analysis()
            
        except Exception as e:
            logger.warning(f"Gemini ì‘ë‹µ íŒŒì‹± ì‹¤íŒ¨: {e}")
            return self._create_fallback_analysis()
    
    def _validate_analysis_result(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """ë¶„ì„ ê²°ê³¼ ê²€ì¦"""
        # ê¸°ë³¸ê°’ ì„¤ì •
        defaults = {
            "category": "ê¸°íƒ€",
            "sentiment": "ì¤‘ë¦½",
            "impact_score": 50,
            "related_stocks": [],
            "keywords": [],
            "korean_summary": "ìš”ì•½ ìƒì„± ì¤‘...",
            "korean_title": "ì œëª© ë²ˆì—­ ì¤‘...",
            "investment_insight": "ë¶„ì„ ì¤‘...",
            "confidence": 0.7
        }
        
        # ëˆ„ë½ëœ í•„ë“œ ë³´ì™„
        for key, default_value in defaults.items():
            if key not in result:
                result[key] = default_value
        
        # ë°ì´í„° íƒ€ì… ê²€ì¦
        try:
            result['impact_score'] = max(0, min(100, int(float(result.get('impact_score', 50)))))
            result['confidence'] = max(0.0, min(1.0, float(result.get('confidence', 0.7))))
        except (ValueError, TypeError):
            result['impact_score'] = 50
            result['confidence'] = 0.7
        
        return result
    
    def _create_fallback_analysis(self) -> Dict[str, Any]:
        """í´ë°± ë¶„ì„ ê²°ê³¼"""
        return {
            "category": "ê¸°íƒ€",
            "sentiment": "ì¤‘ë¦½",
            "impact_score": 50,
            "related_stocks": [],
            "keywords": ["ë‰´ìŠ¤", "ë¶„ì„"],
            "korean_summary": "AI ë¶„ì„ ì²˜ë¦¬ ì¤‘...",
            "korean_title": "ì œëª© ì²˜ë¦¬ ì¤‘...",
            "investment_insight": "ì¶”ê°€ ë¶„ì„ í•„ìš”",
            "confidence": 0.5
        }
    
    def _create_news_data(self, news_item: Dict[str, Any], analysis: Dict[str, Any]) -> NewsData:
        """NewsData ê°ì²´ ìƒì„±"""
        # ì¹´í…Œê³ ë¦¬ ë§¤í•‘
        category_map = {
            "ì‹œì¥": NewsCategory.MARKET,
            "ê¸°ì—…": NewsCategory.COMPANY,
            "ê²½ì œ": NewsCategory.ECONOMIC,
            "ê¸°ìˆ ": NewsCategory.TECHNOLOGY,
            "ì •ì±…": NewsCategory.POLICY,
            "í•´ì™¸": NewsCategory.GLOBAL,
            "ê¸°íƒ€": NewsCategory.OTHER
        }
        
        # ê°ì • ë§¤í•‘
        sentiment_map = {
            "ë§¤ìš°ê¸ì •": SentimentType.VERY_POSITIVE,
            "ê¸ì •": SentimentType.POSITIVE,
            "ì¤‘ë¦½": SentimentType.NEUTRAL,
            "ë¶€ì •": SentimentType.NEGATIVE,
            "ë§¤ìš°ë¶€ì •": SentimentType.VERY_NEGATIVE
        }
        
        return NewsData(
            title=news_item.get('title', ''),
            content=news_item.get('content', news_item.get('description', '')),
            url=news_item.get('url', ''),
            published_time=news_item.get('published_time', datetime.now()),
            source=news_item.get('source', 'Investing.com'),
            category=category_map.get(analysis.get('category', 'ê¸°íƒ€'), NewsCategory.OTHER),
            sentiment=sentiment_map.get(analysis.get('sentiment', 'ì¤‘ë¦½'), SentimentType.NEUTRAL),
            impact_score=analysis.get('impact_score', 50),
            related_stocks=analysis.get('related_stocks', []),
            keywords=analysis.get('keywords', []),
            summary=analysis.get('korean_summary', ''),
            translated_title=analysis.get('korean_title', ''),
            translated_content=analysis.get('investment_insight', '')
        )

class NewsAnalysisSystem:
    """í†µí•© ë‰´ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        self.collector = InvestingNewsCollector()
        self.processor = GeminiNewsProcessor()
        self.cache = {}
        
    async def analyze_latest_news(self, hours_back: int = 6, max_articles: int = 20) -> List[NewsData]:
        """ìµœì‹  ë‰´ìŠ¤ ë¶„ì„"""
        logger.info(f"ğŸ“° ìµœê·¼ {hours_back}ì‹œê°„ ë‰´ìŠ¤ ë¶„ì„ ì‹œì‘")
        
        try:
            # 1. ë‰´ìŠ¤ ìˆ˜ì§‘
            raw_news = await self.collector.collect_latest_news(hours_back, max_articles)
            
            if not raw_news:
                logger.warning("ìˆ˜ì§‘ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            # 2. Gemini 1.5 Flashë¡œ ë¶„ì„
            processed_news = await self.processor.process_news_batch(raw_news)
            
            # 3. ì¤‘ìš”ë„ìˆœ ì •ë ¬
            sorted_news = sorted(
                processed_news, 
                key=lambda x: x.impact_score, 
                reverse=True
            )
            
            logger.info(f"âœ… ì´ {len(sorted_news)}ê°œ ë‰´ìŠ¤ ë¶„ì„ ì™„ë£Œ")
            return sorted_news
            
        except Exception as e:
            logger.error(f"ë‰´ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ ì˜¤ë¥˜: {e}")
            return []
    
    def get_news_by_category(self, news_list: List[NewsData], category: NewsCategory) -> List[NewsData]:
        """ì¹´í…Œê³ ë¦¬ë³„ ë‰´ìŠ¤ í•„í„°ë§"""
        return [news for news in news_list if news.category == category]
    
    def get_news_by_sentiment(self, news_list: List[NewsData], sentiment: SentimentType) -> List[NewsData]:
        """ê°ì •ë³„ ë‰´ìŠ¤ í•„í„°ë§"""
        return [news for news in news_list if news.sentiment == sentiment]
    
    def get_high_impact_news(self, news_list: List[NewsData], min_score: float = 70) -> List[NewsData]:
        """ê³ ì˜í–¥ë„ ë‰´ìŠ¤ í•„í„°ë§"""
        return [news for news in news_list if news.impact_score >= min_score]
    
    def get_stock_related_news(self, news_list: List[NewsData], stock_symbol: str) -> List[NewsData]:
        """íŠ¹ì • ì£¼ì‹ ê´€ë ¨ ë‰´ìŠ¤"""
        return [
            news for news in news_list 
            if stock_symbol.upper() in [s.upper() for s in news.related_stocks]
        ]
    
    def create_news_summary(self, news_list: List[NewsData]) -> Dict[str, Any]:
        """ë‰´ìŠ¤ ìš”ì•½ ë¦¬í¬íŠ¸"""
        if not news_list:
            return {"error": "ë¶„ì„í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤"}
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„
        category_stats = {}
        for category in NewsCategory:
            count = len(self.get_news_by_category(news_list, category))
            if count > 0:
                category_stats[category.value] = count
        
        # ê°ì • ë¶„ì„ í†µê³„
        sentiment_stats = {}
        for sentiment in SentimentType:
            count = len(self.get_news_by_sentiment(news_list, sentiment))
            if count > 0:
                sentiment_stats[sentiment.value] = count
        
        # í‰ê·  ì˜í–¥ë„
        avg_impact = sum(news.impact_score for news in news_list) / len(news_list)
        
        # ìƒìœ„ ë‰´ìŠ¤
        top_news = sorted(news_list, key=lambda x: x.impact_score, reverse=True)[:5]
        
        return {
            "ì´_ë‰´ìŠ¤_ìˆ˜": len(news_list),
            "í‰ê· _ì˜í–¥ë„": round(avg_impact, 1),
            "ì¹´í…Œê³ ë¦¬ë³„_í†µê³„": category_stats,
            "ê°ì •_ë¶„ì„_í†µê³„": sentiment_stats,
            "ìƒìœ„_5ê°œ_ë‰´ìŠ¤": [
                {
                    "ì œëª©": news.translated_title or news.title,
                    "ì˜í–¥ë„": news.impact_score,
                    "ê°ì •": news.sentiment.value,
                    "ì¹´í…Œê³ ë¦¬": news.category.value,
                    "ê´€ë ¨ì£¼ì‹": news.related_stocks
                }
                for news in top_news
            ],
            "ë¶„ì„_ì‹œê°„": datetime.now().isoformat()
        }

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
async def test_news_system():
    """ë‰´ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
    print("ğŸ“° Gemini 1.5 Flash ë‰´ìŠ¤ ë¶„ì„ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    try:
        system = NewsAnalysisSystem()
        
        # ìµœì‹  ë‰´ìŠ¤ ë¶„ì„
        print("ğŸ”„ ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘ ë° ë¶„ì„ ì¤‘...")
        news_list = await system.analyze_latest_news(hours_back=12, max_articles=10)
        
        if not news_list:
            print("âŒ ë¶„ì„í•  ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤")
            return
        
        print(f"âœ… {len(news_list)}ê°œ ë‰´ìŠ¤ ë¶„ì„ ì™„ë£Œ\n")
        
        # ë‰´ìŠ¤ ìš”ì•½ ë¦¬í¬íŠ¸
        summary = system.create_news_summary(news_list)
        print("ğŸ“Š ë‰´ìŠ¤ ë¶„ì„ ìš”ì•½:")
        print(json.dumps(summary, indent=2, ensure_ascii=False))
        
        print("\n" + "=" * 60)
        print("ğŸ† ìƒìœ„ 3ê°œ ë‰´ìŠ¤ ìƒì„¸:")
        
        for i, news in enumerate(news_list[:3], 1):
            print(f"\n{i}. {news.translated_title or news.title}")
            print(f"   ğŸ“ˆ ì˜í–¥ë„: {news.impact_score}ì ")
            print(f"   ğŸ˜Š ê°ì •: {news.sentiment.value}")
            print(f"   ğŸ“‚ ì¹´í…Œê³ ë¦¬: {news.category.value}")
            print(f"   ğŸ¢ ê´€ë ¨ì£¼ì‹: {', '.join(news.related_stocks) if news.related_stocks else 'ì—†ìŒ'}")
            print(f"   ğŸ“ ìš”ì•½: {news.summary}")
            print(f"   ğŸ”— URL: {news.url}")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()

async def main():
    """ë©”ì¸ ì‹¤í–‰"""
    await test_news_system()

if __name__ == "__main__":
    asyncio.run(main()) 