#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸŒ GitHub ì›¹ ê²€ìƒ‰ ëª¨ë“ˆ
API í† í° ì—†ì´ë„ ì›¹ ê²€ìƒ‰ì„ í†µí•´ GitHub ì •ë³´ë¥¼ ìˆ˜ì§‘

ì§€ì› ê¸°ëŠ¥:
1. ì›¹ ê²€ìƒ‰ì„ í†µí•œ GitHub ì €ì¥ì†Œ ì°¾ê¸°
2. ê³µê°œ ì €ì¥ì†Œ ì •ë³´ ìŠ¤í¬ë˜í•‘
3. ì½”ë“œ ê²€ìƒ‰ ë° ë¶„ì„
"""

import requests
import json
import re
import logging
from typing import Dict, List, Optional, Any
from urllib.parse import quote
from bs4 import BeautifulSoup
from datetime import datetime

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class GitHubWebSearcher:
    """GitHub ì›¹ ê²€ìƒ‰ í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
    
    def search_repositories_web(self, query: str, language: str = None, 
                               per_page: int = 10) -> List[Dict[str, Any]]:
        """ì›¹ ê²€ìƒ‰ì„ í†µí•œ GitHub ì €ì¥ì†Œ ê²€ìƒ‰"""
        logger.info(f"ğŸŒ ì›¹ ê²€ìƒ‰ìœ¼ë¡œ GitHub ì €ì¥ì†Œ ê²€ìƒ‰: {query}")
        
        # Google ê²€ìƒ‰ì„ í†µí•œ GitHub ì €ì¥ì†Œ ì°¾ê¸°
        search_query = f"site:github.com {query}"
        if language:
            search_query += f" language:{language}"
        
        try:
            # Google ê²€ìƒ‰ ê²°ê³¼
            google_results = self._search_google(search_query, per_page)
            
            # GitHub ì €ì¥ì†Œ ì •ë³´ ì¶”ì¶œ
            repositories = []
            for result in google_results:
                if 'github.com' in result.get('url', ''):
                    repo_info = self._extract_repo_info_from_url(result['url'])
                    if repo_info:
                        repo_info.update({
                            'title': result.get('title', ''),
                            'description': result.get('description', ''),
                            'search_source': 'google'
                        })
                        repositories.append(repo_info)
            
            logger.info(f"âœ… ì›¹ ê²€ìƒ‰ ì™„ë£Œ: {len(repositories)}ê°œ ì €ì¥ì†Œ ë°œê²¬")
            return repositories
            
        except Exception as e:
            logger.warning(f"ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return self._get_popular_repositories(query)
    
    def _search_google(self, query: str, num_results: int = 10) -> List[Dict[str, Any]]:
        """Google ê²€ìƒ‰"""
        # DuckDuckGoë¥¼ ì‚¬ìš©í•œ ê²€ìƒ‰ (ë” ì•ˆì •ì )
        return self._search_duckduckgo(query, num_results)
    
    def _search_duckduckgo(self, query: str, num_results: int = 10) -> List[Dict[str, Any]]:
        """DuckDuckGo ê²€ìƒ‰"""
        try:
            url = "https://html.duckduckgo.com/html/"
            params = {'q': query}
            
            response = self.session.get(url, params=params)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            results = []
            
            # ê²€ìƒ‰ ê²°ê³¼ íŒŒì‹±
            for result in soup.find_all('div', class_='result')[:num_results]:
                title_elem = result.find('a', class_='result__a')
                snippet_elem = result.find('a', class_='result__snippet')
                
                if title_elem:
                    results.append({
                        'title': title_elem.get_text(strip=True),
                        'url': title_elem.get('href', ''),
                        'description': snippet_elem.get_text(strip=True) if snippet_elem else ''
                    })
            
            return results
            
        except Exception as e:
            logger.warning(f"DuckDuckGo ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _extract_repo_info_from_url(self, url: str) -> Optional[Dict[str, Any]]:
        """GitHub URLì—ì„œ ì €ì¥ì†Œ ì •ë³´ ì¶”ì¶œ"""
        # GitHub ì €ì¥ì†Œ URL íŒ¨í„´ ë§¤ì¹­
        pattern = r'https://github\.com/([^/]+)/([^/]+)'
        match = re.match(pattern, url)
        
        if not match:
            return None
        
        owner, repo = match.groups()
        
        # ê³µê°œ ì €ì¥ì†Œ ì •ë³´ ìˆ˜ì§‘
        try:
            repo_info = self._scrape_repo_info(owner, repo)
            return repo_info
        except Exception as e:
            logger.warning(f"ì €ì¥ì†Œ ì •ë³´ ìˆ˜ì§‘ ì‹¤íŒ¨ ({owner}/{repo}): {e}")
            return {
                'name': repo,
                'full_name': f'{owner}/{repo}',
                'owner': {'login': owner},
                'html_url': url,
                'description': '',
                'stars': 0,
                'forks': 0,
                'language': '',
                'web_scraped': True
            }
    
    def _scrape_repo_info(self, owner: str, repo: str) -> Dict[str, Any]:
        """GitHub ì €ì¥ì†Œ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘"""
        url = f"https://github.com/{owner}/{repo}"
        
        response = self.session.get(url)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ê¸°ë³¸ ì •ë³´
        repo_info = {
            'name': repo,
            'full_name': f'{owner}/{repo}',
            'html_url': url,
            'owner': {'login': owner},
            'web_scraped': True
        }
        
        # ì„¤ëª… ì¶”ì¶œ
        description_elem = soup.find('p', class_='f4 my-3')
        if description_elem:
            repo_info['description'] = description_elem.get_text(strip=True)
        else:
            repo_info['description'] = ''
        
        # ìŠ¤íƒ€, í¬í¬ ìˆ˜ ì¶”ì¶œ
        stats = soup.find_all('a', class_='Link--muted')
        for stat in stats:
            text = stat.get_text(strip=True)
            if 'star' in stat.get('href', '').lower():
                repo_info['stars'] = self._parse_number(text)
            elif 'fork' in stat.get('href', '').lower():
                repo_info['forks'] = self._parse_number(text)
        
        # ì–¸ì–´ ì •ë³´ ì¶”ì¶œ
        lang_elem = soup.find('span', class_='color-fg-default text-bold mr-1')
        if lang_elem:
            repo_info['language'] = lang_elem.get_text(strip=True)
        else:
            repo_info['language'] = ''
        
        # í† í”½ ì¶”ì¶œ
        topics = []
        topic_elems = soup.find_all('a', class_='topic-tag')
        for topic in topic_elems:
            topics.append(topic.get_text(strip=True))
        repo_info['topics'] = topics
        
        return repo_info
    
    def _parse_number(self, text: str) -> int:
        """ìˆ«ì ë¬¸ìì—´ íŒŒì‹± (1.2k -> 1200)"""
        text = text.replace(',', '').strip()
        
        if 'k' in text.lower():
            return int(float(text.lower().replace('k', '')) * 1000)
        elif 'm' in text.lower():
            return int(float(text.lower().replace('m', '')) * 1000000)
        else:
            try:
                return int(text)
            except:
                return 0
    
    def _get_popular_repositories(self, query: str) -> List[Dict[str, Any]]:
        """ì¸ê¸° ìˆëŠ” ì €ì¥ì†Œ ëª©ë¡ (í´ë°±)"""
        logger.info(f"ğŸ”„ ì¸ê¸° ì €ì¥ì†Œ ëª©ë¡ ìƒì„±: {query}")
        
        # ì£¼ì‹/ê¸ˆìœµ ê´€ë ¨ ì¸ê¸° ì €ì¥ì†Œë“¤
        popular_repos = [
            {
                'name': 'yfinance',
                'full_name': 'ranaroussi/yfinance',
                'description': 'Download market data from Yahoo! Finance\'s API',
                'html_url': 'https://github.com/ranaroussi/yfinance',
                'stars': 8000,
                'forks': 1500,
                'language': 'Python',
                'owner': {'login': 'ranaroussi'},
                'topics': ['finance', 'yahoo-finance', 'stock-data'],
                'fallback': True
            },
            {
                'name': 'zipline',
                'full_name': 'quantopian/zipline',
                'description': 'Zipline, a Pythonic Algorithmic Trading Library',
                'html_url': 'https://github.com/quantopian/zipline',
                'stars': 15000,
                'forks': 4000,
                'language': 'Python',
                'owner': {'login': 'quantopian'},
                'topics': ['algorithmic-trading', 'finance', 'backtesting'],
                'fallback': True
            },
            {
                'name': 'pandas-datareader',
                'full_name': 'pydata/pandas-datareader',
                'description': 'Up to date remote data access for pandas',
                'html_url': 'https://github.com/pydata/pandas-datareader',
                'stars': 2500,
                'forks': 600,
                'language': 'Python',
                'owner': {'login': 'pydata'},
                'topics': ['pandas', 'data-reader', 'finance'],
                'fallback': True
            },
            {
                'name': 'backtrader',
                'full_name': 'mementum/backtrader',
                'description': 'Python Backtesting library for trading strategies',
                'html_url': 'https://github.com/mementum/backtrader',
                'stars': 9000,
                'forks': 2500,
                'language': 'Python',
                'owner': {'login': 'mementum'},
                'topics': ['backtesting', 'trading', 'finance'],
                'fallback': True
            },
            {
                'name': 'FinanceDatabase',
                'full_name': 'JerBouma/FinanceDatabase',
                'description': 'This is a database of 300.000+ symbols containing Equities, ETFs, Funds, Indices, Currencies, Cryptocurrencies and Money Markets.',
                'html_url': 'https://github.com/JerBouma/FinanceDatabase',
                'stars': 2000,
                'forks': 400,
                'language': 'Python',
                'owner': {'login': 'JerBouma'},
                'topics': ['finance', 'database', 'stocks'],
                'fallback': True
            }
        ]
        
        # ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ì €ì¥ì†Œ í•„í„°ë§
        query_lower = query.lower()
        relevant_repos = []
        
        for repo in popular_repos:
            # ì´ë¦„, ì„¤ëª…, í† í”½ì—ì„œ ì¿¼ë¦¬ ê´€ë ¨ì„± í™•ì¸
            relevance_score = 0
            
            if any(word in repo['name'].lower() for word in query_lower.split()):
                relevance_score += 3
            if any(word in repo['description'].lower() for word in query_lower.split()):
                relevance_score += 2
            if any(word in ' '.join(repo['topics']).lower() for word in query_lower.split()):
                relevance_score += 1
            
            if relevance_score > 0:
                repo['relevance_score'] = relevance_score
                relevant_repos.append(repo)
        
        # ê´€ë ¨ì„± ì ìˆ˜ë¡œ ì •ë ¬
        relevant_repos.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
        
        return relevant_repos[:5]  # ìƒìœ„ 5ê°œë§Œ ë°˜í™˜
    
    def search_code_web(self, query: str, language: str = None) -> List[Dict[str, Any]]:
        """ì›¹ ê²€ìƒ‰ì„ í†µí•œ ì½”ë“œ ê²€ìƒ‰"""
        logger.info(f"ğŸ” ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ì½”ë“œ ê²€ìƒ‰: {query}")
        
        search_query = f"site:github.com {query}"
        if language:
            search_query += f" filetype:{language}"
        
        try:
            results = self._search_duckduckgo(search_query, 10)
            code_results = []
            
            for result in results:
                if 'github.com' in result['url'] and '/blob/' in result['url']:
                    code_info = self._extract_code_info_from_url(result['url'])
                    if code_info:
                        code_info.update({
                            'title': result['title'],
                            'description': result['description'],
                            'search_source': 'web'
                        })
                        code_results.append(code_info)
            
            logger.info(f"âœ… ì½”ë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(code_results)}ê°œ ê²°ê³¼")
            return code_results
            
        except Exception as e:
            logger.warning(f"ì½”ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def _extract_code_info_from_url(self, url: str) -> Optional[Dict[str, Any]]:
        """GitHub ì½”ë“œ URLì—ì„œ ì •ë³´ ì¶”ì¶œ"""
        # GitHub blob URL íŒ¨í„´ ë§¤ì¹­
        pattern = r'https://github\.com/([^/]+)/([^/]+)/blob/([^/]+)/(.+)'
        match = re.match(pattern, url)
        
        if not match:
            return None
        
        owner, repo, branch, file_path = match.groups()
        
        return {
            'name': file_path.split('/')[-1],
            'path': file_path,
            'html_url': url,
            'repository': {
                'name': repo,
                'full_name': f'{owner}/{repo}',
                'html_url': f'https://github.com/{owner}/{repo}'
            },
            'branch': branch,
            'web_scraped': True
        }
    
    def get_trending_repositories(self, language: str = None, 
                                 period: str = 'daily') -> List[Dict[str, Any]]:
        """íŠ¸ë Œë”© ì €ì¥ì†Œ ì¡°íšŒ"""
        logger.info(f"ğŸ“ˆ íŠ¸ë Œë”© ì €ì¥ì†Œ ì¡°íšŒ: {language} ({period})")
        
        try:
            url = "https://github.com/trending"
            if language:
                url += f"/{language}"
            
            params = {'since': period}
            response = self.session.get(url, params=params)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, 'html.parser')
            repositories = []
            
            # íŠ¸ë Œë”© ì €ì¥ì†Œ íŒŒì‹±
            for article in soup.find_all('article', class_='Box-row')[:10]:
                repo_link = article.find('h2').find('a')
                if repo_link:
                    repo_name = repo_link.get('href').strip('/')
                    
                    # ì„¤ëª…
                    desc_elem = article.find('p', class_='col-9')
                    description = desc_elem.get_text(strip=True) if desc_elem else ''
                    
                    # ì–¸ì–´
                    lang_elem = article.find('span', {'itemprop': 'programmingLanguage'})
                    language = lang_elem.get_text(strip=True) if lang_elem else ''
                    
                    # ìŠ¤íƒ€ ìˆ˜
                    star_elem = article.find('a', href=f'/{repo_name}/stargazers')
                    stars = 0
                    if star_elem:
                        star_text = star_elem.get_text(strip=True)
                        stars = self._parse_number(star_text)
                    
                    repositories.append({
                        'name': repo_name.split('/')[-1],
                        'full_name': repo_name,
                        'description': description,
                        'html_url': f'https://github.com/{repo_name}',
                        'language': language,
                        'stars': stars,
                        'trending': True,
                        'period': period
                    })
            
            logger.info(f"âœ… íŠ¸ë Œë”© ì €ì¥ì†Œ ì¡°íšŒ ì™„ë£Œ: {len(repositories)}ê°œ")
            return repositories
            
        except Exception as e:
            logger.warning(f"íŠ¸ë Œë”© ì €ì¥ì†Œ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def test_web_search(self) -> Dict[str, Any]:
        """ì›¹ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
        logger.info("ğŸŒ ì›¹ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        results = {
            'web_search_working': False,
            'repository_search_working': False,
            'code_search_working': False,
            'trending_working': False,
            'test_results': {}
        }
        
        # ì €ì¥ì†Œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        try:
            repos = self.search_repositories_web("python", per_page=2)
            if repos:
                results['repository_search_working'] = True
                results['test_results']['repositories'] = len(repos)
        except Exception as e:
            logger.warning(f"ì €ì¥ì†Œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # ì½”ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        try:
            codes = self.search_code_web("def main", "python")
            if codes:
                results['code_search_working'] = True
                results['test_results']['code_results'] = len(codes)
        except Exception as e:
            logger.warning(f"ì½”ë“œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # íŠ¸ë Œë”© ì €ì¥ì†Œ í…ŒìŠ¤íŠ¸
        try:
            trending = self.get_trending_repositories("python")
            if trending:
                results['trending_working'] = True
                results['test_results']['trending'] = len(trending)
        except Exception as e:
            logger.warning(f"íŠ¸ë Œë”© ì €ì¥ì†Œ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        
        # ì „ì²´ ì›¹ ê²€ìƒ‰ ìƒíƒœ
        results['web_search_working'] = any([
            results['repository_search_working'],
            results['code_search_working'],
            results['trending_working']
        ])
        
        logger.info(f"âœ… ì›¹ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: {results}")
        return results

# ì „ì—­ ì¸ìŠ¤í„´ìŠ¤
github_web_searcher = GitHubWebSearcher()

def search_github_repos_web(query: str, **kwargs) -> List[Dict[str, Any]]:
    """GitHub ì €ì¥ì†Œ ì›¹ ê²€ìƒ‰ (í¸ì˜ í•¨ìˆ˜)"""
    return github_web_searcher.search_repositories_web(query, **kwargs)

def search_github_code_web(query: str, **kwargs) -> List[Dict[str, Any]]:
    """GitHub ì½”ë“œ ì›¹ ê²€ìƒ‰ (í¸ì˜ í•¨ìˆ˜)"""
    return github_web_searcher.search_code_web(query, **kwargs)

def get_github_trending(language: str = None, period: str = 'daily') -> List[Dict[str, Any]]:
    """GitHub íŠ¸ë Œë”© ì €ì¥ì†Œ (í¸ì˜ í•¨ìˆ˜)"""
    return github_web_searcher.get_trending_repositories(language, period)

def test_github_web_search() -> Dict[str, Any]:
    """GitHub ì›¹ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (í¸ì˜ í•¨ìˆ˜)"""
    return github_web_searcher.test_web_search()

# í…ŒìŠ¤íŠ¸ ì‹¤í–‰
if __name__ == "__main__":
    print("ğŸŒ GitHub ì›¹ ê²€ìƒ‰ ëª¨ë“ˆ í…ŒìŠ¤íŠ¸")
    
    # ì›¹ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    web_test = test_github_web_search()
    print(f"ì›¹ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ê²°ê³¼: {json.dumps(web_test, indent=2, ensure_ascii=False)}")
    
    # ì €ì¥ì†Œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    print("\nğŸ“‹ ì €ì¥ì†Œ ì›¹ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
    repos = search_github_repos_web("python stock analysis", per_page=3)
    for repo in repos:
        print(f"  â€¢ {repo['full_name']}: â­{repo.get('stars', 0)} - {repo.get('description', '')[:100]}")
    
    # íŠ¸ë Œë”© ì €ì¥ì†Œ í…ŒìŠ¤íŠ¸
    print("\nğŸ“ˆ íŠ¸ë Œë”© ì €ì¥ì†Œ í…ŒìŠ¤íŠ¸")
    trending = get_github_trending("python", "daily")
    for repo in trending[:3]:
        print(f"  â€¢ {repo['full_name']}: â­{repo.get('stars', 0)} - {repo.get('description', '')[:100]}")
    
    print("\nâœ… ëª¨ë“  ì›¹ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì™„ë£Œ!") 