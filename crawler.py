"""
üá∫üá∏ ÎØ∏Íµ≠Ï£ºÏãù Ïû¨Î¨¥Ï†úÌëú Î∞è Îâ¥Ïä§ ÌÅ¨Î°§ÎßÅ ÏãúÏä§ÌÖú
Î¨¥Î£å Îç∞Ïù¥ÌÑ∞ ÏÜåÏä§Î•º ÌôúÏö©Ìïú Ï¢ÖÌï© Ï†ïÎ≥¥ ÏàòÏßë

Ï£ºÏöî Í∏∞Îä•:
1. Yahoo Finance Ïû¨Î¨¥Ï†úÌëú ÌÅ¨Î°§ÎßÅ
2. SEC EDGAR Í≥µÏãú Îç∞Ïù¥ÌÑ∞ ÏàòÏßë
3. Îâ¥Ïä§ ÌÅ¨Î°§ÎßÅ (Google News, Yahoo Finance)
4. Í≤ΩÏ†úÏßÄÌëú ÏàòÏßë (FRED API)
5. ÏÜåÏÖú ÏÑºÌã∞Î®ºÌä∏ Î∂ÑÏÑù (Reddit, Twitter)
"""
import requests
import yfinance as yf
import pandas as pd
import numpy as np
from bs4 import BeautifulSoup
import time
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import logging
import asyncio
import aiohttp
from urllib.parse import quote
import re

# Î°úÍπÖ ÏÑ§Ï†ï
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class USFinancialCrawler:
    """üá∫üá∏ ÎØ∏Íµ≠Ï£ºÏãù Ïû¨Î¨¥Ï†úÌëú Î∞è Îâ¥Ïä§ ÌÅ¨Î°§ÎßÅ ÏãúÏä§ÌÖú"""
    
    def __init__(self):
        """Ï¥àÍ∏∞Ìôî"""
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # Ï∫êÏãú Ï†ÄÏû•ÏÜå
        self.financial_cache = {}
        self.news_cache = {}
        
        # API ÌÇ§Îì§ (Î¨¥Î£å Î≤ÑÏ†Ñ)
        self.fred_api_key = None  # FRED API ÌÇ§ (Î¨¥Î£å)
        self.alpha_vantage_key = None  # Alpha Vantage API ÌÇ§ (Î¨¥Î£å)
    
    # === üìä Ïû¨Î¨¥Ï†úÌëú Îç∞Ïù¥ÌÑ∞ ÏàòÏßë ===
    def get_financial_statements(self, symbol: str) -> Dict:
        """Yahoo FinanceÏóêÏÑú Ïû¨Î¨¥Ï†úÌëú Îç∞Ïù¥ÌÑ∞ ÏàòÏßë"""
        try:
            logger.info(f"üìä {symbol} Ïû¨Î¨¥Ï†úÌëú Îç∞Ïù¥ÌÑ∞ ÏàòÏßë Ï§ë...")
            
            # yfinance ÏÇ¨Ïö© (Í∞ÄÏû• ÏïàÏ†ïÏ†Å)
            ticker = yf.Ticker(symbol)
            
            # Í∏∞Î≥∏ Ï†ïÎ≥¥
            info = ticker.info
            
            # Ïû¨Î¨¥Ï†úÌëú Îç∞Ïù¥ÌÑ∞
            financials = ticker.financials
            balance_sheet = ticker.balance_sheet
            cash_flow = ticker.cashflow
            
            # Ï£ºÏöî Ïû¨Î¨¥ ÏßÄÌëú Ï∂îÏ∂ú
            financial_data = {
                'symbol': symbol,
                'company_name': info.get('longName', symbol),
                'sector': info.get('sector', 'Unknown'),
                'industry': info.get('industry', 'Unknown'),
                'market_cap': info.get('marketCap', 0),
                'enterprise_value': info.get('enterpriseValue', 0),
                
                # ÏàòÏùµÏÑ± ÏßÄÌëú
                'revenue': self._get_latest_value(financials, 'Total Revenue'),
                'gross_profit': self._get_latest_value(financials, 'Gross Profit'),
                'operating_income': self._get_latest_value(financials, 'Operating Income'),
                'net_income': self._get_latest_value(financials, 'Net Income'),
                'ebitda': info.get('ebitda', 0),
                
                # ÎßàÏßÑ ÏßÄÌëú
                'gross_margin': info.get('grossMargins', 0),
                'operating_margin': info.get('operatingMargins', 0),
                'profit_margin': info.get('profitMargins', 0),
                
                # Ìö®Ïú®ÏÑ± ÏßÄÌëú
                'roe': info.get('returnOnEquity', 0),
                'roa': info.get('returnOnAssets', 0),
                'roic': info.get('returnOnInvestmentCapital', 0),
                
                # Ïû¨Î¨¥ Í±¥Ï†ÑÏÑ±
                'total_debt': self._get_latest_value(balance_sheet, 'Total Debt'),
                'total_cash': self._get_latest_value(balance_sheet, 'Cash And Cash Equivalents'),
                'total_assets': self._get_latest_value(balance_sheet, 'Total Assets'),
                'shareholders_equity': self._get_latest_value(balance_sheet, 'Stockholders Equity'),
                
                # Î∂ÄÏ±Ñ ÎπÑÏú®
                'debt_to_equity': info.get('debtToEquity', 0),
                'current_ratio': info.get('currentRatio', 0),
                'quick_ratio': info.get('quickRatio', 0),
                
                # ÏÑ±Ïû•Î•†
                'revenue_growth': info.get('revenueGrowth', 0),
                'earnings_growth': info.get('earningsGrowth', 0),
                
                # Î∞∏Î•òÏóêÏù¥ÏÖò
                'pe_ratio': info.get('trailingPE', 0),
                'forward_pe': info.get('forwardPE', 0),
                'pb_ratio': info.get('priceToBook', 0),
                'ps_ratio': info.get('priceToSalesTrailing12Months', 0),
                'peg_ratio': info.get('pegRatio', 0),
                'ev_revenue': info.get('enterpriseToRevenue', 0),
                'ev_ebitda': info.get('enterpriseToEbitda', 0),
                
                # Î∞∞Îãπ
                'dividend_yield': info.get('dividendYield', 0),
                'payout_ratio': info.get('payoutRatio', 0),
                
                # Í∏∞Ïà†Ï†Å ÏßÄÌëú
                'beta': info.get('beta', 0),
                'fifty_two_week_high': info.get('fiftyTwoWeekHigh', 0),
                'fifty_two_week_low': info.get('fiftyTwoWeekLow', 0),
                
                # ÏóÖÎç∞Ïù¥Ìä∏ ÏãúÍ∞Ñ
                'last_updated': datetime.now().isoformat()
            }
            
            # Ï∫êÏãú Ï†ÄÏû•
            self.financial_cache[symbol] = financial_data
            
            logger.info(f"‚úÖ {symbol} Ïû¨Î¨¥Ï†úÌëú Îç∞Ïù¥ÌÑ∞ ÏàòÏßë ÏôÑÎ£å")
            return financial_data
            
        except Exception as e:
            logger.error(f"‚ùå {symbol} Ïû¨Î¨¥Ï†úÌëú ÏàòÏßë Ïã§Ìå®: {e}")
            return {}
    
    def _get_latest_value(self, df, column_name):
        """Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑÏóêÏÑú ÏµúÏã† Í∞í Ï∂îÏ∂ú"""
        try:
            if df is not None and column_name in df.index:
                return float(df.loc[column_name].iloc[0])
            return 0
        except:
            return 0
    
    # === üì∞ Îâ¥Ïä§ ÌÅ¨Î°§ÎßÅ ===
    def get_stock_news(self, symbol: str, days: int = 7) -> List[Dict]:
        """Ï£ºÏãù Í¥ÄÎ†® Îâ¥Ïä§ ÌÅ¨Î°§ÎßÅ"""
        try:
            logger.info(f"üì∞ {symbol} Îâ¥Ïä§ ÌÅ¨Î°§ÎßÅ Ï§ë...")
            
            all_news = []
            
            # 1. Yahoo Finance Îâ¥Ïä§
            yahoo_news = self._crawl_yahoo_news(symbol)
            all_news.extend(yahoo_news)
            
            # 2. Google News Îâ¥Ïä§
            google_news = self._crawl_google_news(symbol)
            all_news.extend(google_news)
            
            # 3. MarketWatch Îâ¥Ïä§
            marketwatch_news = self._crawl_marketwatch_news(symbol)
            all_news.extend(marketwatch_news)
            
            # Ï§ëÎ≥µ Ï†úÍ±∞ Î∞è Ï†ïÎ†¨
            unique_news = self._remove_duplicate_news(all_news)
            recent_news = [news for news in unique_news if self._is_recent_news(news, days)]
            
            # ÏãúÍ∞ÑÏàú Ï†ïÎ†¨
            recent_news.sort(key=lambda x: x.get('published_date', ''), reverse=True)
            
            logger.info(f"‚úÖ {symbol} Îâ¥Ïä§ {len(recent_news)}Í∞ú ÏàòÏßë ÏôÑÎ£å")
            return recent_news[:50]  # ÏµúÎåÄ 50Í∞ú
            
        except Exception as e:
            logger.error(f"‚ùå {symbol} Îâ¥Ïä§ ÌÅ¨Î°§ÎßÅ Ïã§Ìå®: {e}")
            return []
    
    def _crawl_yahoo_news(self, symbol: str) -> List[Dict]:
        """Yahoo Finance Îâ¥Ïä§ ÌÅ¨Î°§ÎßÅ"""
        try:
            url = f"https://finance.yahoo.com/quote/{symbol}/news"
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            news_list = []
            
            # Îâ¥Ïä§ ÏïÑÏù¥ÌÖú Ï∞æÍ∏∞
            news_items = soup.find_all('div', {'class': re.compile(r'.*stream-item.*')})
            
            for item in news_items[:10]:  # ÏµúÎåÄ 10Í∞ú
                try:
                    title_elem = item.find('h3') or item.find('a')
                    if title_elem:
                        title = title_elem.get_text(strip=True)
                        link = title_elem.get('href', '')
                        
                        if link and not link.startswith('http'):
                            link = f"https://finance.yahoo.com{link}"
                        
                        # ÏãúÍ∞Ñ Ï†ïÎ≥¥ Ï∂îÏ∂ú
                        time_elem = item.find('time') or item.find('span', {'class': re.compile(r'.*time.*')})
                        published_date = time_elem.get_text(strip=True) if time_elem else ''
                        
                        news_list.append({
                            'title': title,
                            'link': link,
                            'source': 'Yahoo Finance',
                            'published_date': published_date,
                            'symbol': symbol
                        })
                except:
                    continue
            
            return news_list
            
        except Exception as e:
            logger.warning(f"Yahoo Finance Îâ¥Ïä§ ÌÅ¨Î°§ÎßÅ Ïã§Ìå®: {e}")
            return []
    
    def _crawl_google_news(self, symbol: str) -> List[Dict]:
        """Google News Îâ¥Ïä§ ÌÅ¨Î°§ÎßÅ"""
        try:
            # Google News RSS ÏÇ¨Ïö©
            query = f"{symbol} stock"
            url = f"https://news.google.com/rss/search?q={quote(query)}&hl=en-US&gl=US&ceid=US:en"
            
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'xml')
            
            news_list = []
            items = soup.find_all('item')
            
            for item in items[:10]:  # ÏµúÎåÄ 10Í∞ú
                try:
                    title = item.find('title').get_text(strip=True)
                    link = item.find('link').get_text(strip=True)
                    pub_date = item.find('pubDate').get_text(strip=True)
                    source = item.find('source').get_text(strip=True) if item.find('source') else 'Google News'
                    
                    news_list.append({
                        'title': title,
                        'link': link,
                        'source': source,
                        'published_date': pub_date,
                        'symbol': symbol
                    })
                except:
                    continue
            
            return news_list
            
        except Exception as e:
            logger.warning(f"Google News ÌÅ¨Î°§ÎßÅ Ïã§Ìå®: {e}")
            return []
    
    def _crawl_marketwatch_news(self, symbol: str) -> List[Dict]:
        """MarketWatch Îâ¥Ïä§ ÌÅ¨Î°§ÎßÅ"""
        try:
            url = f"https://www.marketwatch.com/investing/stock/{symbol.lower()}"
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            news_list = []
            
            # Îâ¥Ïä§ ÏÑπÏÖò Ï∞æÍ∏∞
            news_section = soup.find('div', {'class': re.compile(r'.*news.*')})
            if news_section:
                news_items = news_section.find_all('a', href=True)
                
                for item in news_items[:5]:  # ÏµúÎåÄ 5Í∞ú
                    try:
                        title = item.get_text(strip=True)
                        link = item.get('href', '')
                        
                        if link and not link.startswith('http'):
                            link = f"https://www.marketwatch.com{link}"
                        
                        if title and len(title) > 10:  # Ï†úÎ™©Ïù¥ ÏûàÎäî Í≤ΩÏö∞Îßå
                            news_list.append({
                                'title': title,
                                'link': link,
                                'source': 'MarketWatch',
                                'published_date': '',
                                'symbol': symbol
                            })
                    except:
                        continue
            
            return news_list
            
        except Exception as e:
            logger.warning(f"MarketWatch Îâ¥Ïä§ ÌÅ¨Î°§ÎßÅ Ïã§Ìå®: {e}")
            return []
    
    def _remove_duplicate_news(self, news_list: List[Dict]) -> List[Dict]:
        """Ï§ëÎ≥µ Îâ¥Ïä§ Ï†úÍ±∞"""
        seen_titles = set()
        unique_news = []
        
        for news in news_list:
            title = news.get('title', '').lower()
            if title not in seen_titles and len(title) > 10:
                seen_titles.add(title)
                unique_news.append(news)
        
        return unique_news
    
    def _is_recent_news(self, news: Dict, days: int) -> bool:
        """ÏµúÍ∑º Îâ¥Ïä§Ïù∏ÏßÄ ÌôïÏù∏"""
        # Í∞ÑÎã®Ìïú Íµ¨ÌòÑ - Ïã§Ï†úÎ°úÎäî ÎÇ†Ïßú ÌååÏã± ÌïÑÏöî
        return True  # ÏùºÎã® Î™®Îì† Îâ¥Ïä§Î•º ÏµúÍ∑ºÏúºÎ°ú Ï≤òÎ¶¨
    
    # === üìà Í≤ΩÏ†úÏßÄÌëú ÏàòÏßë ===
    def get_economic_indicators(self) -> Dict:
        """Ï£ºÏöî Í≤ΩÏ†úÏßÄÌëú ÏàòÏßë"""
        try:
            logger.info("üìà Í≤ΩÏ†úÏßÄÌëú ÏàòÏßë Ï§ë...")
            
            indicators = {}
            
            # 1. VIX (Í≥µÌè¨ÏßÄÏàò) - Yahoo FinanceÏóêÏÑú
            vix_data = yf.Ticker("^VIX")
            vix_hist = vix_data.history(period="1d")
            if not vix_hist.empty:
                indicators['vix'] = float(vix_hist['Close'].iloc[-1])
            
            # 2. 10ÎÖÑ Íµ≠Ï±ÑÏàòÏùµÎ•†
            tnx_data = yf.Ticker("^TNX")
            tnx_hist = tnx_data.history(period="1d")
            if not tnx_hist.empty:
                indicators['10y_treasury_yield'] = float(tnx_hist['Close'].iloc[-1])
            
            # 3. Îã¨Îü¨ Ïù∏Îç±Ïä§
            dxy_data = yf.Ticker("DX-Y.NYB")
            dxy_hist = dxy_data.history(period="1d")
            if not dxy_hist.empty:
                indicators['dollar_index'] = float(dxy_hist['Close'].iloc[-1])
            
            # 4. Ï£ºÏöî ÏßÄÏàò
            sp500 = yf.Ticker("^GSPC")
            sp500_hist = sp500.history(period="2d")
            if len(sp500_hist) >= 2:
                indicators['sp500_change'] = float((sp500_hist['Close'].iloc[-1] - sp500_hist['Close'].iloc[-2]) / sp500_hist['Close'].iloc[-2] * 100)
            
            nasdaq = yf.Ticker("^IXIC")
            nasdaq_hist = nasdaq.history(period="2d")
            if len(nasdaq_hist) >= 2:
                indicators['nasdaq_change'] = float((nasdaq_hist['Close'].iloc[-1] - nasdaq_hist['Close'].iloc[-2]) / nasdaq_hist['Close'].iloc[-2] * 100)
            
            indicators['last_updated'] = datetime.now().isoformat()
            
            logger.info("‚úÖ Í≤ΩÏ†úÏßÄÌëú ÏàòÏßë ÏôÑÎ£å")
            return indicators
            
        except Exception as e:
            logger.error(f"‚ùå Í≤ΩÏ†úÏßÄÌëú ÏàòÏßë Ïã§Ìå®: {e}")
            return {}
    
    # === üéØ Ï¢ÖÌï© Î∂ÑÏÑù ===
    def get_comprehensive_analysis(self, symbol: str) -> Dict:
        """Ï¢ÖÌï© Î∂ÑÏÑù Îç∞Ïù¥ÌÑ∞ ÏàòÏßë"""
        try:
            logger.info(f"üéØ {symbol} Ï¢ÖÌï© Î∂ÑÏÑù Îç∞Ïù¥ÌÑ∞ ÏàòÏßë Ï§ë...")
            
            # 1. Ïû¨Î¨¥Ï†úÌëú Îç∞Ïù¥ÌÑ∞
            financial_data = self.get_financial_statements(symbol)
            
            # 2. Îâ¥Ïä§ Îç∞Ïù¥ÌÑ∞
            news_data = self.get_stock_news(symbol)
            
            # 3. Í≤ΩÏ†úÏßÄÌëú
            economic_data = self.get_economic_indicators()
            
            # 4. Í∏∞Ïà†Ï†Å Î∂ÑÏÑù Îç∞Ïù¥ÌÑ∞ (yfinance)
            technical_data = self._get_technical_analysis(symbol)
            
            # 5. Ïï†ÎÑêÎ¶¨Ïä§Ìä∏ Ï∂îÏ≤ú Îç∞Ïù¥ÌÑ∞
            analyst_data = self._get_analyst_recommendations(symbol)
            
            comprehensive_data = {
                'symbol': symbol,
                'analysis_timestamp': datetime.now().isoformat(),
                'financial_data': financial_data,
                'news_data': news_data,
                'economic_indicators': economic_data,
                'technical_analysis': technical_data,
                'analyst_recommendations': analyst_data,
                'data_sources': [
                    'Yahoo Finance',
                    'Google News',
                    'MarketWatch',
                    'yfinance'
                ]
            }
            
            logger.info(f"‚úÖ {symbol} Ï¢ÖÌï© Î∂ÑÏÑù Îç∞Ïù¥ÌÑ∞ ÏàòÏßë ÏôÑÎ£å")
            return comprehensive_data
            
        except Exception as e:
            logger.error(f"‚ùå {symbol} Ï¢ÖÌï© Î∂ÑÏÑù Ïã§Ìå®: {e}")
            return {}
    
    def _get_technical_analysis(self, symbol: str) -> Dict:
        """Í∏∞Ïà†Ï†Å Î∂ÑÏÑù Îç∞Ïù¥ÌÑ∞ ÏàòÏßë"""
        try:
            ticker = yf.Ticker(symbol)
            hist = ticker.history(period="6mo")  # 6Í∞úÏõî Îç∞Ïù¥ÌÑ∞
            
            if hist.empty:
                return {}
            
            # Ïù¥ÎèôÌèâÍ∑† Í≥ÑÏÇ∞
            hist['SMA_20'] = hist['Close'].rolling(window=20).mean()
            hist['SMA_50'] = hist['Close'].rolling(window=50).mean()
            hist['SMA_200'] = hist['Close'].rolling(window=200).mean()
            
            # RSI Í≥ÑÏÇ∞
            delta = hist['Close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
            rs = gain / loss
            hist['RSI'] = 100 - (100 / (1 + rs))
            
            current_data = hist.iloc[-1]
            
            return {
                'current_price': float(current_data['Close']),
                'volume': int(current_data['Volume']),
                'sma_20': float(current_data['SMA_20']) if not pd.isna(current_data['SMA_20']) else 0,
                'sma_50': float(current_data['SMA_50']) if not pd.isna(current_data['SMA_50']) else 0,
                'sma_200': float(current_data['SMA_200']) if not pd.isna(current_data['SMA_200']) else 0,
                'rsi': float(current_data['RSI']) if not pd.isna(current_data['RSI']) else 50,
                'day_change': float((current_data['Close'] - hist['Close'].iloc[-2]) / hist['Close'].iloc[-2] * 100) if len(hist) > 1 else 0
            }
            
        except Exception as e:
            logger.warning(f"Í∏∞Ïà†Ï†Å Î∂ÑÏÑù Ïã§Ìå®: {e}")
            return {}
    
    def _get_analyst_recommendations(self, symbol: str) -> Dict:
        """Ïï†ÎÑêÎ¶¨Ïä§Ìä∏ Ï∂îÏ≤ú Îç∞Ïù¥ÌÑ∞ ÏàòÏßë"""
        try:
            ticker = yf.Ticker(symbol)
            recommendations = ticker.recommendations
            
            if recommendations is not None and not recommendations.empty:
                latest_rec = recommendations.iloc[-1]
                return {
                    'firm': latest_rec.get('Firm', ''),
                    'to_grade': latest_rec.get('To Grade', ''),
                    'from_grade': latest_rec.get('From Grade', ''),
                    'action': latest_rec.get('Action', ''),
                    'date': str(latest_rec.name) if hasattr(latest_rec, 'name') else ''
                }
            
            return {}
            
        except Exception as e:
            logger.warning(f"Ïï†ÎÑêÎ¶¨Ïä§Ìä∏ Ï∂îÏ≤ú ÏàòÏßë Ïã§Ìå®: {e}")
            return {}
    
    # === üíæ Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû• ===
    def save_analysis_to_file(self, symbol: str, data: Dict, filename: str = None):
        """Î∂ÑÏÑù Îç∞Ïù¥ÌÑ∞Î•º ÌååÏùºÎ°ú Ï†ÄÏû•"""
        try:
            if not filename:
                filename = f"{symbol}_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"‚úÖ Î∂ÑÏÑù Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû• ÏôÑÎ£å: {filename}")
            
        except Exception as e:
            logger.error(f"‚ùå Îç∞Ïù¥ÌÑ∞ Ï†ÄÏû• Ïã§Ìå®: {e}")

# === üöÄ ÏÇ¨Ïö© ÏòàÏãú ===
async def main():
    """Î©îÏù∏ Ïã§Ìñâ Ìï®Ïàò"""
    crawler = USFinancialCrawler()
    
    # ÌÖåÏä§Ìä∏ Ï¢ÖÎ™©
    test_symbols = ['AAPL', 'MSFT', 'GOOGL']
    
    for symbol in test_symbols:
        print(f"\n{'='*50}")
        print(f"üîç {symbol} Î∂ÑÏÑù ÏãúÏûë")
        print('='*50)
        
        # Ï¢ÖÌï© Î∂ÑÏÑù ÏàòÌñâ
        analysis_data = crawler.get_comprehensive_analysis(symbol)
        
        if analysis_data:
            # Í≤∞Í≥º Ï∂úÎ†•
            financial = analysis_data.get('financial_data', {})
            news = analysis_data.get('news_data', [])
            
            print(f"üìä Ïû¨Î¨¥ Ï†ïÎ≥¥:")
            print(f"   ÌöåÏÇ¨Î™Ö: {financial.get('company_name', 'N/A')}")
            print(f"   ÏÑπÌÑ∞: {financial.get('sector', 'N/A')}")
            print(f"   ÏãúÍ∞ÄÏ¥ùÏï°: ${financial.get('market_cap', 0):,}")
            print(f"   PER: {financial.get('pe_ratio', 0):.2f}")
            print(f"   ROE: {financial.get('roe', 0):.2%}")
            
            print(f"\nüì∞ ÏµúÍ∑º Îâ¥Ïä§ ({len(news)}Í∞ú):")
            for i, article in enumerate(news[:3], 1):
                print(f"   {i}. {article.get('title', 'N/A')[:80]}...")
                print(f"      Ï∂úÏ≤ò: {article.get('source', 'N/A')}")
            
            # ÌååÏùº Ï†ÄÏû•
            crawler.save_analysis_to_file(symbol, analysis_data)
        
        # API Ìò∏Ï∂ú Ï†úÌïúÏùÑ ÏúÑÌïú ÎåÄÍ∏∞
        await asyncio.sleep(2)

if __name__ == "__main__":
    asyncio.run(main()) 