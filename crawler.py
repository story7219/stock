"""
ğŸ‡ºğŸ‡¸ ë¯¸êµ­ì£¼ì‹ ì¬ë¬´ì œí‘œ ë° ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œìŠ¤í…œ
ë¬´ë£Œ ë°ì´í„° ì†ŒìŠ¤ë¥¼ í™œìš©í•œ ì¢…í•© ì •ë³´ ìˆ˜ì§‘

ì£¼ìš” ê¸°ëŠ¥:
1. Yahoo Finance ì¬ë¬´ì œí‘œ í¬ë¡¤ë§
2. SEC EDGAR ê³µì‹œ ë°ì´í„° ìˆ˜ì§‘
3. ë‰´ìŠ¤ í¬ë¡¤ë§ (Google News, Yahoo Finance)
4. ê²½ì œì§€í‘œ ìˆ˜ì§‘ (FRED API)
5. ì†Œì…œ ì„¼í‹°ë¨¼íŠ¸ ë¶„ì„ (Reddit, Twitter)
"""
import requests
import yfinance as yf
import pandas as pd
import numpy as np
from bs4 import BeautifulSoup
import time
import json
from datetime import datetime, timedelta
from typing import Dict, List, Optional
import logging
import asyncio
import aiohttp
from urllib.parse import quote
import re

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class USFinancialCrawler:
    """ğŸ‡ºğŸ‡¸ ë¯¸êµ­ì£¼ì‹ ì¬ë¬´ì œí‘œ ë° ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹œìŠ¤í…œ"""
    
    def __init__(self):
        """ì´ˆê¸°í™”"""
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        
        # ìºì‹œ ì €ì¥ì†Œ
        self.financial_cache = {}
        self.news_cache = {}
        
        # API í‚¤ë“¤ (ë¬´ë£Œ ë²„ì „)
        self.fred_api_key = None  # FRED API í‚¤ (ë¬´ë£Œ)
        self.alpha_vantage_key = None  # Alpha Vantage API í‚¤ (ë¬´ë£Œ)
    
    # === ğŸ“Š ì¬ë¬´ì œí‘œ ë°ì´í„° ìˆ˜ì§‘ ===
    def get_financial_statements(self, symbol: str) -> Dict:
        """Yahoo Financeì—ì„œ ì¬ë¬´ì œí‘œ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            logger.info(f"ğŸ“Š {symbol} ì¬ë¬´ì œí‘œ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            
            # yfinance ì‚¬ìš© (ê°€ì¥ ì•ˆì •ì )
            ticker = yf.Ticker(symbol)
            
            # ê¸°ë³¸ ì •ë³´
            info = ticker.info
            
            # ì¬ë¬´ì œí‘œ ë°ì´í„°
            financials = ticker.financials
            balance_sheet = ticker.balance_sheet
            cash_flow = ticker.cashflow
            
            # ì£¼ìš” ì¬ë¬´ ì§€í‘œ ì¶”ì¶œ
            financial_data = {
                'symbol': symbol,
                'company_name': info.get('longName', symbol),
                'sector': info.get('sector', 'Unknown'),
                'industry': info.get('industry', 'Unknown'),
                'market_cap': info.get('marketCap', 0),
                'enterprise_value': info.get('enterpriseValue', 0),
                
                # ìˆ˜ìµì„± ì§€í‘œ
                'revenue': self._get_latest_value(financials, 'Total Revenue'),
                'gross_profit': self._get_latest_value(financials, 'Gross Profit'),
                'operating_income': self._get_latest_value(financials, 'Operating Income'),
                'net_income': self._get_latest_value(financials, 'Net Income'),
                'ebitda': info.get('ebitda', 0),
                
                # ë§ˆì§„ ì§€í‘œ
                'gross_margin': info.get('grossMargins', 0),
                'operating_margin': info.get('operatingMargins', 0),
                'profit_margin': info.get('profitMargins', 0),
                
                # íš¨ìœ¨ì„± ì§€í‘œ
                'roe': info.get('returnOnEquity', 0),
                'roa': info.get('returnOnAssets', 0),
                'roic': info.get('returnOnInvestmentCapital', 0),
                
                # ì¬ë¬´ ê±´ì „ì„±
                'total_debt': self._get_latest_value(balance_sheet, 'Total Debt'),
                'total_cash': self._get_latest_value(balance_sheet, 'Cash And Cash Equivalents'),
                'total_assets': self._get_latest_value(balance_sheet, 'Total Assets'),
                'shareholders_equity': self._get_latest_value(balance_sheet, 'Stockholders Equity'),
                
                # ë¶€ì±„ ë¹„ìœ¨
                'debt_to_equity': info.get('debtToEquity', 0),
                'current_ratio': info.get('currentRatio', 0),
                'quick_ratio': info.get('quickRatio', 0),
                
                # ì„±ì¥ë¥ 
                'revenue_growth': info.get('revenueGrowth', 0),
                'earnings_growth': info.get('earningsGrowth', 0),
                
                # ë°¸ë¥˜ì—ì´ì…˜
                'pe_ratio': info.get('trailingPE', 0),
                'forward_pe': info.get('forwardPE', 0),
                'pb_ratio': info.get('priceToBook', 0),
                'ps_ratio': info.get('priceToSalesTrailing12Months', 0),
                'peg_ratio': info.get('pegRatio', 0),
                'ev_revenue': info.get('enterpriseToRevenue', 0),
                'ev_ebitda': info.get('enterpriseToEbitda', 0),
                
                # ë°°ë‹¹
                'dividend_yield': info.get('dividendYield', 0),
                'payout_ratio': info.get('payoutRatio', 0),
                
                # ê¸°ìˆ ì  ì§€í‘œ
                'beta': info.get('beta', 0),
                'fifty_two_week_high': info.get('fiftyTwoWeekHigh', 0),
                'fifty_two_week_low': info.get('fiftyTwoWeekLow', 0),
                
                # ì—…ë°ì´íŠ¸ ì‹œê°„
                'last_updated': datetime.now().isoformat()
            }
            
            # ìºì‹œ ì €ì¥
            self.financial_cache[symbol] = financial_data
            
            logger.info(f"âœ… {symbol} ì¬ë¬´ì œí‘œ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
            return financial_data
            
        except Exception as e:
            logger.error(f"âŒ {symbol} ì¬ë¬´ì œí‘œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return {}
    
    def _get_latest_value(self, df, column_name):
        """ë°ì´í„°í”„ë ˆì„ì—ì„œ ìµœì‹  ê°’ ì¶”ì¶œ"""
        try:
            if df is not None and column_name in df.index:
                return float(df.loc[column_name].iloc[0])
            return 0
        except:
            return 0
    
    # === ğŸ“° ë‰´ìŠ¤ í¬ë¡¤ë§ ===
    def get_stock_news(self, symbol: str, days: int = 7) -> List[Dict]:
        """ì£¼ì‹ ê´€ë ¨ ë‰´ìŠ¤ í¬ë¡¤ë§"""
        try:
            logger.info(f"ğŸ“° {symbol} ë‰´ìŠ¤ í¬ë¡¤ë§ ì¤‘...")
            
            all_news = []
            
            # 1. Yahoo Finance ë‰´ìŠ¤
            yahoo_news = self._crawl_yahoo_news(symbol)
            all_news.extend(yahoo_news)
            
            # 2. Google News ë‰´ìŠ¤
            google_news = self._crawl_google_news(symbol)
            all_news.extend(google_news)
            
            # 3. MarketWatch ë‰´ìŠ¤
            marketwatch_news = self._crawl_marketwatch_news(symbol)
            all_news.extend(marketwatch_news)
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë ¬
            unique_news = self._remove_duplicate_news(all_news)
            recent_news = [news for news in unique_news if self._is_recent_news(news, days)]
            
            # ì‹œê°„ìˆœ ì •ë ¬
            recent_news.sort(key=lambda x: x.get('published_date', ''), reverse=True)
            
            logger.info(f"âœ… {symbol} ë‰´ìŠ¤ {len(recent_news)}ê°œ ìˆ˜ì§‘ ì™„ë£Œ")
            return recent_news[:50]  # ìµœëŒ€ 50ê°œ
            
        except Exception as e:
            logger.error(f"âŒ {symbol} ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return []
    
    def _crawl_yahoo_news(self, symbol: str) -> List[Dict]:
        """Yahoo Finance ë‰´ìŠ¤ í¬ë¡¤ë§"""
        try:
            url = f"https://finance.yahoo.com/quote/{symbol}/news"
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            news_list = []
            
            # ë‰´ìŠ¤ ì•„ì´í…œ ì°¾ê¸°
            news_items = soup.find_all('div', {'class': re.compile(r'.*stream-item.*')})
            
            for item in news_items[:10]:  # ìµœëŒ€ 10ê°œ
                try:
                    title_elem = item.find('h3') or item.find('a')
                    if title_elem:
                        title = title_elem.get_text(strip=True)
                        link = title_elem.get('href', '')
                        
                        if link and not link.startswith('http'):
                            link = f"https://finance.yahoo.com{link}"
                        
                        # ì‹œê°„ ì •ë³´ ì¶”ì¶œ
                        time_elem = item.find('time') or item.find('span', {'class': re.compile(r'.*time.*')})
                        published_date = time_elem.get_text(strip=True) if time_elem else ''
                        
                        news_list.append({
                            'title': title,
                            'link': link,
                            'source': 'Yahoo Finance',
                            'published_date': published_date,
                            'symbol': symbol
                        })
                except:
                    continue
            
            return news_list
            
        except Exception as e:
            logger.warning(f"Yahoo Finance ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return []
    
    def _crawl_google_news(self, symbol: str) -> List[Dict]:
        """Google News ë‰´ìŠ¤ í¬ë¡¤ë§"""
        try:
            # Google News RSS ì‚¬ìš©
            query = f"{symbol} stock"
            url = f"https://news.google.com/rss/search?q={quote(query)}&hl=en-US&gl=US&ceid=US:en"
            
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'xml')
            
            news_list = []
            items = soup.find_all('item')
            
            for item in items[:10]:  # ìµœëŒ€ 10ê°œ
                try:
                    title = item.find('title').get_text(strip=True)
                    link = item.find('link').get_text(strip=True)
                    pub_date = item.find('pubDate').get_text(strip=True)
                    source = item.find('source').get_text(strip=True) if item.find('source') else 'Google News'
                    
                    news_list.append({
                        'title': title,
                        'link': link,
                        'source': source,
                        'published_date': pub_date,
                        'symbol': symbol
                    })
                except:
                    continue
            
            return news_list
            
        except Exception as e:
            logger.warning(f"Google News í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return []
    
    def _crawl_marketwatch_news(self, symbol: str) -> List[Dict]:
        """MarketWatch ë‰´ìŠ¤ í¬ë¡¤ë§"""
        try:
            url = f"https://www.marketwatch.com/investing/stock/{symbol.lower()}"
            response = self.session.get(url, timeout=10)
            soup = BeautifulSoup(response.content, 'html.parser')
            
            news_list = []
            
            # ë‰´ìŠ¤ ì„¹ì…˜ ì°¾ê¸°
            news_section = soup.find('div', {'class': re.compile(r'.*news.*')})
            if news_section:
                news_items = news_section.find_all('a', href=True)
                
                for item in news_items[:5]:  # ìµœëŒ€ 5ê°œ
                    try:
                        title = item.get_text(strip=True)
                        link = item.get('href', '')
                        
                        if link and not link.startswith('http'):
                            link = f"https://www.marketwatch.com{link}"
                        
                        if title and len(title) > 10:  # ì œëª©ì´ ìˆëŠ” ê²½ìš°ë§Œ
                            news_list.append({
                                'title': title,
                                'link': link,
                                'source': 'MarketWatch',
                                'published_date': '',
                                'symbol': symbol
                            })
                    except:
                        continue
            
            return news_list
            
        except Exception as e:
            logger.warning(f"MarketWatch ë‰´ìŠ¤ í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
            return []
    
    def _remove_duplicate_news(self, news_list: List[Dict]) -> List[Dict]:
        """ì¤‘ë³µ ë‰´ìŠ¤ ì œê±°"""
        seen_titles = set()
        unique_news = []
        
        for news in news_list:
            title = news.get('title', '').lower()
            if title not in seen_titles and len(title) > 10:
                seen_titles.add(title)
                unique_news.append(news)
        
        return unique_news
    
    def _is_recent_news(self, news: Dict, days: int) -> bool:
        """ìµœê·¼ ë‰´ìŠ¤ì¸ì§€ í™•ì¸"""
        # ê°„ë‹¨í•œ êµ¬í˜„ - ì‹¤ì œë¡œëŠ” ë‚ ì§œ íŒŒì‹± í•„ìš”
        return True  # ì¼ë‹¨ ëª¨ë“  ë‰´ìŠ¤ë¥¼ ìµœê·¼ìœ¼ë¡œ ì²˜ë¦¬
    
    # === ğŸ“ˆ ê²½ì œì§€í‘œ ìˆ˜ì§‘ ===
    def get_economic_indicators(self) -> Dict:
        """ì£¼ìš” ê²½ì œì§€í‘œ ìˆ˜ì§‘"""
        try:
            logger.info("ğŸ“ˆ ê²½ì œì§€í‘œ ìˆ˜ì§‘ ì¤‘...")
            
            indicators = {}
            
            # 1. VIX (ê³µí¬ì§€ìˆ˜) - Yahoo Financeì—ì„œ
            vix_data = yf.Ticker("^VIX")
            vix_hist = vix_data.history(period="1d")
            if not vix_hist.empty:
                indicators['vix'] = float(vix_hist['Close'].iloc[-1])
            
            # 2. 10ë…„ êµ­ì±„ìˆ˜ìµë¥ 
            tnx_data = yf.Ticker("^TNX")
            tnx_hist = tnx_data.history(period="1d")
            if not tnx_hist.empty:
                indicators['10y_treasury_yield'] = float(tnx_hist['Close'].iloc[-1])
            
            # 3. ë‹¬ëŸ¬ ì¸ë±ìŠ¤
            dxy_data = yf.Ticker("DX-Y.NYB")
            dxy_hist = dxy_data.history(period="1d")
            if not dxy_hist.empty:
                indicators['dollar_index'] = float(dxy_hist['Close'].iloc[-1])
            
            # 4. ì£¼ìš” ì§€ìˆ˜
            sp500 = yf.Ticker("^GSPC")
            sp500_hist = sp500.history(period="2d")
            if len(sp500_hist) >= 2:
                indicators['sp500_change'] = float((sp500_hist['Close'].iloc[-1] - sp500_hist['Close'].iloc[-2]) / sp500_hist['Close'].iloc[-2] * 100)
            
            nasdaq = yf.Ticker("^IXIC")
            nasdaq_hist = nasdaq.history(period="2d")
            if len(nasdaq_hist) >= 2:
                indicators['nasdaq_change'] = float((nasdaq_hist['Close'].iloc[-1] - nasdaq_hist['Close'].iloc[-2]) / nasdaq_hist['Close'].iloc[-2] * 100)
            
            indicators['last_updated'] = datetime.now().isoformat()
            
            logger.info("âœ… ê²½ì œì§€í‘œ ìˆ˜ì§‘ ì™„ë£Œ")
            return indicators
            
        except Exception as e:
            logger.error(f"âŒ ê²½ì œì§€í‘œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return {}
    
    # === ğŸ¯ ì¢…í•© ë¶„ì„ ===
    def get_comprehensive_analysis(self, symbol: str) -> Dict:
        """ì¢…í•© ë¶„ì„ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            logger.info(f"ğŸ¯ {symbol} ì¢…í•© ë¶„ì„ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
            
            # 1. ì¬ë¬´ì œí‘œ ë°ì´í„°
            financial_data = self.get_financial_statements(symbol)
            
            # 2. ë‰´ìŠ¤ ë°ì´í„°
            news_data = self.get_stock_news(symbol)
            
            # 3. ê²½ì œì§€í‘œ
            economic_data = self.get_economic_indicators()
            
            # 4. ê¸°ìˆ ì  ë¶„ì„ ë°ì´í„° (yfinance)
            technical_data = self._get_technical_analysis(symbol)
            
            # 5. ì• ë„ë¦¬ìŠ¤íŠ¸ ì¶”ì²œ ë°ì´í„°
            analyst_data = self._get_analyst_recommendations(symbol)
            
            comprehensive_data = {
                'symbol': symbol,
                'analysis_timestamp': datetime.now().isoformat(),
                'financial_data': financial_data,
                'news_data': news_data,
                'economic_indicators': economic_data,
                'technical_analysis': technical_data,
                'analyst_recommendations': analyst_data,
                'data_sources': [
                    'Yahoo Finance',
                    'Google News',
                    'MarketWatch',
                    'yfinance'
                ]
            }
            
            logger.info(f"âœ… {symbol} ì¢…í•© ë¶„ì„ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
            return comprehensive_data
            
        except Exception as e:
            logger.error(f"âŒ {symbol} ì¢…í•© ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def _get_technical_analysis(self, symbol: str) -> Dict:
        """ê¸°ìˆ ì  ë¶„ì„ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            ticker = yf.Ticker(symbol)
            hist = ticker.history(period="6mo")  # 6ê°œì›” ë°ì´í„°
            
            if hist.empty:
                return {}
            
            # ì´ë™í‰ê·  ê³„ì‚°
            hist['SMA_20'] = hist['Close'].rolling(window=20).mean()
            hist['SMA_50'] = hist['Close'].rolling(window=50).mean()
            hist['SMA_200'] = hist['Close'].rolling(window=200).mean()
            
            # RSI ê³„ì‚°
            delta = hist['Close'].diff()
            gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()
            loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()
            rs = gain / loss
            hist['RSI'] = 100 - (100 / (1 + rs))
            
            current_data = hist.iloc[-1]
            
            return {
                'current_price': float(current_data['Close']),
                'volume': int(current_data['Volume']),
                'sma_20': float(current_data['SMA_20']) if not pd.isna(current_data['SMA_20']) else 0,
                'sma_50': float(current_data['SMA_50']) if not pd.isna(current_data['SMA_50']) else 0,
                'sma_200': float(current_data['SMA_200']) if not pd.isna(current_data['SMA_200']) else 0,
                'rsi': float(current_data['RSI']) if not pd.isna(current_data['RSI']) else 50,
                'day_change': float((current_data['Close'] - hist['Close'].iloc[-2]) / hist['Close'].iloc[-2] * 100) if len(hist) > 1 else 0
            }
            
        except Exception as e:
            logger.warning(f"ê¸°ìˆ ì  ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {}
    
    def _get_analyst_recommendations(self, symbol: str) -> Dict:
        """ì• ë„ë¦¬ìŠ¤íŠ¸ ì¶”ì²œ ë°ì´í„° ìˆ˜ì§‘"""
        try:
            ticker = yf.Ticker(symbol)
            recommendations = ticker.recommendations
            
            if recommendations is not None and not recommendations.empty:
                latest_rec = recommendations.iloc[-1]
                return {
                    'firm': latest_rec.get('Firm', ''),
                    'to_grade': latest_rec.get('To Grade', ''),
                    'from_grade': latest_rec.get('From Grade', ''),
                    'action': latest_rec.get('Action', ''),
                    'date': str(latest_rec.name) if hasattr(latest_rec, 'name') else ''
                }
            
            return {}
            
        except Exception as e:
            logger.warning(f"ì• ë„ë¦¬ìŠ¤íŠ¸ ì¶”ì²œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return {}
    
    # === ğŸ’¾ ë°ì´í„° ì €ì¥ ===
    def save_analysis_to_file(self, symbol: str, data: Dict, filename: str = None):
        """ë¶„ì„ ë°ì´í„°ë¥¼ íŒŒì¼ë¡œ ì €ì¥"""
        try:
            if not filename:
                filename = f"{symbol}_analysis_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False, default=str)
            
            logger.info(f"âœ… ë¶„ì„ ë°ì´í„° ì €ì¥ ì™„ë£Œ: {filename}")
            
        except Exception as e:
            logger.error(f"âŒ ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")

# === ğŸš€ ì‚¬ìš© ì˜ˆì‹œ ===
async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    crawler = USFinancialCrawler()
    
    # í…ŒìŠ¤íŠ¸ ì¢…ëª©
    test_symbols = ['AAPL', 'MSFT', 'GOOGL']
    
    for symbol in test_symbols:
        print(f"\n{'='*50}")
        print(f"ğŸ” {symbol} ë¶„ì„ ì‹œì‘")
        print('='*50)
        
        # ì¢…í•© ë¶„ì„ ìˆ˜í–‰
        analysis_data = crawler.get_comprehensive_analysis(symbol)
        
        if analysis_data:
            # ê²°ê³¼ ì¶œë ¥
            financial = analysis_data.get('financial_data', {})
            news = analysis_data.get('news_data', [])
            
            print(f"ğŸ“Š ì¬ë¬´ ì •ë³´:")
            print(f"   íšŒì‚¬ëª…: {financial.get('company_name', 'N/A')}")
            print(f"   ì„¹í„°: {financial.get('sector', 'N/A')}")
            print(f"   ì‹œê°€ì´ì•¡: ${financial.get('market_cap', 0):,}")
            print(f"   PER: {financial.get('pe_ratio', 0):.2f}")
            print(f"   ROE: {financial.get('roe', 0):.2%}")
            
            print(f"\nğŸ“° ìµœê·¼ ë‰´ìŠ¤ ({len(news)}ê°œ):")
            for i, article in enumerate(news[:3], 1):
                print(f"   {i}. {article.get('title', 'N/A')[:80]}...")
                print(f"      ì¶œì²˜: {article.get('source', 'N/A')}")
            
            # íŒŒì¼ ì €ì¥
            crawler.save_analysis_to_file(symbol, analysis_data)
        
        # API í˜¸ì¶œ ì œí•œì„ ìœ„í•œ ëŒ€ê¸°
        await asyncio.sleep(2)

if __name__ == "__main__":
    asyncio.run(main()) 