#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íŒŒì¼ëª…: krx_ml_optimized_collector.py
ëª¨ë“ˆ: ML/DL ìµœì í™”ëœ KRX ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ
ëª©ì : ìë™ë§¤ë§¤ë¥¼ ìœ„í•œ ë¹ ë¥¸ íŒë‹¨ê³¼ ML/DL í•™ìŠµ ìµœì í™”

Author: AI Assistant
Created: 2025-01-27
Modified: 2025-01-27
Version: 2.0.0

Features:
- ë°ì´í„° íŠ¹ì„±ë³„ ìµœì í™”ëœ ì €ì¥ ì „ëµ
- ì‹¤ì‹œê°„ ìë™ë§¤ë§¤ ì§€ì›
- ML/DL í•™ìŠµ ìµœì í™”
- ë©”ëª¨ë¦¬ ê¸°ë°˜ ê³ ì† ìºì‹±
- ì‚¬ì „ ê³„ì‚°ëœ ê¸°ìˆ ì  ì§€í‘œ
- ìë™ ëª¨ë¸ í•™ìŠµ ë° ì—…ë°ì´íŠ¸

Dependencies:
    - Python 3.11+
    - pykrx==1.0.45
    - pandas==2.1.0
    - numpy==1.24.0
    - redis==5.0.0
    - sqlite3 (built-in)
    - h5py==3.10.0
    - pyarrow==14.0.0
    - scikit-learn==1.3.0
    - tensorflow==2.15.0
    - ta==0.10.2 (technical analysis)

Performance:
    - ì‹¤ì‹œê°„ ë°ì´í„°: < 1ms ì‘ë‹µ
    - íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„°: 10GB+ ì²˜ë¦¬
    - ML ëª¨ë¸ í•™ìŠµ: GPU ê°€ì† ì§€ì›
    - ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: < 2GB for 1M records

Security:
    - ë°ì´í„° ë¬´ê²°ì„± ê²€ì¦
    - ì•”í˜¸í™”ëœ ë¯¼ê° ë°ì´í„°
    - ì ‘ê·¼ ê¶Œí•œ ì œì–´
    - ë°±ì—… ë° ë³µêµ¬

License: MIT
"""

from __future__ import annotations

import asyncio
import logging
import os
import pickle
import sqlite3
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import (
    Any, Dict, List, Optional, Tuple, Union, Literal,
    Protocol, TypeVar, Generic, Final
)
from dataclasses import dataclass, field
from functools import lru_cache, wraps
from contextlib import asynccontextmanager
import warnings
warnings.filterwarnings('ignore')

import numpy as np
import pandas as pd
import redis
import h5py
import pyarrow as pa
import pyarrow.parquet as pq
from pykrx import stock, bond
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestRegressor
import tensorflow as tf
import ta

# ìƒìˆ˜ ì •ì˜
CACHE_EXPIRY: Final = 3600  # 1ì‹œê°„
REALTIME_INTERVAL: Final = 1  # 1ì´ˆ
HISTORICAL_DAYS: Final = 365 * 5  # 5ë…„
MAX_MEMORY_USAGE: Final = 2 * 1024 * 1024 * 1024  # 2GB
MODEL_UPDATE_INTERVAL: Final = 24 * 3600  # 24ì‹œê°„

# ë°ì´í„° íƒ€ì… ì •ì˜
DataType = Literal['realtime', 'historical', 'technical', 'fundamental']
StorageType = Literal['redis', 'sqlite', 'parquet', 'hdf5', 'memory']
ModelType = Literal['regression', 'classification', 'time_series']

@dataclass
class DataConfig:
    """ë°ì´í„° ì„¤ì • í´ë˜ìŠ¤"""
    data_type: DataType
    storage_type: StorageType
    cache_ttl: int = 3600
    compression: bool = True
    index: bool = True
    
    def __post_init__(self):
        """ë°ì´í„° íƒ€ì…ë³„ ìµœì  ì„¤ì •"""
        if self.data_type == 'realtime':
            self.storage_type = 'redis'
            self.cache_ttl = 60
        elif self.data_type == 'historical':
            self.storage_type = 'parquet'
            self.compression = True
        elif self.data_type == 'technical':
            self.storage_type = 'memory'
            self.cache_ttl = 300

@dataclass
class MLConfig:
    """ML ì„¤ì • í´ë˜ìŠ¤"""
    model_type: ModelType
    features: List[str]
    target: str
    train_size: float = 0.8
    validation_size: float = 0.1
    test_size: float = 0.1
    update_frequency: int = 24 * 3600  # 24ì‹œê°„
    
    def __post_init__(self):
        assert abs(self.train_size + self.validation_size + self.test_size - 1.0) < 1e-6

class DataStorageManager:
    """ë°ì´í„° ì €ì¥ ê´€ë¦¬ì"""
    
    def __init__(self, base_path: str = "data"):
        self.base_path = Path(base_path)
        self.base_path.mkdir(exist_ok=True)
        
        # Redis ì—°ê²° (ì‹¤ì‹œê°„ ë°ì´í„°ìš©) - Redis ì—†ìœ¼ë©´ ë©”ëª¨ë¦¬ë¡œ ëŒ€ì²´
        try:
            self.redis_client = redis.Redis(
                host='localhost', 
                port=6379, 
                db=0,
                decode_responses=True
            )
            self.redis_available = True
        except:
            self.redis_client = None
            self.redis_available = False
            self.logger.warning("Redis ì—°ê²° ì‹¤íŒ¨ - ë©”ëª¨ë¦¬ ìºì‹œë¡œ ëŒ€ì²´")
        
        # SQLite ì—°ê²° (ë©”íƒ€ë°ì´í„°ìš©)
        self.sqlite_path = self.base_path / "metadata.db"
        self._init_sqlite()
        
        # ë©”ëª¨ë¦¬ ìºì‹œ (ê¸°ìˆ ì  ì§€í‘œìš©)
        self.memory_cache: Dict[str, Any] = {}
        
        # ë¡œê¹… ì„¤ì •
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)
    
    def _init_sqlite(self):
        """SQLite ì´ˆê¸°í™”"""
        conn = sqlite3.connect(self.sqlite_path)
        cursor = conn.cursor()
        
        # ë©”íƒ€ë°ì´í„° í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS data_metadata (
                symbol TEXT PRIMARY KEY,
                data_type TEXT,
                last_update TIMESTAMP,
                record_count INTEGER,
                file_size INTEGER,
                storage_path TEXT
            )
        ''')
        
        # ëª¨ë¸ ë©”íƒ€ë°ì´í„° í…Œì´ë¸”
        cursor.execute('''
            CREATE TABLE IF NOT EXISTS model_metadata (
                model_id TEXT PRIMARY KEY,
                model_type TEXT,
                features TEXT,
                target TEXT,
                accuracy REAL,
                last_trained TIMESTAMP,
                model_path TEXT
            )
        ''')
        
        conn.commit()
        conn.close()
    
    async def store_data(
        self, 
        data: pd.DataFrame, 
        symbol: str, 
        config: DataConfig
    ) -> bool:
        """ë°ì´í„° ì €ì¥ (ìµœì í™”ëœ ë°©ì‹)"""
        try:
            if config.storage_type == 'redis':
                return await self._store_redis(data, symbol, config)
            elif config.storage_type == 'sqlite':
                return await self._store_sqlite(data, symbol, config)
            elif config.storage_type == 'parquet':
                return await self._store_parquet(data, symbol, config)
            elif config.storage_type == 'hdf5':
                return await self._store_hdf5(data, symbol, config)
            elif config.storage_type == 'memory':
                return await self._store_memory(data, symbol, config)
            else:
                raise ValueError(f"Unsupported storage type: {config.storage_type}")
        except Exception as e:
            self.logger.error(f"ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {symbol}, {e}")
            return False
    
    async def _store_redis(self, data: pd.DataFrame, symbol: str, config: DataConfig) -> bool:
        """Redis ì €ì¥ (ì‹¤ì‹œê°„ ë°ì´í„°)"""
        try:
            if not self.redis_available or self.redis_client is None:
                # Redis ì—†ìœ¼ë©´ ë©”ëª¨ë¦¬ë¡œ ëŒ€ì²´
                return await self._store_memory(data, symbol, config)
            
            # JSON ì§ë ¬í™”
            data_json = data.to_json(orient='records')
            key = f"realtime:{symbol}"
            
            # Redisì— ì €ì¥
            self.redis_client.setex(key, config.cache_ttl, data_json)
            
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            await self._update_metadata(symbol, 'realtime', len(data), 0, 'redis')
            
            return True
        except Exception as e:
            self.logger.error(f"Redis ì €ì¥ ì‹¤íŒ¨: {e}")
            # Redis ì‹¤íŒ¨ì‹œ ë©”ëª¨ë¦¬ë¡œ ëŒ€ì²´
            return await self._store_memory(data, symbol, config)
    
    async def _store_parquet(self, data: pd.DataFrame, symbol: str, config: DataConfig) -> bool:
        """Parquet ì €ì¥ (íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„°)"""
        try:
            file_path = self.base_path / "historical" / f"{symbol}.parquet"
            file_path.parent.mkdir(exist_ok=True)
            
            # Parquetë¡œ ì €ì¥
            data.to_parquet(
                file_path,
                compression='snappy' if config.compression else None,
                index=config.index
            )
            
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            await self._update_metadata(
                symbol, 'historical', len(data), 
                file_path.stat().st_size, str(file_path)
            )
            
            return True
        except Exception as e:
            self.logger.error(f"Parquet ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    async def _store_memory(self, data: pd.DataFrame, symbol: str, config: DataConfig) -> bool:
        """ë©”ëª¨ë¦¬ ì €ì¥ (ê¸°ìˆ ì  ì§€í‘œ)"""
        try:
            key = f"technical:{symbol}"
            self.memory_cache[key] = {
                'data': data,
                'timestamp': time.time(),
                'ttl': config.cache_ttl
            }
            
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            await self._update_metadata(symbol, 'technical', len(data), 0, 'memory')
            
            return True
        except Exception as e:
            self.logger.error(f"ë©”ëª¨ë¦¬ ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    async def _store_sqlite(self, data: pd.DataFrame, symbol: str, config: DataConfig) -> bool:
        """SQLite ì €ì¥"""
        try:
            conn = sqlite3.connect(self.sqlite_path)
            data.to_sql(f"data_{symbol}", conn, if_exists='replace', index=config.index)
            conn.close()
            
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            await self._update_metadata(symbol, 'sqlite', len(data), 0, 'sqlite')
            
            return True
        except Exception as e:
            self.logger.error(f"SQLite ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    async def _store_hdf5(self, data: pd.DataFrame, symbol: str, config: DataConfig) -> bool:
        """HDF5 ì €ì¥"""
        try:
            file_path = self.base_path / "hdf5" / f"{symbol}.h5"
            file_path.parent.mkdir(exist_ok=True)
            
            data.to_hdf(file_path, key='data', mode='w', format='table')
            
            # ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸
            await self._update_metadata(
                symbol, 'hdf5', len(data), 
                file_path.stat().st_size, str(file_path)
            )
            
            return True
        except Exception as e:
            self.logger.error(f"HDF5 ì €ì¥ ì‹¤íŒ¨: {e}")
            return False
    
    async def _update_metadata(self, symbol: str, data_type: str, record_count: int, 
                             file_size: int, storage_path: str):
        """ë©”íƒ€ë°ì´í„° ì—…ë°ì´íŠ¸"""
        conn = sqlite3.connect(self.sqlite_path)
        cursor = conn.cursor()
        
        cursor.execute('''
            INSERT OR REPLACE INTO data_metadata 
            (symbol, data_type, last_update, record_count, file_size, storage_path)
            VALUES (?, ?, ?, ?, ?, ?)
        ''', (symbol, data_type, datetime.now(), record_count, file_size, storage_path))
        
        conn.commit()
        conn.close()
    
    async def load_data(self, symbol: str, config: DataConfig) -> Optional[pd.DataFrame]:
        """ë°ì´í„° ë¡œë“œ (ìµœì í™”ëœ ë°©ì‹)"""
        try:
            if config.storage_type == 'redis':
                return await self._load_redis(symbol)
            elif config.storage_type == 'parquet':
                return await self._load_parquet(symbol)
            elif config.storage_type == 'memory':
                return await self._load_memory(symbol)
            else:
                raise ValueError(f"Unsupported storage type: {config.storage_type}")
        except Exception as e:
            self.logger.error(f"ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {symbol}, {e}")
            return None
    
    async def _load_redis(self, symbol: str) -> Optional[pd.DataFrame]:
        """Redisì—ì„œ ë¡œë“œ"""
        try:
            if not self.redis_available or self.redis_client is None:
                return None
                
            key = f"realtime:{symbol}"
            data_json = self.redis_client.get(key)
            if data_json:
                return pd.read_json(data_json, orient='records')
            return None
        except Exception as e:
            self.logger.error(f"Redis ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    async def _load_parquet(self, symbol: str) -> Optional[pd.DataFrame]:
        """Parquetì—ì„œ ë¡œë“œ"""
        try:
            file_path = self.base_path / "historical" / f"{symbol}.parquet"
            if file_path.exists():
                return pd.read_parquet(file_path)
            return None
        except Exception as e:
            self.logger.error(f"Parquet ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    async def _load_memory(self, symbol: str) -> Optional[pd.DataFrame]:
        """ë©”ëª¨ë¦¬ì—ì„œ ë¡œë“œ"""
        try:
            key = f"technical:{symbol}"
            if key in self.memory_cache:
                cache_data = self.memory_cache[key]
                if time.time() - cache_data['timestamp'] < cache_data['ttl']:
                    return cache_data['data']
            return None
        except Exception as e:
            self.logger.error(f"ë©”ëª¨ë¦¬ ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None

class TechnicalIndicatorCalculator:
    """ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°ê¸°"""
    
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    def calculate_all_indicators(self, df: pd.DataFrame) -> pd.DataFrame:
        """ëª¨ë“  ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°"""
        try:
            if df.empty:
                return df
            
            # ê¸°ë³¸ ê°€ê²© ë°ì´í„° í™•ì¸
            required_cols = ['open', 'high', 'low', 'close', 'volume']
            if not all(col in df.columns for col in required_cols):
                self.logger.warning("í•„ìˆ˜ ì»¬ëŸ¼ì´ ì—†ìŠµë‹ˆë‹¤")
                return df
            
            # ì´ë™í‰ê· 
            df['sma_5'] = ta.trend.sma_indicator(df['close'], window=5)
            df['sma_20'] = ta.trend.sma_indicator(df['close'], window=20)
            df['sma_50'] = ta.trend.sma_indicator(df['close'], window=50)
            
            # ì§€ìˆ˜ì´ë™í‰ê· 
            df['ema_12'] = ta.trend.ema_indicator(df['close'], window=12)
            df['ema_26'] = ta.trend.ema_indicator(df['close'], window=26)
            
            # MACD
            df['macd'] = ta.trend.macd_diff(df['close'])
            df['macd_signal'] = ta.trend.macd_signal(df['close'])
            df['macd_histogram'] = ta.trend.macd_diff(df['close'])
            
            # RSI
            df['rsi'] = ta.momentum.rsi(df['close'], window=14)
            
            # ë³¼ë¦°ì € ë°´ë“œ
            df['bb_upper'] = ta.volatility.bollinger_hband(df['close'])
            df['bb_lower'] = ta.volatility.bollinger_lband(df['close'])
            df['bb_middle'] = ta.volatility.bollinger_mavg(df['close'])
            
            # ìŠ¤í† ìºìŠ¤í‹±
            df['stoch_k'] = ta.momentum.stoch(df['high'], df['low'], df['close'])
            df['stoch_d'] = ta.momentum.stoch_signal(df['high'], df['low'], df['close'])
            
            # ATR (Average True Range)
            df['atr'] = ta.volatility.average_true_range(df['high'], df['low'], df['close'])
            
            # ê±°ë˜ëŸ‰ ì§€í‘œ
            df['volume_sma'] = ta.volume.volume_sma(df['close'], df['volume'])
            df['obv'] = ta.volume.on_balance_volume(df['close'], df['volume'])
            
            # ëª¨ë©˜í…€ ì§€í‘œ
            df['roc'] = ta.momentum.roc(df['close'])
            df['williams_r'] = ta.momentum.williams_r(df['high'], df['low'], df['close'])
            
            # ì¶”ì„¸ ì§€í‘œ
            df['adx'] = ta.trend.adx(df['high'], df['low'], df['close'])
            df['cci'] = ta.trend.cci(df['high'], df['low'], df['close'])
            
            # ë³€ë™ì„± ì§€í‘œ
            df['volatility'] = df['close'].pct_change().rolling(window=20).std()
            
            # ê°€ê²© ë³€í™”ìœ¨
            df['price_change'] = df['close'].pct_change()
            df['price_change_5'] = df['close'].pct_change(periods=5)
            df['price_change_20'] = df['close'].pct_change(periods=20)
            
            # ê±°ë˜ëŸ‰ ë³€í™”ìœ¨
            df['volume_change'] = df['volume'].pct_change()
            
            # NaN ê°’ ì²˜ë¦¬
            df = df.fillna(method='ffill').fillna(0)
            
            return df
            
        except Exception as e:
            self.logger.error(f"ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì‹¤íŒ¨: {e}")
            return df

class MLModelManager:
    """ML ëª¨ë¸ ê´€ë¦¬ì"""
    
    def __init__(self, models_path: str = "models"):
        self.models_path = Path(models_path)
        self.models_path.mkdir(exist_ok=True)
        
        self.logger = logging.getLogger(__name__)
        self.scaler = StandardScaler()
        self.models: Dict[str, Any] = {}
    
    def prepare_features(self, df: pd.DataFrame, config: MLConfig) -> Tuple[np.ndarray, np.ndarray]:
        """íŠ¹ì„± ì¤€ë¹„"""
        try:
            # íŠ¹ì„± ì„ íƒ
            feature_cols = [col for col in config.features if col in df.columns]
            if not feature_cols:
                raise ValueError("ìœ íš¨í•œ íŠ¹ì„±ì´ ì—†ìŠµë‹ˆë‹¤")
            
            # íŠ¹ì„± ë°ì´í„°
            X = df[feature_cols].values
            
            # íƒ€ê²Ÿ ë°ì´í„°
            if config.target in df.columns:
                y = df[config.target].values
            else:
                # ë‹¤ìŒ ë‚  ê°€ê²© ë³€í™”ìœ¨ì„ íƒ€ê²Ÿìœ¼ë¡œ ì„¤ì •
                y = df['close'].pct_change().shift(-1).values
                y = np.nan_to_num(y, 0)
            
            # NaN ì œê±°
            valid_indices = ~(np.isnan(X).any(axis=1) | np.isnan(y))
            X = X[valid_indices]
            y = y[valid_indices]
            
            # íŠ¹ì„± ìŠ¤ì¼€ì¼ë§
            X_scaled = self.scaler.fit_transform(X)
            
            return X_scaled, y
            
        except Exception as e:
            self.logger.error(f"íŠ¹ì„± ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            return np.array([]), np.array([])
    
    def train_model(self, symbol: str, config: MLConfig, df: pd.DataFrame) -> bool:
        """ëª¨ë¸ í•™ìŠµ"""
        try:
            X, y = self.prepare_features(df, config)
            
            if len(X) < 100:  # ìµœì†Œ ë°ì´í„° ìš”êµ¬ì‚¬í•­
                self.logger.warning(f"ë°ì´í„° ë¶€ì¡±: {symbol}")
                return False
            
            # ë°ì´í„° ë¶„í• 
            train_size = int(len(X) * config.train_size)
            val_size = int(len(X) * config.validation_size)
            
            X_train = X[:train_size]
            y_train = y[:train_size]
            X_val = X[train_size:train_size + val_size]
            y_val = y[train_size:train_size + val_size]
            X_test = X[train_size + val_size:]
            y_test = y[train_size + val_size:]
            
            # ëª¨ë¸ ì„ íƒ ë° í•™ìŠµ
            if config.model_type == 'regression':
                model = RandomForestRegressor(n_estimators=100, random_state=42)
            elif config.model_type == 'classification':
                from sklearn.ensemble import RandomForestClassifier
                model = RandomForestClassifier(n_estimators=100, random_state=42)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {config.model_type}")
            
            # ëª¨ë¸ í•™ìŠµ
            model.fit(X_train, y_train)
            
            # ì„±ëŠ¥ í‰ê°€
            train_score = model.score(X_train, y_train)
            val_score = model.score(X_val, y_val) if len(X_val) > 0 else 0
            test_score = model.score(X_test, y_test) if len(X_test) > 0 else 0
            
            # ëª¨ë¸ ì €ì¥
            model_path = self.models_path / f"{symbol}_{config.model_type}.pkl"
            with open(model_path, 'wb') as f:
                pickle.dump({
                    'model': model,
                    'scaler': self.scaler,
                    'config': config,
                    'features': config.features,
                    'scores': {
                        'train': train_score,
                        'validation': val_score,
                        'test': test_score
                    },
                    'trained_at': datetime.now()
                }, f)
            
            self.models[symbol] = model
            
            self.logger.info(f"ëª¨ë¸ í•™ìŠµ ì™„ë£Œ: {symbol}, ì •í™•ë„: {test_score:.4f}")
            return True
            
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ í•™ìŠµ ì‹¤íŒ¨: {symbol}, {e}")
            return False
    
    def predict(self, symbol: str, features: np.ndarray) -> Optional[float]:
        """ì˜ˆì¸¡ ìˆ˜í–‰"""
        try:
            if symbol not in self.models:
                # ëª¨ë¸ ë¡œë“œ
                model_path = self.models_path / f"{symbol}_regression.pkl"
                if not model_path.exists():
                    return None
                
                with open(model_path, 'rb') as f:
                    model_data = pickle.load(f)
                    self.models[symbol] = model_data['model']
                    self.scaler = model_data['scaler']
            
            # íŠ¹ì„± ìŠ¤ì¼€ì¼ë§
            features_scaled = self.scaler.transform(features.reshape(1, -1))
            
            # ì˜ˆì¸¡
            prediction = self.models[symbol].predict(features_scaled)[0]
            
            return prediction
            
        except Exception as e:
            self.logger.error(f"ì˜ˆì¸¡ ì‹¤íŒ¨: {symbol}, {e}")
            return None

class KRXSmartCollector:
    """ML/DL ìµœì í™”ëœ KRX ë°ì´í„° ìˆ˜ì§‘ê¸°"""
    
    def __init__(self):
        self.storage_manager = DataStorageManager()
        self.technical_calculator = TechnicalIndicatorCalculator()
        self.ml_manager = MLModelManager()
        
        self.logger = logging.getLogger(__name__)
        self.logger.setLevel(logging.INFO)
        
        # í•¸ë“¤ëŸ¬ ì„¤ì •
        handler = logging.StreamHandler()
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        self.logger.addHandler(handler)
    
    async def collect_and_process(
        self, 
        symbols: List[str], 
        data_type: DataType = 'historical'
    ) -> Dict[str, bool]:
        """ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬"""
        results = {}
        
        for symbol in symbols:
            try:
                self.logger.info(f"ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘: {symbol}")
                
                # ë°ì´í„° ìˆ˜ì§‘
                df = await self._collect_data(symbol, data_type)
                if df is None or df.empty:
                    results[symbol] = False
                    continue
                
                # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
                df_with_indicators = self.technical_calculator.calculate_all_indicators(df)
                
                # ë°ì´í„° ì €ì¥ (íƒ€ì…ë³„ ìµœì í™”)
                config = self._get_optimal_config(data_type)
                success = await self.storage_manager.store_data(df_with_indicators, symbol, config)
                
                # ML ëª¨ë¸ í•™ìŠµ (íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„°ì¸ ê²½ìš°)
                if data_type == 'historical' and success:
                    ml_config = self._get_ml_config(symbol)
                    self.ml_manager.train_model(symbol, ml_config, df_with_indicators)
                
                results[symbol] = success
                self.logger.info(f"ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {symbol}")
                
            except Exception as e:
                self.logger.error(f"ë°ì´í„° ì²˜ë¦¬ ì‹¤íŒ¨: {symbol}, {e}")
                results[symbol] = False
        
        return results
    
    async def _collect_data(self, symbol: str, data_type: DataType) -> Optional[pd.DataFrame]:
        """ë°ì´í„° ìˆ˜ì§‘"""
        try:
            if data_type == 'historical':
                # ìµœëŒ€ íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„°
                end_date = datetime.now()
                start_date = end_date - timedelta(days=HISTORICAL_DAYS)
                
                df = stock.get_market_ohlcv_by_date(
                    start_date.strftime('%Y%m%d'),
                    end_date.strftime('%Y%m%d'),
                    symbol
                )
                
                if df is not None and not df.empty:
                    # ì»¬ëŸ¼ëª… í‘œì¤€í™”
                    df.columns = [col.lower() for col in df.columns]
                    return df
                
            elif data_type == 'realtime':
                # ì‹¤ì‹œê°„ ë°ì´í„° (ìµœê·¼ 1ì¼)
                end_date = datetime.now()
                start_date = end_date - timedelta(days=1)
                
                df = stock.get_market_ohlcv_by_date(
                    start_date.strftime('%Y%m%d'),
                    end_date.strftime('%Y%m%d'),
                    symbol
                )
                
                if df is not None and not df.empty:
                    df.columns = [col.lower() for col in df.columns]
                    return df
            
            return None
            
        except Exception as e:
            self.logger.error(f"ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {symbol}, {e}")
            return None
    
    def _get_optimal_config(self, data_type: DataType) -> DataConfig:
        """ìµœì  ì„¤ì • ë°˜í™˜"""
        if data_type == 'realtime':
            return DataConfig(
                data_type='realtime',
                storage_type='redis',
                cache_ttl=60
            )
        elif data_type == 'historical':
            return DataConfig(
                data_type='historical',
                storage_type='parquet',
                compression=True,
                index=True
            )
        elif data_type == 'technical':
            return DataConfig(
                data_type='technical',
                storage_type='memory',
                cache_ttl=300
            )
        else:
            return DataConfig(
                data_type='historical',
                storage_type='parquet'
            )
    
    def _get_ml_config(self, symbol: str) -> MLConfig:
        """ML ì„¤ì • ë°˜í™˜"""
        return MLConfig(
            model_type='regression',
            features=[
                'sma_5', 'sma_20', 'sma_50', 'ema_12', 'ema_26',
                'macd', 'rsi', 'bb_upper', 'bb_lower', 'atr',
                'volume_sma', 'roc', 'adx', 'volatility',
                'price_change', 'price_change_5', 'price_change_20'
            ],
            target='price_change',
            train_size=0.7,
            validation_size=0.15,
            test_size=0.15
        )
    
    async def get_trading_signal(self, symbol: str) -> Dict[str, Any]:
        """ìë™ë§¤ë§¤ ì‹ í˜¸ ìƒì„±"""
        try:
            # ìµœì‹  ë°ì´í„° ë¡œë“œ
            config = DataConfig(data_type='realtime', storage_type='redis')
            df = await self.storage_manager.load_data(symbol, config)
            
            if df is None or df.empty:
                return {'signal': 'no_data', 'confidence': 0.0}
            
            # ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚°
            df_with_indicators = self.technical_calculator.calculate_all_indicators(df)
            
            # ìµœì‹  ë°ì´í„° ì¶”ì¶œ
            latest_data = df_with_indicators.iloc[-1]
            
            # ML ì˜ˆì¸¡
            features = latest_data[self._get_ml_config(symbol).features].values
            prediction = self.ml_manager.predict(symbol, features)
            
            # ì‹ í˜¸ ìƒì„±
            signal = self._generate_signal(latest_data, prediction)
            
            return {
                'symbol': symbol,
                'signal': signal['action'],
                'confidence': signal['confidence'],
                'prediction': prediction,
                'timestamp': datetime.now(),
                'indicators': {
                    'rsi': latest_data.get('rsi', 0),
                    'macd': latest_data.get('macd', 0),
                    'sma_20': latest_data.get('sma_20', 0),
                    'volume': latest_data.get('volume', 0)
                }
            }
            
        except Exception as e:
            self.logger.error(f"ì‹ í˜¸ ìƒì„± ì‹¤íŒ¨: {symbol}, {e}")
            return {'signal': 'error', 'confidence': 0.0}
    
    def _generate_signal(self, data: pd.Series, prediction: Optional[float]) -> Dict[str, Any]:
        """ë§¤ë§¤ ì‹ í˜¸ ìƒì„±"""
        try:
            signals = []
            confidence = 0.0
            
            # RSI ì‹ í˜¸
            rsi = data.get('rsi', 50)
            if rsi < 30:
                signals.append(('buy', 0.3))
            elif rsi > 70:
                signals.append(('sell', 0.3))
            
            # MACD ì‹ í˜¸
            macd = data.get('macd', 0)
            if macd > 0:
                signals.append(('buy', 0.2))
            else:
                signals.append(('sell', 0.2))
            
            # ì´ë™í‰ê·  ì‹ í˜¸
            close = data.get('close', 0)
            sma_20 = data.get('sma_20', 0)
            if close > sma_20:
                signals.append(('buy', 0.2))
            else:
                signals.append(('sell', 0.2))
            
            # ML ì˜ˆì¸¡ ì‹ í˜¸
            if prediction is not None:
                if prediction > 0.01:  # 1% ì´ìƒ ìƒìŠ¹ ì˜ˆìƒ
                    signals.append(('buy', 0.3))
                elif prediction < -0.01:  # 1% ì´ìƒ í•˜ë½ ì˜ˆìƒ
                    signals.append(('sell', 0.3))
            
            # ì‹ í˜¸ ì§‘ê³„
            buy_signals = [conf for action, conf in signals if action == 'buy']
            sell_signals = [conf for action, conf in signals if action == 'sell']
            
            if buy_signals and max(buy_signals) > max(sell_signals):
                action = 'buy'
                confidence = max(buy_signals)
            elif sell_signals and max(sell_signals) > max(buy_signals):
                action = 'sell'
                confidence = max(sell_signals)
            else:
                action = 'hold'
                confidence = 0.5
            
            return {'action': action, 'confidence': confidence}
            
        except Exception as e:
            self.logger.error(f"ì‹ í˜¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return {'action': 'hold', 'confidence': 0.0}

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    # ë¡œê¹… ì„¤ì •
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )
    
    collector = KRXSmartCollector()
    
    # ì£¼ìš” ì§€ìˆ˜ ë° ETF
    symbols = [
        '005930',  # ì‚¼ì„±ì „ì
        '000660',  # SKí•˜ì´ë‹‰ìŠ¤
        '035420',  # NAVER
        '051910',  # LGí™”í•™
        '006400',  # ì‚¼ì„±SDI
        '035720',  # ì¹´ì¹´ì˜¤
        '207940',  # ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤
        '068270',  # ì…€íŠ¸ë¦¬ì˜¨
        '323410',  # ì¹´ì¹´ì˜¤ë±…í¬
        '051900',  # LGìƒí™œê±´ê°•
    ]
    
    print("ğŸš€ ML/DL ìµœì í™”ëœ KRX ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ ì‹œì‘")
    print("=" * 60)
    
    # íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ìˆ˜ì§‘
    print("ğŸ“Š íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    results = await collector.collect_and_process(symbols, 'historical')
    
    success_count = sum(1 for success in results.values() if success)
    print(f"âœ… íˆìŠ¤í† ë¦¬ì»¬ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {success_count}/{len(symbols)}")
    
    # ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘
    print("âš¡ ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ ì¤‘...")
    realtime_results = await collector.collect_and_process(symbols[:5], 'realtime')
    
    realtime_success = sum(1 for success in realtime_results.values() if success)
    print(f"âœ… ì‹¤ì‹œê°„ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ: {realtime_success}/{len(symbols[:5])}")
    
    # ìë™ë§¤ë§¤ ì‹ í˜¸ í…ŒìŠ¤íŠ¸
    print("ğŸ¤– ìë™ë§¤ë§¤ ì‹ í˜¸ ìƒì„± í…ŒìŠ¤íŠ¸...")
    for symbol in symbols[:3]:
        signal = await collector.get_trading_signal(symbol)
        print(f"ğŸ“ˆ {symbol}: {signal['signal']} (ì‹ ë¢°ë„: {signal['confidence']:.2f})")
    
    print("=" * 60)
    print("ğŸ‰ ML/DL ìµœì í™”ëœ ë°ì´í„° ìˆ˜ì§‘ ì‹œìŠ¤í…œ ì™„ë£Œ!")

if __name__ == "__main__":
    asyncio.run(main()) 