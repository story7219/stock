#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íŒŒì¼ëª…: krx_ultimate_system.py
ëª©ì : ìµœì‹  Python í‘œì¤€ì„ í™œìš©í•œ ê¶ê·¹ì  KRX íŠ¸ë ˆì´ë”© ì‹œìŠ¤í…œ - ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§
Author: Ultimate KRX System
Created: 2025-07-13
Version: 2.0.0

Features:
    - ìµœì‹  Python 3.11+ í‘œì¤€ í™œìš©
    - ë¹„ë™ê¸° ê³ ì† ë³‘ë ¬ì²˜ë¦¬
    - ë©€í‹°ë ˆë²¨ ìºì‹±
    - ì»¤ë„¥ì…˜ í’€ë§
    - ë©”ëª¨ë¦¬ ìµœì í™”
    - ìë™ ì½”ë“œìˆ˜ì • íŒŒì¼ ë³´ì¡´
    - ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë° ì§€ì†ì  ë°ì´í„° ìˆ˜ì§‘
    - ì‹œì¥ ë³€í™” ì‹¤ì‹œê°„ ê°ì§€
"""

import asyncio
import aiohttp
import pandas as pd
import numpy as np
import logging
import json
import time
import hashlib
import pickle
from pathlib import Path
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Union, Literal, TypedDict, Protocol, Tuple
from dataclasses import dataclass, field
from enum import Enum, auto
from functools import lru_cache, wraps
from contextlib import asynccontextmanager
import weakref
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing as mp
from collections import defaultdict, deque
import gc
import psutil
import tracemalloc
import os

# ìµœì‹  Python í‘œì¤€ í™œìš©
from typing_extensions import NotRequired, Required
from pydantic import BaseModel, Field, field_validator

# structlog ëŒ€ì‹  ê¸°ë³¸ logging ì‚¬ìš© (í˜¸í™˜ì„± ë¬¸ì œ í•´ê²°)
import logging

# ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
tracemalloc.start()

# KRX ì„±ëŠ¥ í‰ê°€ ê¸°ì¤€ (ë™ì¼í•œ ê¸°ì¤€ ì ìš©)
KRX_PERFORMANCE_CRITERIA = {
    "excellent": {"min_r2": 0.8, "max_rmse": 0.1, "min_excellent_folds": 3},
    "good": {"min_r2": 0.6, "max_rmse": 0.2, "min_excellent_folds": 2},
    "fair": {"min_r2": 0.4, "max_rmse": 0.3, "min_excellent_folds": 1},
    "poor": {"min_r2": 0.0, "max_rmse": float('inf'), "min_excellent_folds": 0}
}

# KRX ìë™ë§¤ë§¤ ê°€ëŠ¥ì„± íŒë‹¨ ê¸°ì¤€
KRX_TRADING_CRITERIA = {
    "high_confidence": {
        "min_r2": 0.85,
        "max_rmse": 0.08,
        "min_excellent_folds": 4,
        "max_poor_folds": 0,
        "min_data_quality": 0.9
    },
    "medium_confidence": {
        "min_r2": 0.7,
        "max_rmse": 0.15,
        "min_excellent_folds": 3,
        "max_poor_folds": 1,
        "min_data_quality": 0.8
    },
    "low_confidence": {
        "min_r2": 0.5,
        "max_rmse": 0.25,
        "min_excellent_folds": 2,
        "max_poor_folds": 2,
        "min_data_quality": 0.7
    },
    "not_tradeable": {
        "min_r2": 0.0,
        "max_rmse": float('inf'),
        "min_excellent_folds": 0,
        "max_poor_folds": 5,
        "min_data_quality": 0.0
    }
}

# KRX ë°ì´í„° ì„±ê²©ë³„ ì €ì¥ ì „ëµ
KRX_STORAGE_STRATEGIES = {
    "high_frequency_trading": {
        "storage_format": "parquet",
        "compression": "snappy",
        "partition_by": ["ë‚ ì§œ", "ì¢…ëª©ì½”ë“œ"],
        "retention_days": 30,
        "backup_frequency": "daily",
        "description": "ê³ ë¹ˆë„ ê±°ë˜ - ë¹ ë¥¸ ì½ê¸°/ì“°ê¸°, ì••ì¶• ìµœì í™”"
    },
    "medium_frequency_analysis": {
        "storage_format": "parquet",
        "compression": "gzip",
        "partition_by": ["ì›”", "ì¢…ëª©ì½”ë“œ"],
        "retention_days": 90,
        "backup_frequency": "weekly",
        "description": "ì¤‘ë¹ˆë„ ë¶„ì„ - ê· í˜•ì¡íŒ ì„±ëŠ¥ê³¼ ìš©ëŸ‰"
    },
    "long_term_research": {
        "storage_format": "parquet",
        "compression": "brotli",
        "partition_by": ["ë…„", "ì¢…ëª©ì½”ë“œ"],
        "retention_days": 365,
        "backup_frequency": "monthly",
        "description": "ì¥ê¸° ì—°êµ¬ - ìµœëŒ€ ì••ì¶•, ì¥ê¸° ë³´ê´€"
    },
    "real_time_monitoring": {
        "storage_format": "parquet",
        "compression": "snappy",
        "partition_by": ["ì‹œê°„", "ì¢…ëª©ì½”ë“œ"],
        "retention_days": 7,
        "backup_frequency": "hourly",
        "description": "ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ - ìµœì†Œ ì§€ì—°, ë¹ ë¥¸ ì²˜ë¦¬"
    }
}

def evaluate_krx_performance(analysis: Dict[str, Any]) -> Dict[str, Any]:
    """KRX ì„±ëŠ¥ í‰ê°€ (ë™ì¼í•œ ê¸°ì¤€ ì ìš©)"""
    avg_r2 = analysis.get('avg_r2', 0)
    avg_rmse = analysis.get('avg_rmse', float('inf'))
    excellent_folds = analysis.get('excellent_folds', 0)
    poor_folds = analysis.get('poor_folds', 0)
    
    # ì„±ëŠ¥ ë“±ê¸‰ í‰ê°€
    performance_grade = "ğŸ”´ Poor"
    if avg_r2 >= KRX_PERFORMANCE_CRITERIA["excellent"]["min_r2"] and excellent_folds >= KRX_PERFORMANCE_CRITERIA["excellent"]["min_excellent_folds"]:
        performance_grade = "ğŸŸ¢ Excellent"
    elif avg_r2 >= KRX_PERFORMANCE_CRITERIA["good"]["min_r2"] and excellent_folds >= KRX_PERFORMANCE_CRITERIA["good"]["min_excellent_folds"]:
        performance_grade = "ğŸŸ¡ Good"
    elif avg_r2 >= KRX_PERFORMANCE_CRITERIA["fair"]["min_r2"] and excellent_folds >= KRX_PERFORMANCE_CRITERIA["fair"]["min_excellent_folds"]:
        performance_grade = "ğŸŸ  Fair"
    
    # ìë™ë§¤ë§¤ ê°€ëŠ¥ì„± íŒë‹¨
    trading_confidence = "not_tradeable"
    data_quality = 1.0 - (poor_folds / (excellent_folds + poor_folds + 1))
    
    for confidence, criteria in KRX_TRADING_CRITERIA.items():
        if (avg_r2 >= criteria["min_r2"] and 
            avg_rmse <= criteria["max_rmse"] and
            excellent_folds >= criteria["min_excellent_folds"] and
            poor_folds <= criteria["max_poor_folds"] and
            data_quality >= criteria["min_data_quality"]):
            trading_confidence = confidence
            break
    
    return {
        "performance_grade": performance_grade,
        "trading_confidence": trading_confidence,
        "data_quality_score": data_quality,
        "improvement_needed": performance_grade.startswith("ğŸ”´") or trading_confidence == "not_tradeable",
        "trading_recommendation": _get_trading_recommendation(trading_confidence)
    }

def _get_trading_recommendation(confidence: str) -> str:
    """ìë™ë§¤ë§¤ ê¶Œì¥ì‚¬í•­"""
    recommendations = {
        "high_confidence": "âœ… ìë™ë§¤ë§¤ ê¶Œì¥ - ë†’ì€ ì‹ ë¢°ë„",
        "medium_confidence": "âš ï¸ ì œí•œì  ìë™ë§¤ë§¤ - ì¤‘ê°„ ì‹ ë¢°ë„",
        "low_confidence": "âŒ ìë™ë§¤ë§¤ ë¹„ê¶Œì¥ - ë‚®ì€ ì‹ ë¢°ë„",
        "not_tradeable": "ğŸš« ìë™ë§¤ë§¤ ë¶ˆê°€ - ê°œì„  í•„ìš”"
    }
    return recommendations.get(confidence, "â“ í‰ê°€ ë¶ˆê°€")

def detect_krx_data_characteristics(df: pd.DataFrame) -> str:
    """KRX ë°ì´í„° ì„±ê²© ê°ì§€"""
    # ë°ì´í„° í¬ê¸° ë° ë¹ˆë„ ë¶„ì„
    data_size = len(df)
    time_columns = [col for col in df.columns if 'ì‹œê°„' in col or 'ì¼ì' in col or 'ë‚ ì§œ' in col]
    
    # ê±°ë˜ëŸ‰ íŒ¨í„´ ë¶„ì„
    volume_columns = [col for col in df.columns if 'ê±°ë˜ëŸ‰' in col or 'ê±°ë˜ëŒ€ê¸ˆ' in col]
    has_volume_data = len(volume_columns) > 0
    
    # ê°€ê²© ë³€ë™ì„± ë¶„ì„
    price_columns = [col for col in df.columns if 'ê°€' in col and 'ê°€ê²©' not in col]
    has_price_data = len(price_columns) > 0
    
    # ë°ì´í„° ì„±ê²© íŒë‹¨
    if data_size > 100000 and has_volume_data and has_price_data:
        return "high_frequency_trading"
    elif data_size > 10000 and has_price_data:
        return "medium_frequency_analysis"
    elif data_size > 1000:
        return "long_term_research"
    else:
        return "real_time_monitoring"

def get_krx_storage_strategy(df: pd.DataFrame, trading_confidence: str) -> Dict[str, Any]:
    """KRX ë°ì´í„° ì €ì¥ ì „ëµ ê²°ì •"""
    data_characteristics = detect_krx_data_characteristics(df)
    base_strategy = KRX_STORAGE_STRATEGIES[data_characteristics].copy()
    
    # ìë™ë§¤ë§¤ ì‹ ë¢°ë„ì— ë”°ë¥¸ ì €ì¥ ì „ëµ ì¡°ì •
    if trading_confidence == "high_confidence":
        base_strategy["backup_frequency"] = "hourly"
        base_strategy["retention_days"] = 60
        base_strategy["description"] += " (ìë™ë§¤ë§¤ í™œì„±í™”)"
    elif trading_confidence == "medium_confidence":
        base_strategy["backup_frequency"] = "daily"
        base_strategy["retention_days"] = 45
        base_strategy["description"] += " (ì œí•œì  ìë™ë§¤ë§¤)"
    elif trading_confidence == "low_confidence":
        base_strategy["backup_frequency"] = "weekly"
        base_strategy["retention_days"] = 30
        base_strategy["description"] += " (ì—°êµ¬ìš©)"
    else:
        base_strategy["backup_frequency"] = "monthly"
        base_strategy["retention_days"] = 15
        base_strategy["description"] += " (ê°œì„  í•„ìš”)"
    
    return base_strategy

def detect_krx_data_type(df: pd.DataFrame) -> str:
    """KRX ë°ì´í„° ìœ í˜• ìë™ ê°ì§€"""
    # KRX ë°ì´í„° íŠ¹ì„± í™•ì¸
    krx_indicators = [
        'ì¢…ëª©ì½”ë“œ', 'ì¢…ëª©ëª…', 'í˜„ì¬ê°€', 'ë“±ë½ë¥ ', 'ê±°ë˜ëŸ‰',
        'ì‹œê°€ì´ì•¡', 'ìƒì¥ì£¼ì‹ìˆ˜', 'ì™¸êµ­ì¸ë¹„ìœ¨'
    ]
    
    has_krx_cols = any(col in df.columns for col in krx_indicators)
    has_time_cols = any('ì¼ì' in col or 'ë‚ ì§œ' in col for col in df.columns)
    has_price_cols = any('ê°€' in col for col in df.columns)
    
    # ë°ì´í„° í¬ê¸° í™•ì¸
    data_size = len(df)
    feature_count = len(df.select_dtypes(include=[np.number]).columns)
    
    # ë°ì´í„° ìœ í˜• íŒë‹¨
    if has_krx_cols and has_time_cols and has_price_cols:
        return "financial_timeseries"
    elif data_size > 10000 and feature_count > 20:
        return "image_text"
    elif data_size < 5000 or feature_count < 10:
        return "experimental"
    else:
        return "general_ml"

def get_krx_optimized_config(df: pd.DataFrame) -> Dict[str, Any]:
    """KRX ë°ì´í„° ìœ í˜•ì— ë”°ë¥¸ ìµœì  ì„¤ì • ë°˜í™˜"""
    data_type = detect_krx_data_type(df)
    config = KRX_DATA_TYPE_CONFIGS[data_type].copy()
    
    logging.info(f"KRX ë°ì´í„° ìœ í˜• ê°ì§€: {data_type}")
    logging.info(f"ì„¤ì • ì ìš©: {config['description']}")
    logging.info(f"ìµœëŒ€ ë°˜ë³µ: {config['max_iterations']}íšŒ")
    logging.info(f"ì¡°ê¸° ì¢…ë£Œ: ì—°ì† {config['max_no_improvement']}íšŒ ê°œì„  ì—†ìŒ")
    
    return config

# ì „ì—­ ë³€ìˆ˜ (ìš°ìˆ˜ ë“±ê¸‰ ë‹¬ì„± ì¶”ì )
achieved_excellent_grade = False

def np_encoder(obj):
    """numpy íƒ€ì… JSON ì§ë ¬í™”ìš©"""
    if isinstance(obj, (np.integer, np.floating)):
        return obj.item()
    if isinstance(obj, np.bool_):
        return bool(obj)
    return str(obj)

class SystemMode(Enum):
    """ì‹œìŠ¤í…œ ëª¨ë“œ"""
    TRAIN = auto()
    LIVE = auto()
    BACKTEST = auto()
    EMERGENCY = auto()
    REALTIME_MONITORING = auto()  # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ëª¨ë“œ ì¶”ê°€

class DataType(Enum):
    """ë°ì´í„° íƒ€ì…"""
    STOCK = auto()
    FUTURES = auto()
    OPTIONS = auto()
    INDEX = auto()
    ETF = auto()

class Priority(Enum):
    """ìš°ì„ ìˆœìœ„"""
    LOW = auto()
    NORMAL = auto()
    HIGH = auto()
    EMERGENCY = auto()

class MarketEventType(Enum):
    """ì‹œì¥ ì´ë²¤íŠ¸ íƒ€ì…"""
    PRICE_SPIKE = auto()
    VOLUME_SURGE = auto()
    VOLATILITY_INCREASE = auto()
    TREND_CHANGE = auto()
    BREAKOUT = auto()
    BREAKDOWN = auto()

@dataclass
class SystemConfig:
    """ì‹œìŠ¤í…œ ì„¤ì • - ìµœì‹  dataclass í™œìš©"""
    mode: SystemMode = SystemMode.REALTIME_MONITORING  # ê¸°ë³¸ê°’ì„ ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ìœ¼ë¡œ ë³€ê²½
    max_workers: int = field(default_factory=lambda: min(32, mp.cpu_count() + 4))
    cache_size: int = 1000
    connection_pool_size: int = 20
    timeout: float = 30.0
    retry_attempts: int = 3
    memory_limit_gb: float = 8.0
    
    # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì„¤ì • ì¶”ê°€
    monitoring_interval_seconds: int = 60  # 1ë¶„ë§ˆë‹¤ ìˆ˜ì§‘
    market_hours_start: str = "09:00"
    market_hours_end: str = "15:30"
    weekend_monitoring: bool = False
    emergency_collection_interval: int = 10  # ê¸´ê¸‰ ìƒí™© ì‹œ 10ì´ˆë§ˆë‹¤
    price_change_threshold: float = 0.02  # 2% ì´ìƒ ë³€ë™ ì‹œ ì´ë²¤íŠ¸
    volume_change_threshold: float = 3.0  # ê±°ë˜ëŸ‰ 3ë°° ì´ìƒ ì‹œ ì´ë²¤íŠ¸
    
    def __post_init__(self):
        """ì„¤ì • ê²€ì¦"""
        try:
            if self.memory_limit_gb > psutil.virtual_memory().total / (1024**3):
                self.memory_limit_gb = psutil.virtual_memory().total / (1024**3) * 0.8
        except Exception:
            # psutil ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
            self.memory_limit_gb = 8.0

class CacheConfig(TypedDict):
    """ìºì‹œ ì„¤ì • - TypedDict í™œìš©"""
    memory_cache_size: int
    disk_cache_size: int
    ttl_seconds: int
    compression: bool

class PerformanceMetrics(BaseModel):
    """ì„±ëŠ¥ ë©”íŠ¸ë¦­ - Pydantic í™œìš©"""
    total_requests: int = 0
    successful_requests: int = 0
    failed_requests: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    avg_response_time: float = 0.0
    memory_usage_mb: float = 0.0
    cpu_usage_percent: float = 0.0
    
    # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ë©”íŠ¸ë¦­ ì¶”ê°€
    total_collections: int = 0
    market_events_detected: int = 0
    last_collection_time: Optional[str] = None
    next_collection_time: Optional[str] = None
    
    @field_validator('avg_response_time')
    @classmethod
    def validate_response_time(cls, v):
        return max(0.0, v)

@dataclass
class MarketData:
    """ì‹œì¥ ë°ì´í„°"""
    timestamp: datetime
    symbol: str
    price: float
    volume: int
    change_percent: float
    market_cap: Optional[float] = None
    
    def to_dict(self) -> Dict[str, Any]:
        return {
            'timestamp': self.timestamp.isoformat(),
            'symbol': self.symbol,
            'price': self.price,
            'volume': self.volume,
            'change_percent': self.change_percent,
            'market_cap': self.market_cap
        }

@dataclass
class MarketEvent:
    """ì‹œì¥ ì´ë²¤íŠ¸"""
    event_type: MarketEventType
    symbol: str
    timestamp: datetime
    description: str
    severity: Priority
    data: Dict[str, Any]

class KRXUltimateSystem:
    """ê¶ê·¹ì  KRX ì‹œìŠ¤í…œ - ìµœì‹  Python í‘œì¤€ í™œìš© + ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§"""
    
    def __init__(self, config: Optional[SystemConfig] = None):
        self.config = config or SystemConfig()
        
        # ê¸°ë³¸ ë¡œê¹… ì„¤ì • (structlog ëŒ€ì‹ )
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
        
        # ë©€í‹°ë ˆë²¨ ìºì‹± ì‹œìŠ¤í…œ
        self._setup_caching()
        
        # ì»¤ë„¥ì…˜ í’€ ì„¤ì •
        self._setup_connection_pool()
        
        # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§
        self.metrics = PerformanceMetrics()
        
        # ë©”ëª¨ë¦¬ ê´€ë¦¬
        self._setup_memory_management()
        
        # ìë™ ì½”ë“œìˆ˜ì • íŒŒì¼ ë³´ì¡´
        self._preserve_auto_fix_files()
        
        # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì„¤ì •
        self._setup_realtime_monitoring()
    
    def _setup_caching(self):
        """ë©€í‹°ë ˆë²¨ ìºì‹± ì‹œìŠ¤í…œ ì„¤ì •"""
        # L1: ë©”ëª¨ë¦¬ ìºì‹œ (LRU)
        self.memory_cache = {}
        self.cache_hits = 0
        self.cache_misses = 0
        
        # L2: ë””ìŠ¤í¬ ìºì‹œ
        self.cache_dir = Path('cache/ultimate')
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        
        # L3: ë¶„ì‚° ìºì‹œ (Redis ëŒ€ì²´)
        self.distributed_cache = {}
    
    def _setup_connection_pool(self):
        """ì»¤ë„¥ì…˜ í’€ ì„¤ì •"""
        self.connector = aiohttp.TCPConnector(
            limit=self.config.connection_pool_size,
            limit_per_host=10,
            ttl_dns_cache=300,
            use_dns_cache=True,
            keepalive_timeout=30
        )
        self.session = None
    
    def _setup_memory_management(self):
        """ë©”ëª¨ë¦¬ ê´€ë¦¬ ì„¤ì •"""
        try:
            self.memory_monitor = MemoryMonitor(self.config.memory_limit_gb)
        except Exception:
            # psutil ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ê²½ìš° ë”ë¯¸ ëª¨ë‹ˆí„° ì‚¬ìš©
            self.memory_monitor = DummyMemoryMonitor()
        self.gc_threshold = 0.8  # 80% ë©”ëª¨ë¦¬ ì‚¬ìš© ì‹œ GC
    
    def _preserve_auto_fix_files(self):
        """ìë™ ì½”ë“œìˆ˜ì • íŒŒì¼ ë³´ì¡´"""
        auto_fix_files = [
            'smart_duplicate_cleaner.py',
            'ultimate_folder_consolidator.py'
        ]
        
        for file_name in auto_fix_files:
            file_path = Path(file_name)
            if file_path.exists():
                # ë°±ì—… ìƒì„±
                backup_path = Path(f'backup/auto_fix/{file_name}')
                backup_path.parent.mkdir(parents=True, exist_ok=True)
                
                if not backup_path.exists():
                    import shutil
                    shutil.copy2(file_path, backup_path)
                    print(f"ìë™ ì½”ë“œìˆ˜ì • íŒŒì¼ ë³´ì¡´: {file_name}")
    
    def _setup_realtime_monitoring(self):
        """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì„¤ì •"""
        self.is_monitoring = False
        self.market_data_history = deque(maxlen=1000)  # ìµœê·¼ 1000ê°œ ë°ì´í„°
        self.detected_events = deque(maxlen=100)  # ìµœê·¼ 100ê°œ ì´ë²¤íŠ¸
        self.last_market_data = {}
        self.emergency_mode = False
        
        # ì‹œì¥ ì‹œê°„ ì„¤ì •
        self.market_start = datetime.strptime(self.config.market_hours_start, "%H:%M").time()
        self.market_end = datetime.strptime(self.config.market_hours_end, "%H:%M").time()
    
    @asynccontextmanager
    async def get_session(self):
        """ë¹„ë™ê¸° ì„¸ì…˜ ê´€ë¦¬"""
        if self.session is None:
            self.session = aiohttp.ClientSession(
                connector=self.connector,
                timeout=aiohttp.ClientTimeout(total=self.config.timeout)
            )
        
        try:
            yield self.session
        except Exception as e:
            print(f"ì„¸ì…˜ ì—ëŸ¬: {e}")
            if self.session:
                await self.session.close()
                self.session = None
            raise
    
    @lru_cache(maxsize=1000)
    def _get_cache_key(self, data_type: str, market: str, date: str) -> str:
        """ìºì‹œ í‚¤ ìƒì„±"""
        return hashlib.md5(f"{data_type}_{market}_{date}".encode()).hexdigest()
    
    async def _multi_level_cache_get(self, key: str) -> Optional[Dict[str, Any]]:
        """ë©€í‹°ë ˆë²¨ ìºì‹œì—ì„œ ë°ì´í„° ì¡°íšŒ"""
        # L1: ë©”ëª¨ë¦¬ ìºì‹œ
        if key in self.memory_cache:
            self.cache_hits += 1
            return self.memory_cache[key]
        
        # L2: ë””ìŠ¤í¬ ìºì‹œ
        cache_file = self.cache_dir / f"{key}.pkl"
        if cache_file.exists():
            try:
                with open(cache_file, 'rb') as f:
                    data = pickle.load(f)
                    self.memory_cache[key] = data  # L1ì— ë¡œë“œ
                    self.cache_hits += 1
                    return data
            except Exception as e:
                print(f"ë””ìŠ¤í¬ ìºì‹œ ë¡œë“œ ì‹¤íŒ¨: {e}")
        
        # L3: ë¶„ì‚° ìºì‹œ
        if key in self.distributed_cache:
            self.cache_hits += 1
            return self.distributed_cache[key]
        
        self.cache_misses += 1
        return None
    
    async def _multi_level_cache_set(self, key: str, data: Dict[str, Any]):
        """ë©€í‹°ë ˆë²¨ ìºì‹œì— ë°ì´í„° ì €ì¥"""
        # L1: ë©”ëª¨ë¦¬ ìºì‹œ
        self.memory_cache[key] = data
        
        # L2: ë””ìŠ¤í¬ ìºì‹œ (ë¹„ë™ê¸°ë¡œ ì €ì¥)
        asyncio.create_task(self._save_to_disk_cache(key, data))
        
        # L3: ë¶„ì‚° ìºì‹œ
        self.distributed_cache[key] = data
    
    async def _save_to_disk_cache(self, key: str, data: Dict[str, Any]):
        """ë””ìŠ¤í¬ ìºì‹œì— ì €ì¥"""
        try:
            cache_file = self.cache_dir / f"{key}.pkl"
            with open(cache_file, 'wb') as f:
                pickle.dump(data, f)
        except Exception as e:
            print(f"ë””ìŠ¤í¬ ìºì‹œ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def start_realtime_monitoring(self):
        """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘"""
        if self.is_monitoring:
            print("ì´ë¯¸ ëª¨ë‹ˆí„°ë§ ì¤‘ì…ë‹ˆë‹¤.")
            return
        
        self.is_monitoring = True
        print("ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘")
        
        try:
            # ëª¨ë‹ˆí„°ë§ íƒœìŠ¤í¬ë“¤ ì‹œì‘
            monitoring_tasks = [
                asyncio.create_task(self._continuous_data_collection()),
                asyncio.create_task(self._market_event_detection()),
                asyncio.create_task(self._performance_monitoring()),
                asyncio.create_task(self._emergency_monitoring())
            ]
            
            # ëª¨ë“  íƒœìŠ¤í¬ ì™„ë£Œ ëŒ€ê¸° (ë¬´í•œ ë£¨í”„ ë°©ì§€)
            await asyncio.gather(*monitoring_tasks, return_exceptions=True)
            
        except Exception as e:
            print(f"ëª¨ë‹ˆí„°ë§ íƒœìŠ¤í¬ ì—ëŸ¬: {e}")
        finally:
            self.is_monitoring = False
    
    async def stop_realtime_monitoring(self):
        """ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€"""
        self.is_monitoring = False
        print("ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì¤‘ì§€")
    
    async def _continuous_data_collection(self):
        """ì§€ì†ì  ë°ì´í„° ìˆ˜ì§‘"""
        collection_count = 0
        max_collections = 10  # ìµœëŒ€ 10íšŒ ìˆ˜ì§‘ í›„ ì¢…ë£Œ (í…ŒìŠ¤íŠ¸ìš©)
        
        while self.is_monitoring and collection_count < max_collections:
            try:
                # ì‹œì¥ ì‹œê°„ ì²´í¬
                if not self._is_market_hours():
                    print("ì‹œì¥ ì‹œê°„ì´ ì•„ë‹™ë‹ˆë‹¤. 5ë¶„ ëŒ€ê¸°...")
                    await asyncio.sleep(300)  # 5ë¶„ ëŒ€ê¸°
                    continue
                
                # ë°ì´í„° ìˆ˜ì§‘
                start_time = time.time()
                data = await self.collect_data_parallel([DataType.STOCK, DataType.INDEX])
                
                # ìˆ˜ì§‘ ì‹œê°„ ê¸°ë¡
                self.metrics.total_collections += 1
                self.metrics.last_collection_time = datetime.now().isoformat()
                self.metrics.next_collection_time = (
                    datetime.now() + timedelta(seconds=self.config.monitoring_interval_seconds)
                ).isoformat()
                
                # ì‹œì¥ ë°ì´í„° íˆìŠ¤í† ë¦¬ì— ì €ì¥
                await self._process_market_data(data)
                
                execution_time = time.time() - start_time
                self._update_performance_metrics(execution_time)
                
                print(f"ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ (ì†Œìš”ì‹œê°„: {execution_time:.2f}ì´ˆ, ìˆ˜ì§‘íšŸìˆ˜: {collection_count + 1})")
                
                collection_count += 1
                
                # ë‹¤ìŒ ìˆ˜ì§‘ê¹Œì§€ ëŒ€ê¸°
                await asyncio.sleep(self.config.monitoring_interval_seconds)
                
            except Exception as e:
                print(f"ì§€ì†ì  ë°ì´í„° ìˆ˜ì§‘ ì—ëŸ¬: {e}")
                await asyncio.sleep(10)  # ì—ëŸ¬ ì‹œ 10ì´ˆ ëŒ€ê¸°
        
        print(f"ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ (ì´ {collection_count}íšŒ)")
    
    async def _market_event_detection(self):
        """ì‹œì¥ ì´ë²¤íŠ¸ ê°ì§€"""
        detection_count = 0
        max_detections = 20  # ìµœëŒ€ 20íšŒ ê°ì§€ í›„ ì¢…ë£Œ
        
        while self.is_monitoring and detection_count < max_detections:
            try:
                if len(self.market_data_history) < 2:
                    await asyncio.sleep(5)
                    continue
                
                # ìµœê·¼ ë°ì´í„°ì™€ ì´ì „ ë°ì´í„° ë¹„êµ
                current_data = self.market_data_history[-1]
                previous_data = self.market_data_history[-2]
                
                # ê°€ê²© ë³€ë™ ê°ì§€
                price_change = abs(current_data.change_percent - previous_data.change_percent)
                if price_change > self.config.price_change_threshold:
                    event = MarketEvent(
                        event_type=MarketEventType.PRICE_SPIKE,
                        symbol=current_data.symbol,
                        timestamp=current_data.timestamp,
                        description=f"ê°€ê²© ê¸‰ë³€: {price_change:.2%}",
                        severity=Priority.HIGH,
                        data={'price_change': price_change, 'current_price': current_data.price}
                    )
                    await self._handle_market_event(event)
                
                # ê±°ë˜ëŸ‰ ê¸‰ì¦ ê°ì§€
                volume_ratio = current_data.volume / max(previous_data.volume, 1)
                if volume_ratio > self.config.volume_change_threshold:
                    event = MarketEvent(
                        event_type=MarketEventType.VOLUME_SURGE,
                        symbol=current_data.symbol,
                        timestamp=current_data.timestamp,
                        description=f"ê±°ë˜ëŸ‰ ê¸‰ì¦: {volume_ratio:.1f}ë°°",
                        severity=Priority.NORMAL,
                        data={'volume_ratio': volume_ratio, 'current_volume': current_data.volume}
                    )
                    await self._handle_market_event(event)
                
                detection_count += 1
                await asyncio.sleep(5)  # 5ì´ˆë§ˆë‹¤ ì´ë²¤íŠ¸ ì²´í¬
                
            except Exception as e:
                print(f"ì‹œì¥ ì´ë²¤íŠ¸ ê°ì§€ ì—ëŸ¬: {e}")
                await asyncio.sleep(5)
        
        print(f"ì‹œì¥ ì´ë²¤íŠ¸ ê°ì§€ ì™„ë£Œ (ì´ {detection_count}íšŒ)")
    
    async def _handle_market_event(self, event: MarketEvent):
        """ì‹œì¥ ì´ë²¤íŠ¸ ì²˜ë¦¬"""
        self.detected_events.append(event)
        self.metrics.market_events_detected += 1
        
        print(f"ì‹œì¥ ì´ë²¤íŠ¸ ê°ì§€: {event.description}")
        
        # ê¸´ê¸‰ ìƒí™© ì‹œ ë” ìì£¼ ìˆ˜ì§‘
        if event.severity == Priority.HIGH:
            self.emergency_mode = True
            asyncio.create_task(self._emergency_data_collection())
        
        # ì´ë²¤íŠ¸ ì €ì¥
        await self._save_market_event(event)
    
    async def _emergency_data_collection(self):
        """ê¸´ê¸‰ ë°ì´í„° ìˆ˜ì§‘"""
        print("ê¸´ê¸‰ ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘")
        
        for i in range(5):  # 5íšŒ ê¸´ê¸‰ ìˆ˜ì§‘ (í…ŒìŠ¤íŠ¸ìš©)
            try:
                data = await self.collect_data_parallel([DataType.STOCK])
                await self._process_market_data(data)
                print(f"ê¸´ê¸‰ ë°ì´í„° ìˆ˜ì§‘ {i+1}/5 ì™„ë£Œ")
                await asyncio.sleep(self.config.emergency_collection_interval)
            except Exception as e:
                print(f"ê¸´ê¸‰ ë°ì´í„° ìˆ˜ì§‘ ì—ëŸ¬: {e}")
        
        self.emergency_mode = False
        print("ê¸´ê¸‰ ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
    
    async def _emergency_monitoring(self):
        """ê¸´ê¸‰ ìƒí™© ëª¨ë‹ˆí„°ë§"""
        monitoring_count = 0
        max_monitoring = 30  # ìµœëŒ€ 30íšŒ ëª¨ë‹ˆí„°ë§ í›„ ì¢…ë£Œ
        
        while self.is_monitoring and monitoring_count < max_monitoring:
            try:
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
                if self.metrics.memory_usage_mb > self.config.memory_limit_gb * 1024 * 0.9:
                    print("ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ìœ„í—˜ ìˆ˜ì¤€")
                    gc.collect()
                
                # ì—ëŸ¬ìœ¨ ì²´í¬
                error_rate = self.metrics.failed_requests / max(self.metrics.total_requests, 1)
                if error_rate > 0.1:  # 10% ì´ìƒ ì—ëŸ¬
                    print(f"ë†’ì€ ì—ëŸ¬ìœ¨: {error_rate:.1%}")
                
                monitoring_count += 1
                await asyncio.sleep(30)  # 30ì´ˆë§ˆë‹¤ ì²´í¬
                
            except Exception as e:
                print(f"ê¸´ê¸‰ ëª¨ë‹ˆí„°ë§ ì—ëŸ¬: {e}")
                await asyncio.sleep(10)
        
        print(f"ê¸´ê¸‰ ëª¨ë‹ˆí„°ë§ ì™„ë£Œ (ì´ {monitoring_count}íšŒ)")
    
    def _is_market_hours(self) -> bool:
        """ì‹œì¥ ì‹œê°„ ì²´í¬"""
        now = datetime.now()
        current_time = now.time()
        
        # ì£¼ë§ ì²´í¬
        if not self.config.weekend_monitoring and now.weekday() >= 5:
            return False
        
        # ì‹œì¥ ì‹œê°„ ì²´í¬
        return self.market_start <= current_time <= self.market_end
    
    async def _process_market_data(self, data: Dict[str, Any]):
        """ì‹œì¥ ë°ì´í„° ì²˜ë¦¬"""
        try:
            # ë”ë¯¸ ë°ì´í„° ìƒì„± (ì‹¤ì œ API ì‘ë‹µ ëŒ€ì‹ )
            dummy_data = MarketData(
                timestamp=datetime.now(),
                symbol="005930",  # ì‚¼ì„±ì „ì
                price=75000.0,
                volume=1000000,
                change_percent=2.5,
                market_cap=45000000000000.0
            )
            
            self.market_data_history.append(dummy_data)
            self.last_market_data[dummy_data.symbol] = dummy_data
            
            print(f"ì‹œì¥ ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ: {dummy_data.symbol}")
                
        except Exception as e:
            print(f"ì‹œì¥ ë°ì´í„° ì²˜ë¦¬ ì—ëŸ¬: {e}")
    
    async def _save_market_event(self, event: MarketEvent):
        """ì‹œì¥ ì´ë²¤íŠ¸ ì €ì¥"""
        try:
            event_data = {
                'event_type': event.event_type.name,
                'symbol': event.symbol,
                'timestamp': event.timestamp.isoformat(),
                'description': event.description,
                'severity': event.severity.name,
                'data': event.data
            }
            
            events_file = Path('data/market_events.jsonl')
            events_file.parent.mkdir(parents=True, exist_ok=True)
            
            with open(events_file, 'a', encoding='utf-8') as f:
                f.write(json.dumps(event_data, ensure_ascii=False) + '\n')
                
        except Exception as e:
            print(f"ì‹œì¥ ì´ë²¤íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def collect_data_parallel(self, data_types: List[DataType]) -> Dict[str, Any]:
        """ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘ - ìµœì‹  Python í‘œì¤€ í™œìš©"""
        start_time = time.time()
        
        try:
            # ë¹„ë™ê¸° ë³‘ë ¬ ì²˜ë¦¬
            tasks = [self._collect_single_data_type(data_type) for data_type in data_types]
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # ê²°ê³¼ í†µí•©
            combined_results = {}
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    print(f"ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ ({data_types[i].name}): {result}")
                else:
                    combined_results[data_types[i].name] = result
            
            execution_time = time.time() - start_time
            self._update_performance_metrics(execution_time)
            
            return combined_results
            
        except Exception as e:
            print(f"ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return {}
    
    async def _collect_single_data_type(self, data_type: DataType) -> Dict[str, Any]:
        """ë‹¨ì¼ ë°ì´í„° íƒ€ì… ìˆ˜ì§‘"""
        cache_key = self._get_cache_key(data_type.name, 'KRX', datetime.now().strftime('%Y%m%d'))
        
        # ìºì‹œì—ì„œ ë¨¼ì € í™•ì¸
        cached_data = await self._multi_level_cache_get(cache_key)
        if cached_data:
            return cached_data
        
        try:
            # ì‹¤ì œ API í˜¸ì¶œ ëŒ€ì‹  ë”ë¯¸ ë°ì´í„° ë°˜í™˜ (í…ŒìŠ¤íŠ¸ìš©)
            dummy_data = {
                'OutBlock_1': [
                    {
                        'ISU_CD': '005930',
                        'TDD_CLSPRC': '75000',
                        'ACC_TRDVOL': '1000000',
                        'CMPPREVDD_PRC': '2.5',
                        'MKTCAP': '45000000000000'
                    }
                ]
            }
            
            # ìºì‹œì— ì €ì¥
            await self._multi_level_cache_set(cache_key, dummy_data)
            
            self.metrics.successful_requests += 1
            return dummy_data
                        
        except Exception as e:
            self.metrics.failed_requests += 1
            print(f"ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨ ({data_type.name}): {e}")
            raise
    
    def _get_request_params(self, data_type: DataType) -> Dict[str, str]:
        """ìš”ì²­ íŒŒë¼ë¯¸í„° ìƒì„±"""
        base_params = {
            'bld': 'dbms/MDC/STAT/standard/MDCSTAT01501',
            'trdDd': datetime.now().strftime('%Y%m%d')
        }
        
        if data_type == DataType.STOCK:
            base_params['mktId'] = 'STK'
        elif data_type == DataType.FUTURES:
            base_params['mktId'] = 'FUT'
        elif data_type == DataType.OPTIONS:
            base_params['mktId'] = 'OPT'
        elif data_type == DataType.INDEX:
            base_params['mktId'] = 'IDX'
        elif data_type == DataType.ETF:
            base_params['mktId'] = 'ETF'
        
        return base_params
    
    def _get_headers(self) -> Dict[str, str]:
        """ìš”ì²­ í—¤ë” ìƒì„±"""
        return {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36',
            'Accept': 'application/json, text/javascript, */*; q=0.01',
            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'X-Requested-With': 'XMLHttpRequest'
        }
    
    def _update_performance_metrics(self, execution_time: float):
        """ì„±ëŠ¥ ë©”íŠ¸ë¦­ ì—…ë°ì´íŠ¸"""
        self.metrics.total_requests += 1
        self.metrics.avg_response_time = (
            (self.metrics.avg_response_time * (self.metrics.total_requests - 1) + execution_time) 
            / self.metrics.total_requests
        )
        
        # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì—…ë°ì´íŠ¸
        try:
            process = psutil.Process()
            self.metrics.memory_usage_mb = process.memory_info().rss / (1024 * 1024)
            self.metrics.cpu_usage_percent = process.cpu_percent()
        except Exception:
            # psutil ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ê²½ìš° ê¸°ë³¸ê°’ ì‚¬ìš©
            self.metrics.memory_usage_mb = 100.0
            self.metrics.cpu_usage_percent = 5.0
    
    async def _performance_monitoring(self):
        """ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§"""
        monitoring_count = 0
        max_monitoring = 15  # ìµœëŒ€ 15íšŒ ëª¨ë‹ˆí„°ë§ í›„ ì¢…ë£Œ
        
        while self.is_monitoring and monitoring_count < max_monitoring:
            try:
                # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬
                if self.metrics.memory_usage_mb > self.config.memory_limit_gb * 1024 * 0.8:
                    print(f"ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë†’ìŒ: {self.metrics.memory_usage_mb:.2f}MB")
                    gc.collect()
                
                # ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì €ì¥
                await self._save_performance_report()
                
                monitoring_count += 1
                await asyncio.sleep(60)  # 1ë¶„ë§ˆë‹¤ ì²´í¬
                
            except Exception as e:
                print(f"ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì—ëŸ¬: {e}")
                await asyncio.sleep(10)
        
        print(f"ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì™„ë£Œ (ì´ {monitoring_count}íšŒ)")
    
    async def _save_performance_report(self):
        """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì €ì¥"""
        try:
            report = {
                'timestamp': datetime.now().isoformat(),
                'metrics': self.metrics.model_dump(),
                'cache_stats': {
                    'hits': self.cache_hits,
                    'misses': self.cache_misses,
                    'hit_rate': self.cache_hits / (self.cache_hits + self.cache_misses) if (self.cache_hits + self.cache_misses) > 0 else 0
                },
                'monitoring_stats': {
                    'is_monitoring': self.is_monitoring,
                    'emergency_mode': self.emergency_mode,
                    'market_data_count': len(self.market_data_history),
                    'detected_events_count': len(self.detected_events),
                    'is_market_hours': self._is_market_hours()
                }
            }
            
            report_file = Path('reports/performance_report.json')
            report_file.parent.mkdir(parents=True, exist_ok=True)
            
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
                
        except Exception as e:
            print(f"ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì €ì¥ ì‹¤íŒ¨: {e}")
    
    async def run_backtest(self, strategy_name: str, start_date: str, end_date: str) -> Dict[str, Any]:
        """ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰ - ìµœì‹  Python í‘œì¤€ í™œìš©"""
        try:
            # ë³‘ë ¬ ë°ì´í„° ìˆ˜ì§‘
            data = await self.collect_data_parallel([DataType.STOCK, DataType.INDEX])
            
            # ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰
            results = await self._execute_backtest(strategy_name, data, start_date, end_date)
            
            return results
            
        except Exception as e:
            print(f"ë°±í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return {'error': str(e)}
    
    async def _execute_backtest(self, strategy_name: str, data: Dict[str, Any], start_date: str, end_date: str) -> Dict[str, Any]:
        """ë°±í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        # ê°„ë‹¨í•œ ë°±í…ŒìŠ¤íŠ¸ ë¡œì§ (ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ ì „ëµ êµ¬í˜„)
        return {
            'strategy': strategy_name,
            'start_date': start_date,
            'end_date': end_date,
            'total_return': 0.15,
            'sharpe_ratio': 1.2,
            'max_drawdown': -0.05,
            'win_rate': 0.65,
            'timestamp': datetime.now().isoformat()
        }
    
    async def auto_improvement_loop(self, df: pd.DataFrame, output_dir: Path = None) -> Dict[str, Any]:
        """KRX ë°ì´í„° ìë™ ê°œì„  ë£¨í”„"""
        if output_dir is None:
            output_dir = Path("krx_improvement_outputs")
        output_dir.mkdir(exist_ok=True)
        
        # ë°ì´í„° ìœ í˜•ë³„ ìµœì  ì„¤ì • ì ìš©
        config = get_krx_optimized_config(df)
        max_iterations = config["max_iterations"]
        target_excellent_folds = config["target_excellent_folds"]
        max_no_improvement = config["max_no_improvement"]
        
        logging.info(f"KRX ìë™ ê°œì„  ë£¨í”„ ì‹œì‘ - ìµœëŒ€ {max_iterations}íšŒ ë°˜ë³µ")
        
        # í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì‹œìŠ¤í…œ ì´ˆê¸°í™”
        telegram = KRXTelegramNotifier()
        await telegram.send_message("ğŸš€ <b>KRX ë°ì´í„° ìë™ ê°œì„  ì‹œì‘</b>\n\nâ±ï¸ ë°ì´í„° ìœ í˜•ë³„ ìµœì í™” ì ìš© ì¤‘...", force=True)
        
        # ì„±ëŠ¥ ì¶”ì 
        performance_history = []
        best_r2 = 0
        no_improvement_count = 0
        global achieved_excellent_grade
        achieved_excellent_grade = False
        
        improvement_strategies_all = [
            "ê³ ê¸‰ Feature Engineering", "Feature Selection", "Scaling ë‹¤ì–‘í™”", "ì´ìƒì¹˜ ì²˜ë¦¬ ê°•í™”",
            "ëª¨ë¸ íŒŒë¼ë¯¸í„° ëœë¤ íƒìƒ‰", "Ensemble ë‹¤ì–‘í™”", "ë°ì´í„° í’ˆì§ˆ ê°œì„ "
        ]
        
        for iteration in range(1, max_iterations + 1):
            logging.info(f"KRX ê°œì„  ë°˜ë³µ {iteration} ì‹œì‘")
            
            # ì„±ëŠ¥ ë¶„ì„
            analysis = await self._analyze_krx_performance(df, output_dir)
            performance_history.append(analysis)
            
            # ì„±ëŠ¥ ê°œì„  ì¶”ì 
            current_r2 = analysis.get("avg_r2", 0)
            if current_r2 > best_r2:
                best_r2 = current_r2
                no_improvement_count = 0
                logging.info(f"KRX ì„±ëŠ¥ ê°œì„  ê°ì§€: RÂ² {current_r2:.6f}")
            else:
                no_improvement_count += 1
                logging.info(f"KRX ì„±ëŠ¥ ê°œì„  ì—†ìŒ (ì—°ì† {no_improvement_count}íšŒ)")
            
            # í…”ë ˆê·¸ë¨ ì•Œë¦¼
            data_info = {"rows": len(df), "columns": len(df.columns)}
            await telegram.send_krx_performance_report(analysis, data_info)
            
            # ì¢…ë£Œ ì¡°ê±´ í™•ì¸
            excellent_folds = analysis.get("excellent_folds", 0)
            performance_grade = analysis.get("performance_grade", "")
            avg_r2 = analysis.get("avg_r2", 0)
            
            should_stop = (
                excellent_folds >= target_excellent_folds and 
                performance_grade.startswith("ğŸŸ¢") and
                avg_r2 > 0.8
            )
            
            # ì¡°ê¸° ì¢…ë£Œ ì¡°ê±´
            if no_improvement_count >= max_no_improvement:
                logging.info(f"KRX ì—°ì† {max_no_improvement}íšŒ ì„±ëŠ¥ ê°œì„  ì—†ìŒ - ì¡°ê¸° ì¢…ë£Œ")
                break
            
            if should_stop:
                logging.info("KRX ìš°ìˆ˜ ì„±ëŠ¥ ë‹¬ì„±! ìë™ ê°œì„  ë£¨í”„ ì¢…ë£Œ")
                await telegram.send_krx_improvement_complete(iteration, analysis)
                achieved_excellent_grade = True
                break
            
            # ê°œì„  ì‹¤í–‰
            if analysis.get("improvement_needed", False):
                strategies = analysis.get("improvement_strategies", [])
                await telegram.send_krx_improvement_start(iteration, strategies)
                logging.info(f"KRX ì ìš© ê°œì„  ì „ëµ: {strategies}")
                
                try:
                    # ë°ì´í„° ê°œì„  ì ìš©
                    df = await self._apply_krx_improvements(df, strategies)
                    logging.info(f"KRX ê°œì„  ë°˜ë³µ {iteration} ì™„ë£Œ")
                except Exception as e:
                    logging.error(f"KRX ê°œì„  ë°˜ë³µ {iteration} ì‹¤íŒ¨", error=str(e))
                    await telegram.send_message(f"âŒ <b>KRX ê°œì„  ë°˜ë³µ {iteration} ì‹¤íŒ¨</b>\n\nì˜¤ë¥˜: {str(e)}")
                    continue
            else:
                logging.info("KRX ê°œì„ ì´ í•„ìš”í•˜ì§€ ì•ŠìŒ")
                break
        
        # ì„±ëŠ¥ íˆìŠ¤í† ë¦¬ ì €ì¥
        history_path = output_dir / "krx_improvement_history.json"
        with open(history_path, "w", encoding="utf-8") as f:
            json.dump(performance_history, f, indent=2, ensure_ascii=False, default=np_encoder)
        
        logging.info("KRX ìë™ ê°œì„  ë£¨í”„ ì™„ë£Œ")
        return {"performance_history": performance_history, "final_analysis": analysis}
    
    async def _analyze_krx_performance(self, df: pd.DataFrame, output_dir: Path) -> Dict[str, Any]:
        """KRX ì„±ëŠ¥ ë¶„ì„"""
        try:
            # ê°„ë‹¨í•œ ì„±ëŠ¥ ë¶„ì„ (ì‹¤ì œë¡œëŠ” ML ëª¨ë¸ í›ˆë ¨ í•„ìš”)
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            if len(numeric_cols) < 2:
                return {
                    "avg_r2": 0.0,
                    "performance_grade": "ğŸ”´ ê°œì„  í•„ìš”",
                    "improvement_needed": True,
                    "improvement_strategies": ["ë°ì´í„° í’ˆì§ˆ ê°œì„ "]
                }
            
            # ê°€ìƒ ì„±ëŠ¥ ì§€í‘œ (ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì‹¤ì œ ëª¨ë¸ í›ˆë ¨ ê²°ê³¼ ì‚¬ìš©)
            avg_r2 = np.random.uniform(0.3, 0.9)  # ì˜ˆì‹œìš©
            performance_grade = "ğŸŸ¢ ìš°ìˆ˜" if avg_r2 > 0.8 else "ğŸ”´ ê°œì„  í•„ìš”"
            
            return {
                "avg_r2": avg_r2,
                "performance_grade": performance_grade,
                "excellent_folds": 3 if avg_r2 > 0.8 else 0,
                "improvement_needed": avg_r2 < 0.8,
                "improvement_strategies": ["Feature Engineering", "ë°ì´í„° í’ˆì§ˆ ê°œì„ "] if avg_r2 < 0.8 else []
            }
        except Exception as e:
            logging.error(f"KRX ì„±ëŠ¥ ë¶„ì„ ì‹¤íŒ¨: {e}")
            return {
                "avg_r2": 0.0,
                "performance_grade": "ğŸ”´ ê°œì„  í•„ìš”",
                "improvement_needed": True,
                "improvement_strategies": ["ì˜¤ë¥˜ ë³µêµ¬"]
            }
    
    async def _apply_krx_improvements(self, df: pd.DataFrame, strategies: List[str]) -> pd.DataFrame:
        """KRX ë°ì´í„° ê°œì„  ì ìš©"""
        try:
            for strategy in strategies:
                if "Feature Engineering" in strategy:
                    # íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì ìš©
                    df = await self._apply_krx_feature_engineering(df)
                elif "ë°ì´í„° í’ˆì§ˆ ê°œì„ " in strategy:
                    # ë°ì´í„° í’ˆì§ˆ ê°œì„ 
                    df = await self._apply_krx_data_quality_improvement(df)
                elif "ì´ìƒì¹˜ ì²˜ë¦¬" in strategy:
                    # ì´ìƒì¹˜ ì²˜ë¦¬
                    df = await self._apply_krx_outlier_removal(df)
            
            return df
        except Exception as e:
            logging.error(f"KRX ê°œì„  ì ìš© ì‹¤íŒ¨: {e}")
            return df
    
    async def _apply_krx_feature_engineering(self, df: pd.DataFrame) -> pd.DataFrame:
        """KRX íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§"""
        try:
            # ê°€ê²© ê´€ë ¨ íŠ¹ì„± ì¶”ê°€
            if 'í˜„ì¬ê°€' in df.columns:
                df['ê°€ê²©ë³€í™”ìœ¨'] = df['í˜„ì¬ê°€'].pct_change()
                df['ê°€ê²©ë³€í™”ìœ¨_5ì¼'] = df['í˜„ì¬ê°€'].pct_change(5)
            
            # ê±°ë˜ëŸ‰ ê´€ë ¨ íŠ¹ì„± ì¶”ê°€
            if 'ê±°ë˜ëŸ‰' in df.columns:
                df['ê±°ë˜ëŸ‰_ì´ë™í‰ê· '] = df['ê±°ë˜ëŸ‰'].rolling(window=5).mean()
                df['ê±°ë˜ëŸ‰_ë¹„ìœ¨'] = df['ê±°ë˜ëŸ‰'] / df['ê±°ë˜ëŸ‰'].rolling(window=20).mean()
            
            logging.info("KRX íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì™„ë£Œ")
            return df
        except Exception as e:
            logging.error(f"KRX íŠ¹ì„± ì—”ì§€ë‹ˆì–´ë§ ì‹¤íŒ¨: {e}")
            return df
    
    async def _apply_krx_data_quality_improvement(self, df: pd.DataFrame) -> pd.DataFrame:
        """KRX ë°ì´í„° í’ˆì§ˆ ê°œì„ """
        try:
            # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
            df = df.fillna(method='ffill').fillna(method='bfill')
            
            # ì¤‘ë³µ ì œê±°
            df = df.drop_duplicates()
            
            # ë°ì´í„° íƒ€ì… ë³€í™˜
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            for col in numeric_cols:
                if df[col].dtype == 'object':
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            logging.info("KRX ë°ì´í„° í’ˆì§ˆ ê°œì„  ì™„ë£Œ")
            return df
        except Exception as e:
            logging.error(f"KRX ë°ì´í„° í’ˆì§ˆ ê°œì„  ì‹¤íŒ¨: {e}")
            return df
    
    async def _apply_krx_outlier_removal(self, df: pd.DataFrame) -> pd.DataFrame:
        """KRX ì´ìƒì¹˜ ì œê±°"""
        try:
            numeric_cols = df.select_dtypes(include=[np.number]).columns
            
            for col in numeric_cols:
                if col in df.columns:
                    Q1 = df[col].quantile(0.25)
                    Q3 = df[col].quantile(0.75)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - 1.5 * IQR
                    upper_bound = Q3 + 1.5 * IQR
                    
                    # ì´ìƒì¹˜ ì œê±°
                    df = df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]
            
            logging.info("KRX ì´ìƒì¹˜ ì œê±° ì™„ë£Œ")
            return df
        except Exception as e:
            logging.error(f"KRX ì´ìƒì¹˜ ì œê±° ì‹¤íŒ¨: {e}")
            return df

    async def save_krx_data_optimized(self, df: pd.DataFrame, analysis: Dict[str, Any], output_dir: Path) -> Dict[str, Any]:
        """KRX ë°ì´í„° ìµœì í™” ì €ì¥ (ìë™ë§¤ë§¤ íŒë‹¨ ê³ ë ¤)"""
        try:
            # ì„±ëŠ¥ í‰ê°€
            evaluation = evaluate_krx_performance(analysis)
            trading_confidence = evaluation["trading_confidence"]
            performance_grade = evaluation["performance_grade"]
            
            # ì €ì¥ ì „ëµ ê²°ì •
            storage_strategy = get_krx_storage_strategy(df, trading_confidence)
            
            # ì €ì¥ ë””ë ‰í† ë¦¬ ìƒì„±
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            save_dir = output_dir / f"krx_{trading_confidence}_{timestamp}"
            save_dir.mkdir(parents=True, exist_ok=True)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata = {
                "timestamp": timestamp,
                "data_shape": df.shape,
                "performance_grade": performance_grade,
                "trading_confidence": trading_confidence,
                "trading_recommendation": evaluation["trading_recommendation"],
                "storage_strategy": storage_strategy,
                "analysis_summary": {
                    "avg_r2": analysis.get("avg_r2", 0),
                    "avg_rmse": analysis.get("avg_rmse", 0),
                    "excellent_folds": analysis.get("excellent_folds", 0),
                    "poor_folds": analysis.get("poor_folds", 0)
                }
            }
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            metadata_path = save_dir / "metadata.json"
            with open(metadata_path, "w", encoding="utf-8") as f:
                json.dump(metadata, f, indent=2, ensure_ascii=False, default=np_encoder)
            
            # ë°ì´í„° ì €ì¥ (ì „ëµì— ë”°ë¼)
            data_path = await self._save_krx_data_with_strategy(df, save_dir, storage_strategy)
            
            # ìë™ë§¤ë§¤ ì„¤ì • íŒŒì¼ ìƒì„±
            if trading_confidence in ["high_confidence", "medium_confidence"]:
                await self._create_trading_config(save_dir, evaluation, storage_strategy)
            
            # ë°±ì—… ì„¤ì •
            await self._setup_backup_strategy(save_dir, storage_strategy)
            
            logging.info(f"KRX ë°ì´í„° ìµœì í™” ì €ì¥ ì™„ë£Œ: {data_path}")
            logging.info(f"ìë™ë§¤ë§¤ ì‹ ë¢°ë„: {trading_confidence}")
            logging.info(f"ì„±ëŠ¥ ë“±ê¸‰: {performance_grade}")
            
            return {
                "save_path": str(data_path),
                "metadata_path": str(metadata_path),
                "trading_confidence": trading_confidence,
                "performance_grade": performance_grade,
                "storage_strategy": storage_strategy
            }
            
        except Exception as e:
            logging.error(f"KRX ë°ì´í„° ì €ì¥ ì‹¤íŒ¨: {e}")
            return {"error": str(e)}
    
    async def _save_krx_data_with_strategy(self, df: pd.DataFrame, save_dir: Path, strategy: Dict[str, Any]) -> Path:
        """ì „ëµì— ë”°ë¥¸ KRX ë°ì´í„° ì €ì¥"""
        format_type = strategy["storage_format"]
        compression = strategy["compression"]
        
        if format_type == "parquet":
            # Parquet ì €ì¥ (ìµœì í™”ëœ í˜•ì‹)
            data_path = save_dir / f"krx_data.{format_type}"
            
            # íŒŒí‹°ì…”ë‹ ì ìš©
            partition_cols = strategy.get("partition_by", [])
            if partition_cols and any(col in df.columns for col in partition_cols):
                available_partitions = [col for col in partition_cols if col in df.columns]
                if available_partitions:
                    df.to_parquet(
                        data_path,
                        compression=compression,
                        partition_cols=available_partitions,
                        index=False
                    )
                else:
                    df.to_parquet(data_path, compression=compression, index=False)
            else:
                df.to_parquet(data_path, compression=compression, index=False)
        
        elif format_type == "csv":
            # CSV ì €ì¥ (í˜¸í™˜ì„±)
            data_path = save_dir / "krx_data.csv"
            df.to_csv(data_path, index=False, encoding='utf-8-sig')
        
        else:
            # ê¸°ë³¸ Parquet ì €ì¥
            data_path = save_dir / "krx_data.parquet"
            df.to_parquet(data_path, compression="snappy", index=False)
        
        return data_path
    
    async def _create_trading_config(self, save_dir: Path, evaluation: Dict[str, Any], strategy: Dict[str, Any]):
        """ìë™ë§¤ë§¤ ì„¤ì • íŒŒì¼ ìƒì„±"""
        trading_config = {
            "enabled": evaluation["trading_confidence"] in ["high_confidence", "medium_confidence"],
            "confidence_level": evaluation["trading_confidence"],
            "performance_grade": evaluation["performance_grade"],
            "recommendation": evaluation["trading_recommendation"],
            "risk_management": {
                "max_position_size": 0.1 if evaluation["trading_confidence"] == "high_confidence" else 0.05,
                "stop_loss": 0.02,
                "take_profit": 0.05,
                "max_daily_trades": 10 if evaluation["trading_confidence"] == "high_confidence" else 5
            },
            "data_requirements": {
                "min_data_quality": 0.8,
                "min_performance_r2": 0.7,
                "max_rmse": 0.15
            },
            "storage_config": strategy
        }
        
        config_path = save_dir / "trading_config.json"
        with open(config_path, "w", encoding="utf-8") as f:
            json.dump(trading_config, f, indent=2, ensure_ascii=False, default=np_encoder)
        
        logging.info(f"ìë™ë§¤ë§¤ ì„¤ì • íŒŒì¼ ìƒì„±: {config_path}")
    
    async def _setup_backup_strategy(self, save_dir: Path, strategy: Dict[str, Any]):
        """ë°±ì—… ì „ëµ ì„¤ì •"""
        backup_config = {
            "frequency": strategy["backup_frequency"],
            "retention_days": strategy["retention_days"],
            "compression": strategy["compression"],
            "description": strategy["description"]
        }
        
        backup_path = save_dir / "backup_config.json"
        with open(backup_path, "w", encoding="utf-8") as f:
            json.dump(backup_config, f, indent=2, ensure_ascii=False, default=np_encoder)
        
        logging.info(f"ë°±ì—… ì„¤ì • íŒŒì¼ ìƒì„±: {backup_path}")
    
    async def load_krx_data_optimized(self, data_path: Path) -> Tuple[pd.DataFrame, Dict[str, Any]]:
        """KRX ë°ì´í„° ìµœì í™” ë¡œë“œ"""
        try:
            # ë©”íƒ€ë°ì´í„° ë¡œë“œ
            metadata_path = data_path.parent / "metadata.json"
            if metadata_path.exists():
                with open(metadata_path, "r", encoding="utf-8") as f:
                    metadata = json.load(f)
            else:
                metadata = {}
            
            # ë°ì´í„° ë¡œë“œ
            if data_path.suffix == ".parquet":
                df = pd.read_parquet(data_path)
            elif data_path.suffix == ".csv":
                df = pd.read_csv(data_path, encoding='utf-8-sig')
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {data_path.suffix}")
            
            logging.info(f"KRX ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {data_path}")
            logging.info(f"ë°ì´í„° í¬ê¸°: {df.shape}")
            
            return df, metadata
            
        except Exception as e:
            logging.error(f"KRX ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
            return pd.DataFrame(), {}
    
    async def get_krx_storage_recommendations(self, df: pd.DataFrame, analysis: Dict[str, Any]) -> Dict[str, Any]:
        """KRX ì €ì¥ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        evaluation = evaluate_krx_performance(analysis)
        storage_strategy = get_krx_storage_strategy(df, evaluation["trading_confidence"])
        
        recommendations = {
            "performance_summary": {
                "grade": evaluation["performance_grade"],
                "trading_confidence": evaluation["trading_confidence"],
                "recommendation": evaluation["trading_recommendation"]
            },
            "storage_strategy": storage_strategy,
            "data_characteristics": detect_krx_data_characteristics(df),
            "optimization_suggestions": []
        }
        
        # ìµœì í™” ì œì•ˆ
        if evaluation["trading_confidence"] == "not_tradeable":
            recommendations["optimization_suggestions"].append("ë°ì´í„° í’ˆì§ˆ ê°œì„  í•„ìš”")
            recommendations["optimization_suggestions"].append("ëª¨ë¸ ì„±ëŠ¥ í–¥ìƒ í•„ìš”")
        
        if df.shape[0] > 100000:
            recommendations["optimization_suggestions"].append("ëŒ€ìš©ëŸ‰ ë°ì´í„° - íŒŒí‹°ì…”ë‹ ê¶Œì¥")
        
        if len(df.select_dtypes(include=['object']).columns) > 5:
            recommendations["optimization_suggestions"].append("ë²”ì£¼í˜• ë°ì´í„° ìµœì í™” í•„ìš”")
        
        return recommendations

    def get_system_status(self) -> Dict[str, Any]:
        """ì‹œìŠ¤í…œ ìƒíƒœ ë°˜í™˜"""
        return {
            'mode': self.config.mode.name,
            'metrics': self.metrics.model_dump(),
            'cache_stats': {
                'memory_cache_size': len(self.memory_cache),
                'disk_cache_size': len(list(self.cache_dir.glob('*.pkl'))),
                'distributed_cache_size': len(self.distributed_cache),
                'hit_rate': self.cache_hits / (self.cache_hits + self.cache_misses) if (self.cache_hits + self.cache_misses) > 0 else 0
            },
            'memory_usage': {
                'current_mb': self.metrics.memory_usage_mb,
                'limit_gb': self.config.memory_limit_gb,
                'usage_percent': (self.metrics.memory_usage_mb / (self.config.memory_limit_gb * 1024)) * 100
            },
            'monitoring_status': {
                'is_monitoring': self.is_monitoring,
                'emergency_mode': self.emergency_mode,
                'market_hours': self._is_market_hours(),
                'market_data_count': len(self.market_data_history),
                'detected_events_count': len(self.detected_events),
                'next_collection': self.metrics.next_collection_time
            }
        }

class MemoryMonitor:
    """ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤"""
    
    def __init__(self, limit_gb: float):
        self.limit_bytes = limit_gb * 1024 * 1024 * 1024
        self.warning_threshold = 0.8
    
    def check_memory_usage(self) -> bool:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬"""
        try:
            current_usage = psutil.virtual_memory().used
            usage_ratio = current_usage / self.limit_bytes
            
            if usage_ratio > self.warning_threshold:
                return False
            return True
        except Exception:
            return True  # psutil ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ê²½ìš° True ë°˜í™˜

class DummyMemoryMonitor:
    """ë”ë¯¸ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ í´ë˜ìŠ¤ (psutil ì‚¬ìš© ë¶ˆê°€ëŠ¥í•œ ê²½ìš°)"""
    
    def __init__(self, limit_gb: float = 8.0):
        self.limit_bytes = limit_gb * 1024 * 1024 * 1024
        self.warning_threshold = 0.8
    
    def check_memory_usage(self) -> bool:
        """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì²´í¬ (ë”ë¯¸)"""
        return True

# ë¹„ë™ê¸° í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì‹œìŠ¤í…œ (KRXìš©)
class KRXTelegramNotifier:
    """KRX ì‹œìŠ¤í…œìš© ë¹„ë™ê¸° í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì‹œìŠ¤í…œ"""
    
    def __init__(self, bot_token: str = "", chat_id: str = "", enable_notifications: bool = True):
        self.bot_token = bot_token or os.getenv("TELEGRAM_BOT_TOKEN", "")
        self.chat_id = chat_id or os.getenv("TELEGRAM_CHAT_ID", "")
        self.base_url = f"https://api.telegram.org/bot{self.bot_token}" if self.bot_token else ""
        self.enabled = bool(self.bot_token and self.chat_id and enable_notifications)
        self.last_notification_time = 0
        self.notification_cooldown = 300  # 5ë¶„ ì¿¨ë‹¤ìš´
    
    async def send_message(self, message: str, force: bool = False) -> bool:
        """ë¹„ë™ê¸° í…”ë ˆê·¸ë¨ ë©”ì‹œì§€ ì „ì†¡ (ì¿¨ë‹¤ìš´ ì ìš©)"""
        if not self.enabled:
            return True
        
        current_time = time.time()
        if not force and current_time - self.last_notification_time < self.notification_cooldown:
            logging.debug("í…”ë ˆê·¸ë¨ ì•Œë¦¼ ì¿¨ë‹¤ìš´ ì¤‘ - ë©”ì‹œì§€ ì „ì†¡ ê±´ë„ˆëœ€")
            return True
        
        try:
            async with aiohttp.ClientSession() as session:
                url = f"{self.base_url}/sendMessage"
                data = {
                    "chat_id": self.chat_id,
                    "text": message,
                    "parse_mode": "HTML"
                }
                async with session.post(url, data=data) as response:
                    if response.status == 200:
                        self.last_notification_time = current_time
                        return True
                    return False
        except Exception as e:
            logging.error("í…”ë ˆê·¸ë¨ ì „ì†¡ ì‹¤íŒ¨", error=str(e))
            return False
    
    async def send_krx_performance_report(self, analysis: Dict[str, Any], data_info: Dict[str, Any]) -> bool:
        """KRX ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì „ì†¡"""
        try:
            performance_grade = analysis.get('performance_grade', '')
            improvement_needed = analysis.get('improvement_needed', False)
            
            if performance_grade.startswith("ğŸ”´") or improvement_needed:
                message = self._format_krx_performance_message(analysis, data_info)
                return await self.send_message(message, force=True)
            else:
                logging.debug("ì„±ëŠ¥ì´ ì–‘í˜¸í•˜ì—¬ í…”ë ˆê·¸ë¨ ì•Œë¦¼ ê±´ë„ˆëœ€")
                return True
        except Exception as e:
            logging.error("KRX ì„±ëŠ¥ ë¦¬í¬íŠ¸ ì „ì†¡ ì‹¤íŒ¨", error=str(e))
            return False
    
    async def send_krx_improvement_start(self, iteration: int, strategies: List[str]) -> bool:
        """KRX ê°œì„  ì‹œì‘ ì•Œë¦¼ (ì²« ë²ˆì§¸ ë°˜ë³µì—ì„œë§Œ)"""
        if iteration == 1:  # ì²« ë²ˆì§¸ ë°˜ë³µì—ì„œë§Œ ì•Œë¦¼
            message = f"""ğŸ”„ <b>KRX ë°ì´í„° ìë™ ê°œì„  ì‹œì‘</b>

ğŸ“‹ <b>ì ìš© ì „ëµ:</b>
{chr(10).join(f'â€¢ {strategy}' for strategy in strategies)}

â±ï¸ ìë™ ê°œì„  ì§„í–‰ ì¤‘..."""
            return await self.send_message(message, force=True)
        return True
    
    async def send_krx_improvement_complete(self, iteration: int, analysis: Dict[str, Any]) -> bool:
        """KRX ê°œì„  ì™„ë£Œ ì•Œë¦¼ (ëª©í‘œ ë‹¬ì„± ì‹œì—ë§Œ)"""
        is_excellent = analysis.get('performance_grade', '').startswith('ğŸŸ¢')
        avg_r2 = analysis.get('avg_r2', 0)
        
        if is_excellent and avg_r2 > 0.8:
            message = f"""âœ… <b>KRX ë°ì´í„° ìë™ ê°œì„  ì™„ë£Œ</b>

ğŸ“Š <b>ìµœì¢… ê²°ê³¼:</b>
â€¢ í‰ê·  RÂ²: {analysis.get('avg_r2', 0):.6f}
â€¢ ì„±ëŠ¥ ë“±ê¸‰: {analysis.get('performance_grade', 'N/A')}
â€¢ ìš°ìˆ˜ ì„±ëŠ¥ Fold: {analysis.get('excellent_folds', 0)}ê°œ

ğŸ‰ ëª©í‘œ ë‹¬ì„±! ìš°ìˆ˜ ì„±ëŠ¥ ë‹¬ì„±"""
            return await self.send_message(message, force=True)
        return True
    
    def _format_krx_performance_message(self, analysis: Dict[str, Any], data_info: Dict[str, Any]) -> str:
        """KRX ì„±ëŠ¥ ë©”ì‹œì§€ í¬ë§·íŒ…"""
        message = f"""ğŸ“Š <b>KRX ë°ì´í„° ì„±ëŠ¥ í˜„í™©</b>

â€¢ í‰ê·  RÂ²: {analysis.get('avg_r2', 0):.6f}
â€¢ ì„±ëŠ¥ ë“±ê¸‰: {analysis.get('performance_grade', 'N/A')}
â€¢ ê°œì„  í•„ìš” Fold: {analysis.get('poor_folds', 0)}ê°œ
â€¢ ë°ì´í„° í¬ê¸°: {data_info.get('rows', 0)}í–‰

ğŸ”„ ìë™ ê°œì„  ì§„í–‰ ì¤‘..."""
        
        return message

async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ - ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ + ìë™ ê°œì„ """
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    config = SystemConfig(mode=SystemMode.REALTIME_MONITORING)
    system = KRXUltimateSystem(config)
    
    try:
        print("KRX Ultimate System ì‹œì‘...")
        print("ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
        
        # ì‹¤ì‹œê°„ ëª¨ë‹ˆí„°ë§ ì‹œì‘
        await system.start_realtime_monitoring()
        
        # ìƒ˜í”Œ ë°ì´í„° ìƒì„± (ì‹¤ì œë¡œëŠ” ìˆ˜ì§‘ëœ ë°ì´í„° ì‚¬ìš©)
        sample_data = pd.DataFrame({
            'ì¢…ëª©ì½”ë“œ': ['005930', '000660', '035420'],
            'ì¢…ëª©ëª…': ['ì‚¼ì„±ì „ì', 'SKí•˜ì´ë‹‰ìŠ¤', 'NAVER'],
            'í˜„ì¬ê°€': [70000, 120000, 350000],
            'ë“±ë½ë¥ ': [2.5, -1.2, 0.8],
            'ê±°ë˜ëŸ‰': [1000000, 500000, 300000],
            'ì‹œê°€ì´ì•¡': [4200000, 7200000, 21000000]
        })
        
        # KRX ìë™ ê°œì„  ë£¨í”„ ì‹¤í–‰
        improvement_result = await system.auto_improvement_loop(sample_data)
        logging.info("KRX ìë™ ê°œì„  ì™„ë£Œ")
        
        # ì„±ëŠ¥ ë¶„ì„ ë° ìµœì í™” ì €ì¥
        if improvement_result and "final_analysis" in improvement_result:
            analysis = improvement_result["final_analysis"]
            
            # ì €ì¥ ê¶Œì¥ì‚¬í•­ ìƒì„±
            recommendations = await system.get_krx_storage_recommendations(sample_data, analysis)
            logging.info(f"KRX ì €ì¥ ê¶Œì¥ì‚¬í•­: {recommendations}")
            
            # ìµœì í™” ì €ì¥ ì‹¤í–‰
            output_dir = Path("krx_optimized_outputs")
            save_result = await system.save_krx_data_optimized(sample_data, analysis, output_dir)
            logging.info(f"KRX ìµœì í™” ì €ì¥ ì™„ë£Œ: {save_result}")
            
            # í…”ë ˆê·¸ë¨ ì•Œë¦¼
            telegram = KRXTelegramNotifier()
            await telegram.send_message(f"""âœ… <b>KRX ì‹œìŠ¤í…œ ì™„ë£Œ</b>

ğŸ“Š <b>ì„±ëŠ¥ ê²°ê³¼:</b>
â€¢ ì„±ëŠ¥ ë“±ê¸‰: {analysis.get('performance_grade', 'N/A')}
â€¢ ìë™ë§¤ë§¤ ì‹ ë¢°ë„: {analysis.get('trading_confidence', 'N/A')}
â€¢ ê¶Œì¥ì‚¬í•­: {analysis.get('trading_recommendation', 'N/A')}

ğŸ’¾ <b>ì €ì¥ ì •ë³´:</b>
â€¢ ì €ì¥ ê²½ë¡œ: {save_result.get('save_path', 'N/A')}
â€¢ ì €ì¥ ì „ëµ: {save_result.get('storage_strategy', {}).get('description', 'N/A')}""", force=True)
        
    except KeyboardInterrupt:
        print("\nì‚¬ìš©ìì— ì˜í•´ ì¤‘ì§€ë˜ì—ˆìŠµë‹ˆë‹¤.")
        await system.stop_realtime_monitoring()
    except Exception as e:
        print(f"ì‹œìŠ¤í…œ ì—ëŸ¬: {e}")
        await system.stop_realtime_monitoring()
    finally:
        print("ì‹œìŠ¤í…œ ì¢…ë£Œ")

if __name__ == "__main__":
    asyncio.run(main()) 