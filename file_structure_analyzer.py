"""
í”„ë¡œì íŠ¸ íŒŒì¼ êµ¬ì¡° ë¶„ì„ ë° ì •ë¦¬ ë„êµ¬
- íŒŒì¼ ì—­í•  ë¶„ì„
- ì¤‘ë³µ ì½”ë“œ íƒì§€
- í†µí•© ê°€ëŠ¥í•œ íŒŒì¼ ì‹ë³„
"""

import os
import ast
import hashlib
from pathlib import Path
from typing import Dict, List, Set, Tuple
from dataclasses import dataclass
from collections import defaultdict
import json

@dataclass
class FileInfo:
    """íŒŒì¼ ì •ë³´ í´ë˜ìŠ¤"""
    path: str
    size: int
    lines: int
    functions: List[str]
    classes: List[str]
    imports: List[str]
    main_purpose: str
    similarity_hash: str

class ProjectStructureAnalyzer:
    """í”„ë¡œì íŠ¸ êµ¬ì¡° ë¶„ì„ê¸°"""
    
    def __init__(self, project_path: str = "."):
        self.project_path = Path(project_path)
        self.files_info: Dict[str, FileInfo] = {}
        
    def analyze_project_structure(self) -> Dict[str, any]:
        """ì „ì²´ í”„ë¡œì íŠ¸ êµ¬ì¡° ë¶„ì„"""
        print("ğŸ” í”„ë¡œì íŠ¸ íŒŒì¼ êµ¬ì¡° ë¶„ì„ ì¤‘...")
        
        # Python íŒŒì¼ë“¤ë§Œ ë¶„ì„
        python_files = list(self.project_path.glob("*.py"))
        
        analysis_result = {
            'total_files': len(python_files),
            'file_categories': defaultdict(list),
            'duplicate_groups': [],
            'integration_suggestions': [],
            'cleanup_recommendations': []
        }
        
        print(f"ğŸ“ ë°œê²¬ëœ Python íŒŒì¼: {len(python_files)}ê°œ")
        
        # ê° íŒŒì¼ ë¶„ì„
        for i, file_path in enumerate(python_files, 1):
            try:
                print(f"  ğŸ“„ ë¶„ì„ ì¤‘ ({i}/{len(python_files)}): {file_path.name}")
                file_info = self.analyze_single_file(file_path)
                self.files_info[str(file_path)] = file_info
                
                # íŒŒì¼ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜
                category = self.categorize_file(file_info)
                analysis_result['file_categories'][category].append(str(file_path))
                
            except Exception as e:
                print(f"âš ï¸ íŒŒì¼ ë¶„ì„ ì‹¤íŒ¨ {file_path}: {e}")
        
        # ì¤‘ë³µ íŒŒì¼ ê·¸ë£¹ ì°¾ê¸°
        analysis_result['duplicate_groups'] = self.find_duplicate_groups()
        
        # í†µí•© ì œì•ˆ ìƒì„±
        analysis_result['integration_suggestions'] = self.generate_integration_suggestions()
        
        # ì •ë¦¬ ê¶Œì¥ì‚¬í•­
        analysis_result['cleanup_recommendations'] = self.generate_cleanup_recommendations()
        
        return analysis_result
    
    def analyze_single_file(self, file_path: Path) -> FileInfo:
        """ê°œë³„ íŒŒì¼ ë¶„ì„"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                content = f.read()
        except UnicodeDecodeError:
            # UTF-8ë¡œ ì½ê¸° ì‹¤íŒ¨ ì‹œ ë‹¤ë¥¸ ì¸ì½”ë”© ì‹œë„
            try:
                with open(file_path, 'r', encoding='cp949') as f:
                    content = f.read()
            except:
                with open(file_path, 'r', encoding='latin-1') as f:
                    content = f.read()
        
        # ê¸°ë³¸ ì •ë³´
        size = file_path.stat().st_size
        lines = len(content.splitlines())
        
        # AST íŒŒì‹±ìœ¼ë¡œ êµ¬ì¡° ë¶„ì„
        try:
            tree = ast.parse(content)
            functions = [node.name for node in ast.walk(tree) if isinstance(node, ast.FunctionDef)]
            classes = [node.name for node in ast.walk(tree) if isinstance(node, ast.ClassDef)]
            imports = self.extract_imports(tree)
        except:
            functions, classes, imports = [], [], []
        
        # íŒŒì¼ ëª©ì  ì¶”ë¡ 
        main_purpose = self.infer_file_purpose(file_path.name, content, functions, classes)
        
        # ìœ ì‚¬ì„± í•´ì‹œ (ì¤‘ë³µ íƒì§€ìš©)
        similarity_hash = self.calculate_similarity_hash(content)
        
        return FileInfo(
            path=str(file_path),
            size=size,
            lines=lines,
            functions=functions,
            classes=classes,
            imports=imports,
            main_purpose=main_purpose,
            similarity_hash=similarity_hash
        )
    
    def extract_imports(self, tree: ast.AST) -> List[str]:
        """import ë¬¸ ì¶”ì¶œ"""
        imports = []
        for node in ast.walk(tree):
            if isinstance(node, ast.Import):
                for alias in node.names:
                    imports.append(alias.name)
            elif isinstance(node, ast.ImportFrom):
                module = node.module or ""
                for alias in node.names:
                    imports.append(f"{module}.{alias.name}")
        return imports
    
    def infer_file_purpose(self, filename: str, content: str, functions: List[str], classes: List[str]) -> str:
        """íŒŒì¼ ëª©ì  ì¶”ë¡ """
        filename_lower = filename.lower()
        content_lower = content.lower()
        
        # íŒŒì¼ëª… ê¸°ë°˜ ë¶„ë¥˜
        if 'test' in filename_lower:
            return "í…ŒìŠ¤íŠ¸"
        elif 'config' in filename_lower or 'setting' in filename_lower:
            return "ì„¤ì •"
        elif 'util' in filename_lower or 'helper' in filename_lower:
            return "ìœ í‹¸ë¦¬í‹°"
        elif 'api' in filename_lower:
            return "API ì—°ë™"
        elif 'trading' in filename_lower or 'trade' in filename_lower:
            return "íŠ¸ë ˆì´ë”© ë¡œì§"
        elif 'analyzer' in filename_lower or 'analysis' in filename_lower:
            return "ë¶„ì„ ë„êµ¬"
        elif 'bot' in filename_lower or 'telegram' in filename_lower:
            return "ë´‡/ì•Œë¦¼"
        elif 'quality' in filename_lower or 'refactor' in filename_lower:
            return "ì½”ë“œ í’ˆì§ˆ"
        elif 'main' in filename_lower or filename_lower == 'app.py':
            return "ë©”ì¸ ì‹¤í–‰"
        elif 'debug' in filename_lower:
            return "ë””ë²„ê¹… ë„êµ¬"
        elif 'chart' in filename_lower or 'graph' in filename_lower:
            return "ì°¨íŠ¸/ì‹œê°í™”"
        elif 'backtest' in filename_lower:
            return "ë°±í…ŒìŠ¤íŒ…"
        elif 'logger' in filename_lower or 'log' in filename_lower:
            return "ë¡œê¹…"
        elif 'screener' in filename_lower or 'screen' in filename_lower:
            return "ì¢…ëª© ìŠ¤í¬ë¦¬ë‹"
        elif 'cycle' in filename_lower:
            return "ì‚¬ì´í´ ë¶„ì„"
        elif 'fetch' in filename_lower:
            return "ë°ì´í„° ìˆ˜ì§‘"
        elif 'spread' in filename_lower:
            return "ìŠ¤í”„ë ˆë“œ ë¶„ì„"
        elif 'throttle' in filename_lower:
            return "ì†ë„ ì œí•œ"
        elif 'system' in filename_lower:
            return "ì‹œìŠ¤í…œ ê´€ë¦¬"
        elif 'mixed' in filename_lower:
            return "í˜¼í•© ì „ëµ"
        elif 'client' in filename_lower:
            return "í´ë¼ì´ì–¸íŠ¸"
        elif 'sheets' in filename_lower:
            return "ì‹œíŠ¸ ì—°ë™"
        elif 'gemini' in filename_lower:
            return "AI ì—°ë™"
        
        # ë‚´ìš© ê¸°ë°˜ ë¶„ë¥˜
        if 'class' in content_lower and len(classes) > 0:
            if 'trading' in content_lower or 'stock' in content_lower:
                return "íŠ¸ë ˆì´ë”© í´ë˜ìŠ¤"
            elif 'analyzer' in content_lower or 'analysis' in content_lower:
                return "ë¶„ì„ í´ë˜ìŠ¤"
            else:
                return "ì¼ë°˜ í´ë˜ìŠ¤"
        elif len(functions) > 5:
            return "í•¨ìˆ˜ ëª¨ìŒ"
        elif 'if __name__' in content:
            return "ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸"
        else:
            return "ê¸°íƒ€"
    
    def calculate_similarity_hash(self, content: str) -> str:
        """ì½”ë“œ ìœ ì‚¬ì„± í•´ì‹œ ê³„ì‚°"""
        # ê³µë°±, ì£¼ì„ ì œê±° í›„ í•´ì‹œ
        lines = []
        for line in content.splitlines():
            line = line.strip()
            if line and not line.startswith('#') and not line.startswith('"""') and not line.startswith("'''"):
                lines.append(line)
        
        normalized_content = '\n'.join(lines)
        return hashlib.md5(normalized_content.encode()).hexdigest()[:8]
    
    def categorize_file(self, file_info: FileInfo) -> str:
        """íŒŒì¼ ì¹´í…Œê³ ë¦¬ ë¶„ë¥˜"""
        return file_info.main_purpose
    
    def find_duplicate_groups(self) -> List[List[str]]:
        """ì¤‘ë³µ íŒŒì¼ ê·¸ë£¹ ì°¾ê¸°"""
        hash_groups = defaultdict(list)
        
        for file_path, file_info in self.files_info.items():
            if file_info.lines > 5:  # ë„ˆë¬´ ì‘ì€ íŒŒì¼ì€ ì œì™¸
                hash_groups[file_info.similarity_hash].append(file_path)
        
        # 2ê°œ ì´ìƒì¸ ê·¸ë£¹ë§Œ ë°˜í™˜
        return [group for group in hash_groups.values() if len(group) > 1]
    
    def generate_integration_suggestions(self) -> List[Dict[str, any]]:
        """í†µí•© ì œì•ˆ ìƒì„±"""
        suggestions = []
        
        # ì¹´í…Œê³ ë¦¬ë³„ íŒŒì¼ ê·¸ë£¹í•‘
        categories = defaultdict(list)
        for file_path, file_info in self.files_info.items():
            categories[file_info.main_purpose].append((file_path, file_info))
        
        for category, files in categories.items():
            if len(files) > 2 and category not in ['ë©”ì¸ ì‹¤í–‰', 'ì„¤ì •']:  # 3ê°œ ì´ìƒì¸ ì¹´í…Œê³ ë¦¬
                suggestions.append({
                    'type': 'ì¹´í…Œê³ ë¦¬ í†µí•©',
                    'category': category,
                    'files': [f[0] for f in files],
                    'reason': f'{category} ê´€ë ¨ íŒŒì¼ë“¤ì„ í•˜ë‚˜ì˜ ëª¨ë“ˆë¡œ í†µí•©',
                    'suggested_name': self.suggest_module_name(category)
                })
        
        # ì‘ì€ íŒŒì¼ë“¤ í†µí•© ì œì•ˆ
        small_files = [(path, info) for path, info in self.files_info.items() 
                      if info.lines < 50 and info.main_purpose not in ['ë©”ì¸ ì‹¤í–‰', 'ì„¤ì •', 'ë””ë²„ê¹… ë„êµ¬']]
        
        if len(small_files) > 3:
            suggestions.append({
                'type': 'ì†Œí˜• íŒŒì¼ í†µí•©',
                'files': [f[0] for f in small_files],
                'reason': '50ì¤„ ë¯¸ë§Œì˜ ì‘ì€ íŒŒì¼ë“¤ì„ utils ëª¨ë“ˆë¡œ í†µí•©',
                'suggested_name': 'utils.py'
            })
        
        return suggestions
    
    def suggest_module_name(self, category: str) -> str:
        """ëª¨ë“ˆëª… ì œì•ˆ"""
        name_mapping = {
            'íŠ¸ë ˆì´ë”© ë¡œì§': 'trading_core.py',
            'ë¶„ì„ ë„êµ¬': 'analyzers.py',
            'API ì—°ë™': 'api_clients.py',
            'ë´‡/ì•Œë¦¼': 'notification_bots.py',
            'ì½”ë“œ í’ˆì§ˆ': 'quality_tools.py',
            'ìœ í‹¸ë¦¬í‹°': 'utils.py',
            'í…ŒìŠ¤íŠ¸': 'test_suite.py',
            'ì°¨íŠ¸/ì‹œê°í™”': 'visualization.py',
            'ë°±í…ŒìŠ¤íŒ…': 'backtesting.py',
            'ë¡œê¹…': 'logging_utils.py',
            'ì¢…ëª© ìŠ¤í¬ë¦¬ë‹': 'stock_screening.py',
            'ì‚¬ì´í´ ë¶„ì„': 'cycle_analysis.py',
            'ë°ì´í„° ìˆ˜ì§‘': 'data_fetchers.py',
            'ìŠ¤í”„ë ˆë“œ ë¶„ì„': 'spread_analysis.py',
            'ì†ë„ ì œí•œ': 'throttling.py',
            'ì‹œìŠ¤í…œ ê´€ë¦¬': 'system_management.py',
            'í˜¼í•© ì „ëµ': 'mixed_strategies.py',
            'í´ë¼ì´ì–¸íŠ¸': 'clients.py',
            'ì‹œíŠ¸ ì—°ë™': 'sheet_integration.py',
            'AI ì—°ë™': 'ai_integration.py'
        }
        return name_mapping.get(category, f'{category.lower().replace(" ", "_").replace("/", "_")}.py')
    
    def generate_cleanup_recommendations(self) -> List[str]:
        """ì •ë¦¬ ê¶Œì¥ì‚¬í•­ ìƒì„±"""
        recommendations = []
        
        # ë¹ˆ íŒŒì¼ì´ë‚˜ ê±°ì˜ ë¹ˆ íŒŒì¼
        empty_files = [path for path, info in self.files_info.items() if info.lines < 5]
        if empty_files:
            recommendations.append(f"ğŸ—‘ï¸ ë¹ˆ íŒŒì¼ ì‚­ì œ ê¶Œì¥: {len(empty_files)}ê°œ")
        
        # ì¤‘ë³µ íŒŒì¼
        duplicate_groups = self.find_duplicate_groups()
        if duplicate_groups:
            recommendations.append(f"ğŸ”„ ì¤‘ë³µ íŒŒì¼ ì •ë¦¬ í•„ìš”: {len(duplicate_groups)}ê°œ ê·¸ë£¹")
        
        # ë„ˆë¬´ í° íŒŒì¼
        large_files = [path for path, info in self.files_info.items() if info.lines > 500]
        if large_files:
            recommendations.append(f"ğŸ“„ í° íŒŒì¼ ë¶„í•  ê³ ë ¤: {len(large_files)}ê°œ (500ì¤„ ì´ìƒ)")
        
        # í•¨ìˆ˜ê°€ ë„ˆë¬´ ë§ì€ íŒŒì¼
        function_heavy = [path for path, info in self.files_info.items() if len(info.functions) > 20]
        if function_heavy:
            recommendations.append(f"ğŸ”§ í•¨ìˆ˜ ê³¼ë‹¤ íŒŒì¼ ëª¨ë“ˆí™”: {len(function_heavy)}ê°œ (20ê°œ ì´ìƒ)")
        
        # í´ë˜ìŠ¤ê°€ ì—†ëŠ” í° íŒŒì¼
        no_class_large = [path for path, info in self.files_info.items() 
                         if info.lines > 200 and len(info.classes) == 0 and len(info.functions) > 10]
        if no_class_large:
            recommendations.append(f"ğŸ—ï¸ í´ë˜ìŠ¤ êµ¬ì¡°í™” ê¶Œì¥: {len(no_class_large)}ê°œ íŒŒì¼")
        
        return recommendations

def print_analysis_report(analysis: Dict[str, any]):
    """ë¶„ì„ ê²°ê³¼ ë¦¬í¬íŠ¸ ì¶œë ¥"""
    print("\n" + "="*80)
    print("ğŸ“Š í”„ë¡œì íŠ¸ êµ¬ì¡° ë¶„ì„ ê²°ê³¼")
    print("="*80)
    
    print(f"\nğŸ“ ì „ì²´ íŒŒì¼ ìˆ˜: {analysis['total_files']}ê°œ")
    
    # ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜
    print(f"\nğŸ“‹ íŒŒì¼ ì¹´í…Œê³ ë¦¬ë³„ ë¶„ë¥˜:")
    total_categorized = 0
    for category, files in sorted(analysis['file_categories'].items()):
        print(f"  ğŸ“‚ {category}: {len(files)}ê°œ")
        total_categorized += len(files)
        for file in files[:3]:  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
            print(f"    - {Path(file).name}")
        if len(files) > 3:
            print(f"    ... ì™¸ {len(files)-3}ê°œ")
    
    # ì¤‘ë³µ íŒŒì¼ ê·¸ë£¹
    if analysis['duplicate_groups']:
        print(f"\nğŸ”„ ì¤‘ë³µ íŒŒì¼ ê·¸ë£¹: {len(analysis['duplicate_groups'])}ê°œ")
        for i, group in enumerate(analysis['duplicate_groups'], 1):
            print(f"  ê·¸ë£¹ {i}: {[Path(f).name for f in group]}")
    else:
        print(f"\nâœ… ì¤‘ë³µ íŒŒì¼ ì—†ìŒ")
    
    # í†µí•© ì œì•ˆ
    if analysis['integration_suggestions']:
        print(f"\nğŸ’¡ í†µí•© ì œì•ˆ: {len(analysis['integration_suggestions'])}ê°œ")
        for suggestion in analysis['integration_suggestions']:
            print(f"  ğŸ“¦ {suggestion['type']}: {suggestion['suggested_name']}")
            print(f"     ëŒ€ìƒ: {len(suggestion['files'])}ê°œ íŒŒì¼")
            print(f"     ì´ìœ : {suggestion['reason']}")
    else:
        print(f"\nâœ… ì¶”ê°€ í†µí•© ì œì•ˆ ì—†ìŒ")
    
    # ì •ë¦¬ ê¶Œì¥ì‚¬í•­
    if analysis['cleanup_recommendations']:
        print(f"\nğŸ§¹ ì •ë¦¬ ê¶Œì¥ì‚¬í•­:")
        for rec in analysis['cleanup_recommendations']:
            print(f"  {rec}")
    else:
        print(f"\nâœ… ì •ë¦¬í•  í•­ëª© ì—†ìŒ")
    
    # ì •ë¦¬ íš¨ê³¼ ì˜ˆìƒ
    print(f"\nğŸ“ˆ ì •ë¦¬ íš¨ê³¼ ì˜ˆìƒ:")
    current_files = analysis['total_files']
    
    # ì¤‘ë³µ ì œê±° íš¨ê³¼
    duplicate_reduction = sum(len(group) - 1 for group in analysis['duplicate_groups'])
    
    # í†µí•© íš¨ê³¼
    integration_reduction = 0
    for suggestion in analysis['integration_suggestions']:
        if suggestion['type'] == 'ì†Œí˜• íŒŒì¼ í†µí•©':
            integration_reduction += len(suggestion['files']) - 1
        elif suggestion['type'] == 'ì¹´í…Œê³ ë¦¬ í†µí•©':
            integration_reduction += len(suggestion['files']) - 1
    
    estimated_final = current_files - duplicate_reduction - integration_reduction
    reduction_percent = ((current_files - estimated_final) / current_files * 100) if current_files > 0 else 0
    
    print(f"  ğŸ“‰ íŒŒì¼ ìˆ˜ ê°ì†Œ: {current_files}ê°œ â†’ {estimated_final}ê°œ ({reduction_percent:.1f}% ê°ì†Œ)")
    print(f"  ğŸ—‘ï¸ ì¤‘ë³µ ì œê±°: {duplicate_reduction}ê°œ")
    print(f"  ğŸ“¦ í†µí•© íš¨ê³¼: {integration_reduction}ê°œ")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    try:
        print("ğŸš€ í”„ë¡œì íŠ¸ íŒŒì¼ êµ¬ì¡° ë¶„ì„ ì‹œì‘")
        print("="*50)
        
        analyzer = ProjectStructureAnalyzer()
        analysis = analyzer.analyze_project_structure()
        
        # ê²°ê³¼ ì¶œë ¥
        print_analysis_report(analysis)
        
        # JSON íŒŒì¼ë¡œ ì €ì¥
        with open('project_structure_analysis.json', 'w', encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ ìƒì„¸ ë¶„ì„ ê²°ê³¼ê°€ 'project_structure_analysis.json'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # ì‚¬ìš©ì ì„ íƒ
        print(f"\nğŸ¯ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ì„ íƒí•˜ì„¸ìš”:")
        print(f"1. ìë™ íŒŒì¼ ì •ë¦¬ ì‹¤í–‰")
        print(f"2. ìˆ˜ë™ ê²€í†  í›„ ì •ë¦¬")
        print(f"3. ë¶„ì„ë§Œ í•˜ê³  ì¢…ë£Œ")
        
        choice = input("ì„ íƒ (1-3): ").strip()
        
        if choice == '1':
            print("ğŸš€ ìë™ íŒŒì¼ ì •ë¦¬ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            # ìë™ ì •ë¦¬ ì‹¤í–‰
            import subprocess
            try:
                result = subprocess.run(['python', 'auto_file_organizer.py'], 
                                      capture_output=True, text=True, encoding='utf-8')
                if result.returncode == 0:
                    print("âœ… ìë™ ì •ë¦¬ ì™„ë£Œ!")
                    print(result.stdout)
                else:
                    print("âŒ ìë™ ì •ë¦¬ ì‹¤íŒ¨:")
                    print(result.stderr)
            except Exception as e:
                print(f"âŒ ìë™ ì •ë¦¬ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
                print("ğŸ’¡ ìˆ˜ë™ìœ¼ë¡œ 'python auto_file_organizer.py'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
                
        elif choice == '2':
            print("ğŸ“‹ ìˆ˜ë™ ê²€í†  ëª¨ë“œì…ë‹ˆë‹¤.")
            print("ğŸ’¡ 'project_structure_analysis.json' íŒŒì¼ì„ í™•ì¸í•˜ê³ ")
            print("ğŸ’¡ 'python auto_file_organizer.py'ë¥¼ ì‹¤í–‰í•˜ì—¬ ì •ë¦¬í•˜ì„¸ìš”.")
        else:
            print("ğŸ“Š ë¶„ì„ ì™„ë£Œ. ê²°ê³¼ë¥¼ ê²€í† í•˜ì„¸ìš”.")
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 