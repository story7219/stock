"""
ğŸ”¥ ì‹¤ì‹œê°„ ë‰´ìŠ¤ & ê³µì‹œ ìˆ˜ì§‘ê¸° (ì†ë„ ìµœì í™” ë²„ì „)
- ë„¤ì´ë²„ ê¸ˆìœµ ì‹¤ì‹œê°„ ë‰´ìŠ¤ í¬ë¡¤ë§
- í•œêµ­ê±°ë˜ì†Œ ì „ìê³µì‹œ ìˆ˜ì§‘
- í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì •ë¶„ì„
- ì¢…ëª©ë³„ ë‰´ìŠ¤ í•„í„°ë§
"""
import requests
from bs4 import BeautifulSoup
import re
import time
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional
import concurrent.futures
from dataclasses import dataclass
import json
from urllib.parse import urlencode, quote
import threading
from collections import defaultdict

logger = logging.getLogger(__name__)

@dataclass
class NewsItem:
    """ë‰´ìŠ¤ ì•„ì´í…œ ë°ì´í„° í´ë˜ìŠ¤"""
    title: str
    content: str
    url: str
    timestamp: datetime
    source: str
    sentiment: str  # 'positive', 'negative', 'neutral'
    sentiment_score: float  # -1.0 ~ 1.0
    related_stocks: List[str] = None

@dataclass
class AnnouncementItem:
    """ê³µì‹œ ì•„ì´í…œ ë°ì´í„° í´ë˜ìŠ¤"""
    title: str
    company: str
    stock_code: str
    announcement_type: str
    timestamp: datetime
    url: str
    content: str = None
    importance: str = "medium"  # 'high', 'medium', 'low'

class NewsCollector:
    """ğŸš€ ì´ˆê³ ì† ë‰´ìŠ¤ & ê³µì‹œ ìˆ˜ì§‘ í´ë˜ìŠ¤"""
    
    def __init__(self):
        """NewsCollector ì´ˆê¸°í™”"""
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        })
        
        # ìºì‹œ ì‹œìŠ¤í…œ (5ë¶„ ìºì‹œ)
        self.cache_duration = 300  # 5ë¶„
        self.news_cache = {}
        self.announcement_cache = {}
        
        # ê°ì •ë¶„ì„ í‚¤ì›Œë“œ ì‚¬ì „
        self._init_sentiment_keywords()
        
        # ì¢…ëª©ëª…-ì½”ë“œ ë§¤í•‘ (ì£¼ìš” ì¢…ëª©)
        self._init_stock_mapping()
        
        # ë³‘ë ¬ ì²˜ë¦¬ ì„¤ì •
        self.max_workers = 4
        
        logger.info("ğŸ”¥ NewsCollector ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _init_sentiment_keywords(self):
        """ê°ì •ë¶„ì„ìš© í‚¤ì›Œë“œ ì‚¬ì „ ì´ˆê¸°í™”"""
        self.positive_keywords = {
            # ì£¼ê°€ ìƒìŠ¹ ê´€ë ¨
            'ìƒìŠ¹', 'ê¸‰ë“±', 'ê°•ì„¸', 'í˜¸ì¬', 'ì„±ì¥', 'ì¦ê°€', 'í™•ëŒ€', 'ê°œì„ ', 
            'í˜¸ì¡°', 'ë°˜ë“±', 'ëŒíŒŒ', 'ì‹ ê³ ê°€', 'ëª©í‘œê°€', 'ìƒí–¥', 'ë§¤ìˆ˜',
            # ì‹¤ì  ê´€ë ¨
            'í‘ì', 'ì¦ìµ', 'ë§¤ì¶œì¦ê°€', 'ìˆ˜ìµê°œì„ ', 'ì‹¤ì í˜¸ì¡°', 'ì–´ë‹ì„œí”„ë¼ì´ì¦ˆ',
            # ì‚¬ì—… ê´€ë ¨  
            'ì‹ ì œí’ˆ', 'ì‹ ê·œê³„ì•½', 'íˆ¬ììœ ì¹˜', 'ì œíœ´', 'í•©ë³‘', 'ì¸ìˆ˜',
            'í™•ì¥', 'ì§„ì¶œ', 'ê°œë°œì™„ë£Œ', 'ìŠ¹ì¸', 'íŠ¹í—ˆ', 'ìˆ˜ì£¼',
            # ê¸ì • ê°ì •
            'ê¸ì •ì ', 'ë‚™ê´€ì ', 'ê¸°ëŒ€', 'ì „ë§ë°ìŒ', 'ìœ ë§', 'ì„±ê³µì '
        }
        
        self.negative_keywords = {
            # ì£¼ê°€ í•˜ë½ ê´€ë ¨
            'í•˜ë½', 'ê¸‰ë½', 'í­ë½', 'ì•½ì„¸', 'ì•…ì¬', 'ê°ì†Œ', 'ì¶•ì†Œ', 'ì•…í™”',
            'ë¶€ì§„', 'ì¡°ì •', 'í•˜í–¥', 'ë§¤ë„', 'ì†ì ˆ', 'ì €ì¡°',
            # ì‹¤ì  ê´€ë ¨
            'ì ì', 'ê°ìµ', 'ë§¤ì¶œê°ì†Œ', 'ì‹¤ì ë¶€ì§„', 'ì–´ë‹ì‡¼í¬', 'ì˜ì—…ì†ì‹¤',
            # ì‚¬ì—… ê´€ë ¨
            'ì·¨ì†Œ', 'ì—°ê¸°', 'ì¤‘ë‹¨', 'ì² íšŒ', 'ì‹¤íŒ¨', 'ì†ì‹¤', 'ë¦¬ì½œ',
            'ì†Œì†¡', 'ë¶„ìŸ', 'ì œì¬', 'ê·œì œ', 'ì¡°ì‚¬', 'ìˆ˜ì‚¬',
            # ë¶€ì • ê°ì •
            'ë¶€ì •ì ', 'ë¹„ê´€ì ', 'ìš°ë ¤', 'ìœ„í—˜', 'ë¶ˆì•ˆ', 'ì‹¤ë§'
        }
    
    def _init_stock_mapping(self):
        """ì£¼ìš” ì¢…ëª© ì½”ë“œ-ì´ë¦„ ë§¤í•‘"""
        self.stock_mapping = {
            '005930': 'ì‚¼ì„±ì „ì', '000660': 'SKí•˜ì´ë‹‰ìŠ¤', '035420': 'NAVER',
            '051910': 'LGí™”í•™', '006400': 'ì‚¼ì„±SDI', '035720': 'ì¹´ì¹´ì˜¤',
            '207940': 'ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤', '005380': 'í˜„ëŒ€ì°¨', '000270': 'ê¸°ì•„',
            '068270': 'ì…€íŠ¸ë¦¬ì˜¨', '003670': 'í¬ìŠ¤ì½”í™€ë”©ìŠ¤', '096770': 'SKì´ë…¸ë² ì´ì…˜',
            '323410': 'ì¹´ì¹´ì˜¤ë±…í¬', '373220': 'LGì—ë„ˆì§€ì†”ë£¨ì…˜', '028260': 'ì‚¼ì„±ë¬¼ì‚°'
        }
        
        # ì—­ë°©í–¥ ë§¤í•‘ë„ ìƒì„±
        self.company_to_code = {v: k for k, v in self.stock_mapping.items()}
    
    def get_realtime_news(self, keywords: Optional[List[str]] = None, limit: int = 20) -> List[NewsItem]:
        """ğŸ“° ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘ (ë‹¤ì¤‘ ì†ŒìŠ¤)"""
        cache_key = f"news_{keywords}_{limit}"
        
        # ìºì‹œ í™•ì¸
        if self._is_cache_valid(cache_key, 'news'):
            logger.info("ğŸ“‹ ë‰´ìŠ¤ ìºì‹œ ì‚¬ìš©")
            return self.news_cache[cache_key]['data']
        
        try:
            start_time = time.time()
            logger.info("ğŸ“° ì‹¤ì‹œê°„ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹œì‘...")
            
            news_list = []
            
            # ë³‘ë ¬ë¡œ ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ë‰´ìŠ¤ ìˆ˜ì§‘
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = []
                
                # 1. ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ (70%)
                futures.append(executor.submit(self._get_naver_finance_news, int(limit * 0.7)))
                
                # 2. í•œêµ­ê²½ì œ ë‰´ìŠ¤ (20%)
                futures.append(executor.submit(self._get_hankyung_news, int(limit * 0.2)))
                
                # 3. ì´ë°ì¼ë¦¬ ë‰´ìŠ¤ (10%)
                futures.append(executor.submit(self._get_edaily_news, int(limit * 0.1)))
                
                # ê²°ê³¼ ìˆ˜ì§‘
                for future in concurrent.futures.as_completed(futures, timeout=10):
                    try:
                        result = future.result()
                        if result:
                            news_list.extend(result)
                    except Exception as e:
                        logger.warning(f"âš ï¸ ë‰´ìŠ¤ ì†ŒìŠ¤ ì‹¤íŒ¨: {e}")
                        continue
            
            # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
            seen_titles = set()
            unique_news = []
            for news in news_list:
                if news.title not in seen_titles:
                    seen_titles.add(news.title)
                    unique_news.append(news)
            
            # 2. í‚¤ì›Œë“œ í•„í„°ë§ (ì„ íƒì‚¬í•­)
            if keywords:
                unique_news = self._filter_by_keywords(unique_news, keywords)
            
            # 3. ê°ì •ë¶„ì„ ì ìš©
            with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_workers) as executor:
                futures = [executor.submit(self._analyze_sentiment_item, item) for item in unique_news]
                analyzed_news = []
                
                for future in concurrent.futures.as_completed(futures, timeout=10):
                    try:
                        result = future.result()
                        if result:
                            analyzed_news.append(result)
                    except Exception as e:
                        logger.warning(f"âš ï¸ ê°ì •ë¶„ì„ ì‹¤íŒ¨: {e}")
                        continue
            
            # 4. ì¤‘ìš”ë„ìˆœ ì •ë ¬ (ê°ì •ì ìˆ˜ ê¸°ì¤€)
            analyzed_news.sort(key=lambda x: abs(x.sentiment_score), reverse=True)
            
            # ìºì‹œ ì €ì¥
            self._cache_data(cache_key, 'news', analyzed_news[:limit])
            
            elapsed = time.time() - start_time
            logger.info(f"ğŸ“° ë‰´ìŠ¤ ìˆ˜ì§‘ ì™„ë£Œ: {len(analyzed_news)}ê°œ, {elapsed:.3f}ì´ˆ")
            
            return analyzed_news[:limit]
            
        except Exception as e:
            logger.error(f"âŒ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    def _get_hankyung_news(self, limit: int) -> List[NewsItem]:
        """í•œêµ­ê²½ì œ(hankyung.com) ì¦ê¶Œ ë‰´ìŠ¤ í¬ë¡¤ë§"""
        news_list = []
        url = "https://www.hankyung.com/finance"
        try:
            response = self.session.get(url, timeout=8)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            items = soup.select('.news-list .news-item a')
            for item in items[:limit]:
                try:
                    title = item.select_one('h3.news-tit').get_text(strip=True)
                    link = item.get('href', '')
                    if not link.startswith('http'):
                        link = "https://www.hankyung.com" + link
                    
                    news_list.append(NewsItem(
                        title=title, content='', url=link, timestamp=datetime.now(),
                        source='í•œêµ­ê²½ì œ', sentiment='neutral', sentiment_score=0.0
                    ))
                    time.sleep(0.1) # í¬ë¡¤ë§ ì˜ˆì˜ ì¤€ìˆ˜
                except Exception:
                    continue
        except Exception as e:
            logger.warning(f"âš ï¸ í•œêµ­ê²½ì œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return news_list

    def _get_edaily_news(self, limit: int) -> List[NewsItem]:
        """ì´ë°ì¼ë¦¬(edaily.co.kr) ì¦ê¶Œ ë‰´ìŠ¤ í¬ë¡¤ë§"""
        news_list = []
        url = "https://www.edaily.co.kr/news/stock"
        try:
            response = self.session.get(url, timeout=8)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            items = soup.select('.news-list a.news-title')
            for item in items[:limit]:
                try:
                    title = item.get_text(strip=True)
                    link = item.get('href', '')
                    if not link.startswith('http'):
                        link = "https://www.edaily.co.kr" + link

                    news_list.append(NewsItem(
                        title=title, content='', url=link, timestamp=datetime.now(),
                        source='ì´ë°ì¼ë¦¬', sentiment='neutral', sentiment_score=0.0
                    ))
                    time.sleep(0.1) # í¬ë¡¤ë§ ì˜ˆì˜ ì¤€ìˆ˜
                except Exception:
                    continue
        except Exception as e:
            logger.warning(f"âš ï¸ ì´ë°ì¼ë¦¬ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        return news_list
    
    def _get_naver_finance_news(self, limit: int) -> List[NewsItem]:
        """ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ í¬ë¡¤ë§ (DART ê³µì‹œ ì—°ê³„ ë° 30ì¼ í•„í„°ë§ ê°•í™”)"""
        news_list = []
        url = "https://finance.naver.com/news/mainnews.naver"
        try:
            response = self.session.get(url, timeout=8)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')

            # ë‰´ìŠ¤ ëª©ë¡ ì•„ì´í…œ ì„ íƒì ìˆ˜ì • (ë” êµ¬ì²´ì ìœ¼ë¡œ)
            news_items = soup.select('.mainNewsList li')
            
            thirty_days_ago = datetime.now() - timedelta(days=30)

            for item in news_items:
                if len(news_list) >= limit:
                    break
                
                try:
                    dt_span = item.select_one('span.date')
                    if not dt_span: continue
                    
                    # ë‚ ì§œ íŒŒì‹± ë° í•„í„°ë§
                    news_date_str = dt_span.get_text(strip=True)
                    news_dt = self._parse_naver_time(news_date_str)
                    
                    if news_dt < thirty_days_ago:
                        continue # 30ì¼ ì´ì „ ë‰´ìŠ¤ëŠ” ê±´ë„ˆë›°ê¸°

                    title_tag = item.select_one('dd a')
                    if not title_tag: continue
                    
                    title = title_tag.get_text(strip=True)
                    link = "https://finance.naver.com" + title_tag['href']

                    # DART ê³µì‹œ ê´€ë ¨ ë‰´ìŠ¤ ê°€ì¤‘ì¹˜ ë¶€ì—¬
                    sentiment_score = 0.0
                    if 'ê³µì‹œ' in title or '[ìœ ê°€ì¦ê¶Œ]' in title or '[ì½”ìŠ¤ë‹¥]' in title:
                        sentiment_score = 0.1 # ê¸°ë³¸ ê°€ì¤‘ì¹˜

                    news_list.append(NewsItem(
                        title=title, content='', url=link, timestamp=news_dt,
                        source='ë„¤ì´ë²„ê¸ˆìœµ', sentiment='neutral', sentiment_score=sentiment_score
                    ))
                    time.sleep(0.1) # í¬ë¡¤ë§ ì˜ˆì˜ ì¤€ìˆ˜
                except Exception:
                    continue
                    
        except Exception as e:
            logger.warning(f"âš ï¸ ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            
        return news_list
    
    def _get_naver_stock_news(self, limit: int) -> List[NewsItem]:
        """ë„¤ì´ë²„ ì¦ê¶Œ ë‰´ìŠ¤ í¬ë¡¤ë§ (ë³´ì¡° ì†ŒìŠ¤)"""
        news_list = []
        
        try:
            # ë„¤ì´ë²„ ì¦ê¶Œ ì‹¤ì‹œê°„ ë‰´ìŠ¤
            url = "https://finance.naver.com/sise/sise_market_sum.naver"
            
            response = self.session.get(url, timeout=8)
            response.raise_for_status()
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì¢…ëª© ë‰´ìŠ¤ ë§í¬ ì°¾ê¸°
            stock_links = soup.select('a[href*="item.naver"]')
            
            for link in stock_links[:limit]:
                try:
                    title = link.get_text(strip=True)
                    if not title or len(title) < 5:
                        continue
                    
                    href = link.get('href', '')
                    if href and not href.startswith('http'):
                        href = 'https://finance.naver.com' + href
                    
                    news_item = NewsItem(
                        title=f"[ì¦ê¶Œ] {title}",
                        content='',
                        url=href,
                        timestamp=datetime.now(),
                        source='ë„¤ì´ë²„ì¦ê¶Œ',
                        sentiment='neutral',
                        sentiment_score=0.0
                    )
                    
                    news_list.append(news_item)
                    
                except Exception as e:
                    continue
            
            logger.info(f"ğŸ“° ë„¤ì´ë²„ ì¦ê¶Œ ë‰´ìŠ¤ {len(news_list)}ê°œ ìˆ˜ì§‘")
            
        except Exception as e:
            logger.warning(f"âš ï¸ ë„¤ì´ë²„ ì¦ê¶Œ ë‰´ìŠ¤ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        return news_list
    
    def _get_sample_news(self, limit: int) -> List[NewsItem]:
        """ìƒ˜í”Œ ë‰´ìŠ¤ ìƒì„± (í…ŒìŠ¤íŠ¸/ë°ëª¨ìš©)"""
        sample_titles = [
            "ì‚¼ì„±ì „ì, 3ë¶„ê¸° ì‹¤ì  í˜¸ì¡°ë¡œ ëª©í‘œê°€ ìƒí–¥ ì¡°ì •",
            "ì½”ìŠ¤í”¼, ì™¸êµ­ì¸ ë§¤ìˆ˜ì„¸ì— í˜ì…ì–´ ìƒìŠ¹ì„¸ ì§€ì†",
            "SKí•˜ì´ë‹‰ìŠ¤, ë©”ëª¨ë¦¬ ë°˜ë„ì²´ ìˆ˜ìš” ì¦ê°€ë¡œ ì£¼ê°€ ê¸‰ë“±",
            "LGí™”í•™, ë°°í„°ë¦¬ ì‚¬ì—… í™•ì¥ìœ¼ë¡œ ì„±ì¥ ì „ë§ ë°ì•„",
            "ì¹´ì¹´ì˜¤, ìƒˆë¡œìš´ í”Œë«í¼ ì„œë¹„ìŠ¤ ì¶œì‹œ ë°œí‘œ",
            "í˜„ëŒ€ì°¨, ì „ê¸°ì°¨ íŒë§¤ ëª©í‘œ ìƒí–¥ ì¡°ì •",
            "NAVER, AI ê¸°ìˆ  íˆ¬ì í™•ëŒ€ ê³„íš ë°œí‘œ",
            "ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤, ì‹ ê·œ ê³„ì•½ ì²´ê²°ë¡œ ìˆ˜ì£¼ ì¦ê°€",
            "í¬ìŠ¤ì½”í™€ë”©ìŠ¤, ì¹œí™˜ê²½ ì² ê°• ê¸°ìˆ  ê°œë°œ ì„±ê³µ",
            "ì…€íŠ¸ë¦¬ì˜¨, ë°”ì´ì˜¤ì‹œë°€ëŸ¬ ë§¤ì¶œ ì¦ê°€ì„¸"
        ]
        
        news_list = []
        
        for i, title in enumerate(sample_titles[:limit]):
            news_item = NewsItem(
                title=title,
                content=f"{title}ì— ëŒ€í•œ ìƒì„¸ ë‚´ìš©...",
                url=f"https://example.com/news/{i+1}",
                timestamp=datetime.now() - timedelta(minutes=i*5),
                source='ìƒ˜í”Œë‰´ìŠ¤',
                sentiment='neutral',
                sentiment_score=0.0
            )
            news_list.append(news_item)
        
        logger.info(f"ğŸ“° ìƒ˜í”Œ ë‰´ìŠ¤ {len(news_list)}ê°œ ìƒì„±")
        return news_list
    
    def get_announcements(self, days: int = 1) -> List[AnnouncementItem]:
        """ğŸ“‹ ì „ìê³µì‹œ ìˆ˜ì§‘ (ê°œì„ ëœ ë²„ì „)"""
        cache_key = f"announcements_{days}"
        
        # ìºì‹œ í™•ì¸
        if self._is_cache_valid(cache_key, 'announcement'):
            logger.info("ğŸ“‹ ê³µì‹œ ìºì‹œ ì‚¬ìš©")
            return self.announcement_cache[cache_key]['data']
        
        try:
            start_time = time.time()
            logger.info("ğŸ“‹ ì „ìê³µì‹œ ìˆ˜ì§‘ ì‹œì‘...")
            
            announcements = []
            
            # ë³‘ë ¬ë¡œ ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ê³µì‹œ ìˆ˜ì§‘
            with concurrent.futures.ThreadPoolExecutor(max_workers=2) as executor:
                futures = []
                
                # 1. KIND ê³µì‹œ (ìˆ˜ì •ëœ URL)
                futures.append(executor.submit(self._get_kind_announcements_v2, days))
                
                # 2. ìƒ˜í”Œ ê³µì‹œ ìƒì„±
                futures.append(executor.submit(self._get_sample_announcements, 10))
                
                # ê²°ê³¼ ìˆ˜ì§‘
                for future in concurrent.futures.as_completed(futures, timeout=15):
                    try:
                        result = future.result()
                        if result:
                            announcements.extend(result)
                    except Exception as e:
                        logger.warning(f"âš ï¸ ê³µì‹œ ì†ŒìŠ¤ ì‹¤íŒ¨: {e}")
                        continue
            
            # ì¤‘ë³µ ì œê±° (ì œëª© ê¸°ì¤€)
            seen_titles = set()
            unique_announcements = []
            for ann in announcements:
                if ann.title not in seen_titles:
                    seen_titles.add(ann.title)
                    unique_announcements.append(ann)
            
            # ì¤‘ìš”ë„ë³„ ì •ë ¬
            unique_announcements.sort(key=lambda x: (
                x.importance == 'high' and 3 or 
                x.importance == 'medium' and 2 or 1, 
                x.timestamp
            ), reverse=True)
            
            # ìºì‹œ ì €ì¥
            self._cache_data(cache_key, 'announcement', unique_announcements)
            
            elapsed = time.time() - start_time
            logger.info(f"ğŸ“‹ ê³µì‹œ ìˆ˜ì§‘ ì™„ë£Œ: {len(unique_announcements)}ê°œ, {elapsed:.3f}ì´ˆ")
            
            return unique_announcements
            
        except Exception as e:
            logger.error(f"âŒ ê³µì‹œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            return []
    
    def _get_kind_announcements_v2(self, days: int) -> List[AnnouncementItem]:
        """KIND ê³µì‹œ í¬ë¡¤ë§ (ê°œì„ ëœ ë²„ì „)"""
        announcements = []
        
        try:
            # KIND ìƒˆë¡œìš´ API ì—”ë“œí¬ì¸íŠ¸ë“¤ ì‹œë„
            endpoints = [
                "https://kind.krx.co.kr/common/disclsviewer.do",
                "https://kind.krx.co.kr/disclosureservice/disclosureservice.do",
                "https://opendart.fss.or.kr/api/list.json"  # ëŒ€ì²´ ê³µì‹œ API
            ]
            
            from_date = (datetime.now() - timedelta(days=days)).strftime('%Y%m%d')
            to_date = datetime.now().strftime('%Y%m%d')
            
            for base_url in endpoints[:1]:  # ì²« ë²ˆì§¸ë§Œ ì‹œë„
                try:
                    params = {
                        'method': 'searchDisclosureSummary',
                        'currentPageSize': '50',
                        'pageIndex': '1',
                        'orderMode': '0',
                        'orderStat': 'D',
                        'forward': 'disclosuresummary',
                        'elDate': f"{from_date}~{to_date}"
                    }
                    
                    response = self.session.get(base_url, params=params, timeout=10)
                    
                    if response.status_code == 200:
                        soup = BeautifulSoup(response.text, 'html.parser')
                        
                        # í…Œì´ë¸” í˜•íƒœ íŒŒì‹± ì‹œë„
                        rows = soup.select('tr')
                        
                        for row in rows[1:11]:  # í—¤ë” ì œì™¸í•˜ê³  ìµœëŒ€ 10ê°œ
                            try:
                                cols = row.select('td')
                                if len(cols) >= 3:
                                    # ê¸°ë³¸ ì •ë³´ë§Œ ì¶”ì¶œ
                                    date_str = cols[0].get_text(strip=True) if cols[0] else ''
                                    company = cols[1].get_text(strip=True) if len(cols) > 1 else 'íšŒì‚¬ëª…'
                                    title = cols[2].get_text(strip=True) if len(cols) > 2 else 'ê³µì‹œì œëª©'
                                    
                                    if title and len(title) > 5:
                                        announcement = AnnouncementItem(
                                            title=title,
                                            company=company,
                                            stock_code=self._extract_stock_code(company),
                                            announcement_type=self._classify_announcement_type(title),
                                            timestamp=self._parse_kind_time(date_str),
                                            url=base_url,
                                            importance=self._evaluate_importance(title, 'ê¸°íƒ€')
                                        )
                                        
                                        announcements.append(announcement)
                                        
                            except Exception as e:
                                continue
                    
                    if announcements:
                        break
                        
                except Exception as e:
                    continue
            
            logger.info(f"ğŸ“‹ KIND ê³µì‹œ {len(announcements)}ê°œ ìˆ˜ì§‘")
            
        except Exception as e:
            logger.warning(f"âš ï¸ KIND ê³µì‹œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
        
        return announcements
    
    def _get_sample_announcements(self, limit: int) -> List[AnnouncementItem]:
        """ìƒ˜í”Œ ê³µì‹œ ìƒì„± (í…ŒìŠ¤íŠ¸ìš©)"""
        sample_announcements = [
            ("ì‚¼ì„±ì „ì", "005930", "2024ë…„ 3ë¶„ê¸° ì‹¤ì ë°œí‘œ", "ì‹¤ì ê³µì‹œ", "high"),
            ("SKí•˜ì´ë‹‰ìŠ¤", "000660", "ì£¼ìš”ì‚¬í•­ë³´ê³ ì„œ(íˆ¬ìê²°ì •)", "ì‚¬ì—…ê´€ë ¨", "medium"),
            ("NAVER", "035420", "ìê¸°ì£¼ì‹ ì·¨ë“ ê²°ì •", "ìë³¸ê´€ë ¨", "medium"),
            ("LGí™”í•™", "051910", "í•´ì™¸ë²•ì¸ ì„¤ë¦½ ê²°ì •", "ì‚¬ì—…ê´€ë ¨", "medium"),
            ("ì¹´ì¹´ì˜¤", "035720", "ì •ê¸°ì£¼ì£¼ì´íšŒ ê²°ì˜ì‚¬í•­", "ì£¼ì£¼ê´€ë ¨", "low"),
            ("í˜„ëŒ€ì°¨", "005380", "ë¶„ê¸°ë³´ê³ ì„œ ì œì¶œ", "ì‹¤ì ê³µì‹œ", "medium"),
            ("ì…€íŠ¸ë¦¬ì˜¨", "068270", "ì‹ ì œí’ˆ í—ˆê°€ ì·¨ë“", "ì‚¬ì—…ê´€ë ¨", "high"),
            ("ì‚¼ì„±ë°”ì´ì˜¤ë¡œì§ìŠ¤", "207940", "ê³„ì•½ ì²´ê²°", "ì‚¬ì—…ê´€ë ¨", "medium"),
            ("í¬ìŠ¤ì½”í™€ë”©ìŠ¤", "003670", "ë°°ë‹¹ê¸ˆ ì§€ê¸‰ ê²°ì •", "ìë³¸ê´€ë ¨", "low"),
            ("SKì´ë…¸ë² ì´ì…˜", "096770", "ì£¼ìš”ê³„ì•½ ì²´ê²°", "ì‚¬ì—…ê´€ë ¨", "medium")
        ]
        
        announcements = []
        
        for i, (company, code, title, type_, importance) in enumerate(sample_announcements[:limit]):
            announcement = AnnouncementItem(
                title=title,
                company=company,
                stock_code=code,
                announcement_type=type_,
                timestamp=datetime.now() - timedelta(hours=i),
                url=f"https://example.com/announcement/{i+1}",
                importance=importance
            )
            announcements.append(announcement)
        
        logger.info(f"ğŸ“‹ ìƒ˜í”Œ ê³µì‹œ {len(announcements)}ê°œ ìƒì„±")
        return announcements

    def analyze_sentiment(self, text: str) -> tuple[str, float]:
        """ğŸ¤– í‚¤ì›Œë“œ ê¸°ë°˜ ê°ì •ë¶„ì„"""
        if not text:
            return 'neutral', 0.0
        
        text = text.lower()
        
        # ê¸ì •/ë¶€ì • í‚¤ì›Œë“œ ì¹´ìš´íŠ¸
        positive_count = sum(1 for keyword in self.positive_keywords if keyword in text)
        negative_count = sum(1 for keyword in self.negative_keywords if keyword in text)
        
        # ì ìˆ˜ ê³„ì‚°
        total_keywords = positive_count + negative_count
        
        if total_keywords == 0:
            return 'neutral', 0.0
        
        # ê°ì • ì ìˆ˜ (-1.0 ~ 1.0)
        sentiment_score = (positive_count - negative_count) / max(total_keywords, 1)
        
        # ê°ì • ë¶„ë¥˜
        if sentiment_score > 0.2:
            sentiment = 'positive'
        elif sentiment_score < -0.2:
            sentiment = 'negative'
        else:
            sentiment = 'neutral'
        
        return sentiment, sentiment_score
    
    def filter_stock_related(self, news_list: List[NewsItem], stock_code: str) -> List[NewsItem]:
        """ğŸ“ˆ ì¢…ëª© ê´€ë ¨ ë‰´ìŠ¤ í•„í„°ë§"""
        if not stock_code or stock_code not in self.stock_mapping:
            return []
        
        company_name = self.stock_mapping[stock_code]
        related_news = []
        
        for news in news_list:
            # ì œëª©ì´ë‚˜ ë‚´ìš©ì— íšŒì‚¬ëª…/ì¢…ëª©ì½”ë“œê°€ í¬í•¨ëœ ê²½ìš°
            text = f"{news.title} {news.content}".lower()
            
            if (company_name in text or 
                stock_code in text or
                any(keyword in text for keyword in [company_name.lower()])):
                
                # ê´€ë ¨ ì¢…ëª© ì •ë³´ ì¶”ê°€
                if not news.related_stocks:
                    news.related_stocks = []
                if stock_code not in news.related_stocks:
                    news.related_stocks.append(stock_code)
                
                related_news.append(news)
        
        logger.info(f"ğŸ“ˆ {company_name}({stock_code}) ê´€ë ¨ ë‰´ìŠ¤ {len(related_news)}ê°œ í•„í„°ë§")
        return related_news
    
    # === í—¬í¼ ë©”ì„œë“œë“¤ ===
    
    def _analyze_sentiment_item(self, news_item: NewsItem) -> NewsItem:
        """ë‰´ìŠ¤ ì•„ì´í…œ ê°ì •ë¶„ì„"""
        sentiment, score = self.analyze_sentiment(f"{news_item.title} {news_item.content}")
        news_item.sentiment = sentiment
        news_item.sentiment_score = score
        return news_item
    
    def _filter_by_keywords(self, news_list: List[NewsItem], keywords: List[str]) -> List[NewsItem]:
        """í‚¤ì›Œë“œë¡œ ë‰´ìŠ¤ í•„í„°ë§"""
        filtered = []
        
        for news in news_list:
            text = f"{news.title} {news.content}".lower()
            if any(keyword.lower() in text for keyword in keywords):
                filtered.append(news)
        
        return filtered
    
    def _parse_naver_time(self, time_str: str) -> datetime:
        """ë„¤ì´ë²„ ê¸ˆìœµ ë‰´ìŠ¤ ì‹œê°„ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
        now = datetime.now()
        time_str = time_str.strip()
        
        if 'ë¶„ ì „' in time_str:
            minutes = int(re.search(r'(\d+)ë¶„ ì „', time_str).group(1))
            return now - timedelta(minutes=minutes)
        elif 'ì‹œê°„ ì „' in time_str:
            hours = int(re.search(r'(\d+)ì‹œê°„ ì „', time_str).group(1))
            return now - timedelta(hours=hours)
        else:
            # 'YYYY.MM.DD HH:mm' ë˜ëŠ” 'YYYY.MM.DD' í˜•ì‹ ì²˜ë¦¬
            try:
                # ë‚ ì§œì™€ ì‹œê°„ì´ ëª¨ë‘ ìˆëŠ” ê²½ìš°
                if len(time_str.split()) > 1:
                    return datetime.strptime(time_str, '%Y.%m.%d %H:%M')
                # ë‚ ì§œë§Œ ìˆëŠ” ê²½ìš° (ì‹œê°„ì€ 00:00ìœ¼ë¡œ ì„¤ì •)
                else:
                    return datetime.strptime(time_str, '%Y.%m.%d')
            except ValueError:
                return now # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„ ë°˜í™˜
    
    def _parse_kind_time(self, date_str: str) -> datetime:
        """KIND ì‹œê°„ ë¬¸ìì—´ íŒŒì‹±"""
        try:
            # í˜•ì‹: "2024/01/15 15:30"
            return datetime.strptime(date_str.replace('-', '/'), '%Y/%m/%d %H:%M')
        except:
            try:
                # í˜•ì‹: "2024-01-15"
                return datetime.strptime(date_str.split()[0], '%Y-%m-%d')
            except:
                return datetime.now()
    
    def _extract_stock_code(self, company_text: str) -> str:
        """íšŒì‚¬ëª…ì—ì„œ ì¢…ëª©ì½”ë“œ ì¶”ì¶œ"""
        # ê´„í˜¸ ì•ˆì˜ 6ìë¦¬ ìˆ«ì ì°¾ê¸°
        match = re.search(r'\((\d{6})\)', company_text)
        if match:
            return match.group(1)
        
        # íšŒì‚¬ëª…ìœ¼ë¡œ ì½”ë“œ ì°¾ê¸°
        for name in self.company_to_code:
            if name in company_text:
                return self.company_to_code[name]
        
        return ''
    
    def _classify_announcement_type(self, title: str) -> str:
        """ê³µì‹œ ìœ í˜• ë¶„ë¥˜"""
        title_lower = title.lower()
        
        if any(word in title_lower for word in ['ì‹¤ì ', 'ë§¤ì¶œ', 'ì˜ì—…ì´ìµ', 'ìˆœì´ìµ']):
            return 'ì‹¤ì ê³µì‹œ'
        elif any(word in title_lower for word in ['í•©ë³‘', 'ì¸ìˆ˜', 'ë¶„í• ']):
            return 'M&A'
        elif any(word in title_lower for word in ['íˆ¬ì', 'ê³„ì•½', 'ìˆ˜ì£¼']):
            return 'ì‚¬ì—…ê´€ë ¨'
        elif any(word in title_lower for word in ['ì£¼ì£¼ì´íšŒ', 'ì´ì‚¬íšŒ']):
            return 'ì£¼ì£¼ê´€ë ¨'
        elif any(word in title_lower for word in ['ì¦ì', 'ê°ì', 'ë°°ë‹¹']):
            return 'ìë³¸ê´€ë ¨'
        else:
            return 'ê¸°íƒ€'
    
    def _evaluate_importance(self, title: str, announcement_type: str) -> str:
        """ê³µì‹œ ì¤‘ìš”ë„ í‰ê°€"""
        title_lower = title.lower()
        
        # ê³ ì¤‘ìš”ë„ í‚¤ì›Œë“œ
        high_keywords = ['í•©ë³‘', 'ì¸ìˆ˜', 'ë¶„í• ', 'ìƒì¥íì§€', 'ê±°ë˜ì •ì§€', 'ì˜ì—…ì •ì§€']
        if any(keyword in title_lower for keyword in high_keywords):
            return 'high'
        
        # ì¤‘ìš”ë„ í‚¤ì›Œë“œ
        medium_keywords = ['ì‹¤ì ', 'ì¦ì', 'ë°°ë‹¹', 'ì£¼ìš”ê³„ì•½', 'íˆ¬ì']
        if any(keyword in title_lower for keyword in medium_keywords):
            return 'medium'
        
        return 'low'
    
    def _is_cache_valid(self, key: str, cache_type: str) -> bool:
        """ìºì‹œ ìœ íš¨ì„± ê²€ì‚¬"""
        cache = self.news_cache if cache_type == 'news' else self.announcement_cache
        
        if key not in cache:
            return False
        
        cache_time = cache[key].get('timestamp')
        if not cache_time:
            return False
        
        return (datetime.now() - cache_time).total_seconds() < self.cache_duration
    
    def _cache_data(self, key: str, cache_type: str, data: Any) -> None:
        """ë°ì´í„° ìºì‹±"""
        cache = self.news_cache if cache_type == 'news' else self.announcement_cache
        cache[key] = {
            'data': data,
            'timestamp': datetime.now()
        }
    
    def get_market_sentiment_summary(self) -> Dict[str, Any]:
        """ğŸ“Š ì‹œì¥ ì „ì²´ ê°ì • ìš”ì•½"""
        try:
            # ìµœì‹  ë‰´ìŠ¤ ìˆ˜ì§‘
            news_list = self.get_realtime_news(limit=50)
            
            if not news_list:
                return {'sentiment': 'neutral', 'score': 0.0, 'news_count': 0}
            
            # ê°ì • ì ìˆ˜ í‰ê·  ê³„ì‚°
            total_score = sum(news.sentiment_score for news in news_list)
            avg_score = total_score / len(news_list)
            
            # ê°ì • ë¶„í¬ ê³„ì‚°
            sentiment_counts = {'positive': 0, 'negative': 0, 'neutral': 0}
            for news in news_list:
                sentiment_counts[news.sentiment] += 1
            
            # ì „ì²´ ê°ì • ê²°ì •
            if avg_score > 0.1:
                overall_sentiment = 'positive'
            elif avg_score < -0.1:
                overall_sentiment = 'negative'
            else:
                overall_sentiment = 'neutral'
            
            return {
                'sentiment': overall_sentiment,
                'score': round(avg_score, 3),
                'news_count': len(news_list),
                'distribution': sentiment_counts,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"âŒ ì‹œì¥ ê°ì • ìš”ì•½ ì‹¤íŒ¨: {e}")
            return {'sentiment': 'neutral', 'score': 0.0, 'news_count': 0}
    
    def cleanup(self):
        """ë¦¬ì†ŒìŠ¤ ì •ë¦¬"""
        try:
            self.session.close()
            self.news_cache.clear()
            self.announcement_cache.clear()
            logger.info("ğŸ§¹ NewsCollector ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì™„ë£Œ")
        except Exception as e:
            logger.error(f"âŒ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ ì‹¤íŒ¨: {e}") 