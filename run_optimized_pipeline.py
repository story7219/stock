#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
íŒŒì¼ëª…: run_optimized_pipeline.py
ëª¨ë“ˆ: ìµœì í™”ëœ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
ëª©ì : ë°ì´í„° ìˆ˜ì§‘ë¶€í„° GPU í›ˆë ¨ê¹Œì§€ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

Author: AI Assistant
Created: 2025-01-27
Modified: 2025-01-27
Version: 1.0.0
"""

from __future__ import annotations

import asyncio
import json
import os
import subprocess
import sys
import time
from datetime import datetime
from pathlib import Path

try:
    import torch
except ImportError:
    torch = None

def check_environment() -> bool:
    """í™˜ê²½ ì„¤ì • í™•ì¸"""
    print("ğŸ” í™˜ê²½ ì„¤ì • í™•ì¸ ì¤‘...")

    # í•„ìˆ˜ í™˜ê²½ë³€ìˆ˜ í™•ì¸
    required_env_vars = [
        'LIVE_KIS_APP_KEY',
        'LIVE_KIS_APP_SECRET',
        'LIVE_KIS_ACCOUNT_NUMBER',
    ]
    missing_vars = [var for var in required_env_vars if not os.getenv(var)]
    if missing_vars:
        print(f"âŒ ëˆ„ë½ëœ í™˜ê²½ë³€ìˆ˜: {', '.join(missing_vars)}")
        print("qubole_env_example.txt íŒŒì¼ì„ ì°¸ê³ í•˜ì—¬ í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•˜ì„¸ìš”.")
        return False

    # Python íŒ¨í‚¤ì§€ í™•ì¸
    required_packages = [
        'pandas', 'numpy', 'pyarrow', 'dask',
        'redis', 'sqlalchemy', 'aiohttp', 'pykis',
    ]
    missing_packages = []
    for package in required_packages:
        try:
            __import__(package)
        except ImportError:
            missing_packages.append(package)
    if missing_packages:
        print(f"âŒ ëˆ„ë½ëœ íŒ¨í‚¤ì§€: {', '.join(missing_packages)}")
        print("pip install -r requirements.txtë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
        return False

    # GPU í™•ì¸
    if torch is not None:
        try:
            if torch.cuda.is_available():
                gpu_name = torch.cuda.get_device_name(0)
                gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9
                print(f"âœ… GPU í™•ì¸: {gpu_name} ({gpu_memory:.1f}GB)")
            else:
                print("âš ï¸ GPUë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
        except Exception:
            print("âš ï¸ PyTorchê°€ ì„¤ì¹˜ë˜ì—ˆìœ¼ë‚˜ GPU í™•ì¸ ì‹¤íŒ¨. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    else:
        print("âš ï¸ PyTorchê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")

    print("âœ… í™˜ê²½ ì„¤ì • í™•ì¸ ì™„ë£Œ")
    return True

def run_data_collection() -> bool:
    """ë°ì´í„° ìˆ˜ì§‘ ì‹¤í–‰"""
    print("\nğŸ“Š ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    try:
        result = subprocess.run([
            sys.executable, "optimized_data_pipeline.py"
        ], capture_output=True, text=True, timeout=3600)
        if result.returncode == 0:
            print("âœ… ë°ì´í„° ìˆ˜ì§‘ ì™„ë£Œ")
            return True
        else:
            print(f"âŒ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨: {result.stderr}")
            return False
    except subprocess.TimeoutExpired:
        print("âŒ ë°ì´í„° ìˆ˜ì§‘ ì‹œê°„ ì´ˆê³¼")
        return False
    except Exception as e:
        print(f"âŒ ë°ì´í„° ìˆ˜ì§‘ ì˜¤ë¥˜: {e}")
        return False

def run_gpu_training() -> bool:
    """GPU í›ˆë ¨ ì‹¤í–‰"""
    print("\nğŸ¤– GPU í›ˆë ¨ ì‹œì‘...")
    data_files = list(Path(".").glob("optimized_data_*.parquet"))
    if not data_files:
        print("âŒ í›ˆë ¨í•  ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False
    latest_data_file = max(data_files, key=lambda x: x.stat().st_mtime)
    print(f"ğŸ“ ì‚¬ìš©í•  ë°ì´í„° íŒŒì¼: {latest_data_file}")
    try:
        result = subprocess.run([
            sys.executable, "gpu_optimized_training.py"
        ], capture_output=True, text=True, timeout=7200)
        if result.returncode == 0:
            print("âœ… GPU í›ˆë ¨ ì™„ë£Œ")
            return True
        else:
            print(f"âŒ GPU í›ˆë ¨ ì‹¤íŒ¨: {result.stderr}")
            return False
    except subprocess.TimeoutExpired:
        print("âŒ GPU í›ˆë ¨ ì‹œê°„ ì´ˆê³¼")
        return False
    except Exception as e:
        print(f"âŒ GPU í›ˆë ¨ ì˜¤ë¥˜: {e}")
        return False

def generate_performance_report() -> None:
    """ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±"""
    print("\nğŸ“ˆ ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„±...")
    report = {
        "timestamp": datetime.now().isoformat(),
        "data_collection": {
            "status": "completed",
            "files": [str(f) for f in Path(".").glob("optimized_data_*.parquet")],
        },
        "gpu_training": {
            "status": "completed",
            "models": [str(f) for f in Path(".").glob("gpu_optimized_model_*.pth")],
        },
        "logs": {
            "pipeline": "optimized_pipeline.log",
            "training": "gpu_training.log",
        },
    }
    with open("performance_report.json", "w", encoding="utf-8") as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    print("âœ… ì„±ëŠ¥ ë¦¬í¬íŠ¸ ìƒì„± ì™„ë£Œ: performance_report.json")

def main() -> None:
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸš€ ìµœì í™”ëœ AI íŠ¸ë ˆì´ë”© íŒŒì´í”„ë¼ì¸ ì‹¤í–‰")
    print("=" * 60)
    start_time = time.time()
    if not check_environment():
        print("âŒ í™˜ê²½ ì„¤ì • í™•ì¸ ì‹¤íŒ¨")
        return
    if not run_data_collection():
        print("âŒ ë°ì´í„° ìˆ˜ì§‘ ì‹¤íŒ¨")
        return
    if not run_gpu_training():
        print("âŒ GPU í›ˆë ¨ ì‹¤íŒ¨")
        return
    generate_performance_report()
    total_time = time.time() - start_time
    print(f"\nğŸ‰ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ!")
    print(f"ì´ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")
    print("\nğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:")
    for pattern in ["optimized_data_*.parquet", "gpu_optimized_model_*.pth", "*.log"]:
        files = list(Path(".").glob(pattern))
        for file in files:
            size_mb = file.stat().st_size / (1024 * 1024)
            print(f"  {file.name} ({size_mb:.1f}MB)")

if __name__ == "__main__":
    main()

